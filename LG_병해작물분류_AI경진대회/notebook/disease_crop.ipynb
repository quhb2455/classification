{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4636868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from easydict import EasyDict\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import json \n",
    "import timm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa2ce4",
   "metadata": {},
   "source": [
    "# Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd44a54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def label_preprocessing(path) :\n",
    "#     path = os.path.join(path, 'train.csv')\n",
    "    labels = pd.read_csv(path)\n",
    "\n",
    "    cnt = 0\n",
    "    label_encoder = {}\n",
    "    for i, label in enumerate(tqdm(sorted(labels['label']))) :\n",
    "        \n",
    "        if label not in label_encoder.values() :\n",
    "            label_encoder[cnt] = label\n",
    "            cnt += 1\n",
    "        \n",
    "    label_decoder = {val : key for key, val in label_encoder.items()}\n",
    "    \n",
    "    return label_encoder, label_decoder\n",
    "\n",
    "# enc, dec = label_preprocessing(\"../data/train.csv\")\n",
    "# display(enc)\n",
    "# display(dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45adac9d",
   "metadata": {},
   "source": [
    "# CSV feature - min, max value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195590dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_feature_dict(path, csv_features) :\n",
    "    \n",
    "    csv_files = sorted(glob(os.path.join(path, '*/*.csv')))\n",
    "\n",
    "    temp_csv = pd.read_csv(csv_files[0])[csv_features]\n",
    "    max_arr, min_arr = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "\n",
    "    # feature 별 최대값, 최솟값 계산\n",
    "    for csv in tqdm(csv_files[1:]):\n",
    "        temp_csv = pd.read_csv(csv)[csv_features]\n",
    "        temp_csv = temp_csv.replace('-',np.nan).dropna()\n",
    "        if len(temp_csv) == 0:\n",
    "            continue\n",
    "        temp_csv = temp_csv.astype(float)\n",
    "        temp_max, temp_min = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "        max_arr = np.max([max_arr,temp_max], axis=0)\n",
    "        min_arr = np.min([min_arr,temp_min], axis=0)\n",
    "\n",
    "    # feature 별 최대값, 최솟값 dictionary return\n",
    "    return {csv_features[i]:[min_arr[i], max_arr[i]] for i in range(len(csv_features))}\n",
    "\n",
    "# csv_feature_dict = csv_feature_dict('../data/train', csv_features)\n",
    "# csv_feature_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420ac20a",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a55be8e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_split(path, label_decoder, kfold=False, mode=\"train\", test_size=0.2) : \n",
    "    imgs = glob(os.path.join(path, '*/*.jpg'))\n",
    "\n",
    "    if mode == \"train\" :\n",
    "        json_files = glob(os.path.join(path, '*/*.json'))\n",
    "\n",
    "        label_list = []\n",
    "        for json_path in tqdm(json_files) :\n",
    "            json_file = json.load(open(json_path, 'r'))\n",
    "\n",
    "            crop = json_file[\"annotations\"][\"crop\"]\n",
    "            disease = json_file[\"annotations\"][\"disease\"]\n",
    "            risk = json_file[\"annotations\"][\"risk\"]\n",
    "\n",
    "            label = f'{crop}_{disease}_{risk}'\n",
    "            label_list.append(label_decoder[label])\n",
    "        \n",
    "        if kfold :\n",
    "            return imgs, label_list\n",
    "        else :\n",
    "            return train_test_split(imgs, test_size=test_size, shuffle=True, stratify=label_list)\n",
    "        \n",
    "    else :\n",
    "        return imgs\n",
    "        \n",
    "# a, b = data_split('../data/train', label_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a210c4",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f283bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(size=224):\n",
    "    train_transforms = A.Compose([\n",
    "                A.Resize(size ,size),\n",
    "                A.OneOf([\n",
    "                    A.Rotate(),\n",
    "                    A.HorizontalFlip(),\n",
    "                    A.VerticalFlip()\n",
    "                ], p=1)\n",
    "            ])\n",
    "\n",
    "    val_transforms = A.Compose([\n",
    "        A.Resize(size,size)\n",
    "    ])\n",
    "    \n",
    "    return train_transforms, val_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69b5c3",
   "metadata": {},
   "source": [
    "# Custom Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b2705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 files, \n",
    "                 transforms, \n",
    "                 label_decoder, \n",
    "                 opt,\n",
    "                 mode='train'):\n",
    "        \n",
    "        if opt.use_kfold :\n",
    "            self.files = self.kfold_files(files, opt)\n",
    "        else : \n",
    "            self.files = files\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.label_decoder = label_decoder #label_encoder\n",
    "        self.csv_feature_dict = opt.csv_feature_dict\n",
    "        self.max_len = opt.max_len\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        file = self.files[i]\n",
    "        \n",
    "        # CSV\n",
    "        csv_data, seq_len = self.csv_preprocessing(file)\n",
    "        \n",
    "        # image\n",
    "        img = self.img_preprocessing(file)\n",
    "        \n",
    "        if self.mode == 'train':         \n",
    "            # Label\n",
    "            label = self.label_preprocessing(file)\n",
    "            \n",
    "            return {\n",
    "                'img' : torch.tensor(img, dtype=torch.float32),\n",
    "                'label' : torch.tensor(self.label_decoder[label], dtype=torch.long),\n",
    "                'csv_feature': torch.tensor(csv_data, dtype=torch.float32),\n",
    "                'seq_len' : seq_len\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            return {\n",
    "                'img' : torch.tensor(img, dtype=torch.float32),\n",
    "                'csv_feature': torch.tensor(csv_data, dtype=torch.float32),\n",
    "                'seq_len' : seq_len\n",
    "            }\n",
    "        \n",
    "    def kfold_files(self, data_index, opt) :\n",
    "        file_list = glob(os.path.join(opt.dataset_path, \"*/*.jpg\"))\n",
    "        return [file_list[idx] for idx in data_index]\n",
    "        \n",
    "    \n",
    "    def csv_preprocessing(self, file) :\n",
    "        # CSV\n",
    "        csv_path = file.replace(\"jpg\",\"csv\")\n",
    "        df = pd.read_csv(csv_path)[self.csv_feature_dict.keys()]\n",
    "        df = df.replace('-', 0)\n",
    "        \n",
    "        # MinMax scaling\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].astype(float) - self.csv_feature_dict[col][0]\n",
    "            df[col] = df[col] / (self.csv_feature_dict[col][1]-self.csv_feature_dict[col][0])\n",
    "\n",
    "        # pack_padded_sequence 하기 위한 len 추가\n",
    "        seq_len = len(df)\n",
    "\n",
    "        df_np = df.to_numpy()\n",
    "        df_len, df_features = df_np.shape\n",
    "        \n",
    "        csv_data = np.zeros([self.max_len, df_features])\n",
    "        csv_data[0:df_len, :] = df_np\n",
    "        \n",
    "        return csv_data, seq_len\n",
    "\n",
    "    def img_preprocessing(self, file) :\n",
    "        image_path = file\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = self.transforms(image=img)[\"image\"]\n",
    "        img = img.transpose(2,0,1)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def label_preprocessing(self, file) :\n",
    "        json_path = file.replace(\"jpg\",\"json\")\n",
    "        with open(json_path, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "\n",
    "        crop = json_file['annotations']['crop']\n",
    "        disease = json_file['annotations']['disease']\n",
    "        risk = json_file['annotations']['risk']\n",
    "        \n",
    "        return f'{crop}_{disease}_{risk}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce6704d",
   "metadata": {},
   "source": [
    "# Model - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1580252",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CNN_Encoder(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, pretrained_path=None):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        \n",
    "        if pretrained_path :\n",
    "            # no use pretrained model trained with Public dataset\n",
    "            self.model = self.create_pretrained_model(model_name, num_classes, pretrained_path)\n",
    "            \n",
    "        else :            \n",
    "            self.model = timm.create_model(model_name, num_classes=num_classes, pretrained=True)\n",
    "            \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        output = self.model(inputs)\n",
    "        return output\n",
    "\n",
    "    def create_pretrained_model(self, model_name, num_classes, pretrained_path):\n",
    "        pre_model = torch.load(pretrained_path, map_location=\"cpu\")\n",
    "        output_size = pre_model[list(pre_model.keys())[-1]].shape[0]\n",
    " \n",
    "        return nn.Sequential(\n",
    "                    timm.create_model(model_name, num_classes=output_size, pretrained=True),\n",
    "                    nn.Linear(output_size, num_classes)\n",
    "                )\n",
    "\n",
    "# model = CNN_Encoder(\"efficientnetv2_rw_s\", 1000)\n",
    "# model = CNN_Encoder(\"efficientnetv2_rw_s\", 1000, \"../model/k_fold_50k_pretrained_effiv2S/4_f9462_public_vill_50k_pretrain_efficientnetv2S.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5f419",
   "metadata": {},
   "source": [
    "# Model - RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0db46531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class MLSTMfcn(nn.Module):\n",
    "    def __init__(self, *, num_classes, max_seq_len, num_features,\n",
    "                 num_lstm_out=128, num_lstm_layers=1, \n",
    "                 conv1_nf=128, conv2_nf=256, conv3_nf=128,\n",
    "                 lstm_drop_p=0.8, fc_drop_p=0.3):\n",
    "        \n",
    "        super(MLSTMfcn, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_features = num_features\n",
    "\n",
    "        self.num_lstm_out = num_lstm_out\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "\n",
    "        self.conv1_nf = conv1_nf\n",
    "        self.conv2_nf = conv2_nf\n",
    "        self.conv3_nf = conv3_nf\n",
    "\n",
    "        self.lstm_drop_p = lstm_drop_p\n",
    "        self.fc_drop_p = fc_drop_p\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.num_features, \n",
    "                            hidden_size=self.num_lstm_out,\n",
    "                            num_layers=self.num_lstm_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(self.num_features, self.conv1_nf, 8)\n",
    "        self.conv2 = nn.Conv1d(self.conv1_nf, self.conv2_nf, 5)\n",
    "        self.conv3 = nn.Conv1d(self.conv2_nf, self.conv3_nf, 3)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(self.conv1_nf)\n",
    "        self.bn2 = nn.BatchNorm1d(self.conv2_nf)\n",
    "        self.bn3 = nn.BatchNorm1d(self.conv3_nf)\n",
    "\n",
    "        self.se1 = SELayer(self.conv1_nf)  # ex 128\n",
    "        self.se2 = SELayer(self.conv2_nf)  # ex 256\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstmDrop = nn.Dropout(self.lstm_drop_p)\n",
    "        self.convDrop = nn.Dropout(self.fc_drop_p)\n",
    "\n",
    "        self.fc = nn.Linear(self.conv3_nf+self.num_lstm_out, 128)\n",
    "\n",
    "        self.out_layer = nn.Linear(self.num_classes+128, self.num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, enc_out, x, seq_lens):\n",
    "        ''' input x should be in size [B,T,F], where \n",
    "            B = Batch size\n",
    "            T = Time samples\n",
    "            F = features\n",
    "        '''\n",
    "        x1 = nn.utils.rnn.pack_padded_sequence(x, seq_lens.cpu(), \n",
    "                                               batch_first=True, \n",
    "                                               enforce_sorted=False)\n",
    "        x1, (ht,ct) = self.lstm(x1)\n",
    "        x1, _ = nn.utils.rnn.pad_packed_sequence(x1, batch_first=True, \n",
    "                                                 padding_value=0.0)\n",
    "        x1 = x1[:,-1,:]\n",
    "        \n",
    "        x2 = x.transpose(2,1)\n",
    "        x2 = self.convDrop(self.relu(self.bn1(self.conv1(x2))))\n",
    "        x2 = self.se1(x2)\n",
    "        x2 = self.convDrop(self.relu(self.bn2(self.conv2(x2))))\n",
    "        x2 = self.se2(x2)\n",
    "        x2 = self.convDrop(self.relu(self.bn3(self.conv3(x2))))\n",
    "        x2 = torch.mean(x2,2)\n",
    "        \n",
    "        x_all = torch.cat((x1,x2),dim=1)\n",
    "        x_out = self.fc(x_all)\n",
    "        concat = torch.cat([enc_out, x_out], dim=1)  # enc_out + hidden \n",
    "        output = self.dropout(concat)\n",
    "        x_output = self.out_layer(output)\n",
    "        x_out = F.log_softmax(x_output, dim=1)\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "# model = MLSTMfcn(num_classes=38, max_seq_len=512, num_features=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100c687",
   "metadata": {},
   "source": [
    "# Model - CNN + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b59e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2RNN(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(CNN2RNN, self).__init__()\n",
    "        \n",
    "        self.cnn = CNN_Encoder(opt.model_name, opt.num_classes, opt.pretrained_path)\n",
    "        self.rnn = MLSTMfcn(num_classes=opt.num_classes, max_seq_len=opt.max_len, num_features=opt.num_features)\n",
    "\n",
    "\n",
    "    def forward(self, img, seq, seq_len):\n",
    "        cnn_output = self.cnn(img)\n",
    "        output = self.rnn(cnn_output, seq, seq_len)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# model = CNN2RNN(\"efficientnetv2_rw_s\", \n",
    "#                 1000, \n",
    "#                 512,\n",
    "#                 6,\n",
    "#                 \"../model/k_fold_50k_pretrained_effiv2S/4_f9462_public_vill_50k_pretrain_efficientnetv2S.pt\")\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13736063",
   "metadata": {},
   "source": [
    "# CutMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caedba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    " \n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e10041f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e49d8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_function(real, pred):    \n",
    "    real = real.cpu()\n",
    "    pred = torch.argmax(pred, dim=1).cpu()\n",
    "    score = f1_score(real, pred, average='macro')\n",
    "    return score\n",
    "\n",
    "def run(train_loader, valid_loader, opt) :\n",
    "    \n",
    "    model = CNN2RNN(opt).to(opt.device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr= opt.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    early_stopping_step = 0\n",
    "    best_loss = 10\n",
    "    for epoch in range(opt.epochs) : \n",
    "\n",
    "        # training\n",
    "        tqdm_train = tqdm(train_loader)\n",
    "        train_loss, train_macro_f1 = 0, 0\n",
    "        for batch, batch_item in enumerate(tqdm_train) :\n",
    "            model.train()\n",
    "            \n",
    "            img = batch_item['img'].to(opt.device)\n",
    "            label = batch_item['label'].to(opt.device)\n",
    "            csv_feature = batch_item['csv_feature'].to(opt.device)\n",
    "            seq_lens = batch_item['seq_len'].to(opt.device)\n",
    "\n",
    "            lam = np.random.beta(1.0, 1.0)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # add - cutmix\n",
    "                rand_index = torch.randperm(img.size()[0])\n",
    "                target_a = label\n",
    "                target_b = label[rand_index]\n",
    "                bbx1, bby1, bbx2, bby2 = rand_bbox(img.size(), lam)\n",
    "                img[:, :, bbx1:bbx2, bby1:bby2] = img[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "                lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (img.size()[-1] * img.size()[-2]))\n",
    "\n",
    "                output = model(img, csv_feature, seq_lens)\n",
    "                loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1. - lam)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            score = accuracy_function(label, output)\n",
    "            \n",
    "            train_loss += loss\n",
    "            train_macro_f1 += score\n",
    "            \n",
    "            tqdm_train.set_postfix({\"Epoch\" : epoch+1,\n",
    "                                    \"Mean train loss\" : \"{:06f}\".format(train_loss/(batch+1)),\n",
    "                                    \"Mean train f1\" : \"{:06f}\".format(train_macro_f1/(batch+1))\n",
    "                                   })\n",
    "            \n",
    "#             print(f\"Traing Epoch : [{epoch}/{opt.epochs}] loss : {train_loss}  f1 : {train_macro_f1}\",end='\\r')\n",
    "            \n",
    "#         print(f\"Traing Epoch : [{epoch}/{opt.epochs}] loss : {train_loss}  f1 : {train_macro_f1}\")\n",
    "        \n",
    "        # validation\n",
    "        tqdm_valid = tqdm(valid_loader)\n",
    "        mean_valid_loss, valid_loss, valid_macro_f1 = 0, 0, 0\n",
    "        for batch, batch_item in enumerate(tqdm_valid) :\n",
    "            img = batch_item['img'].to(opt.device)\n",
    "            label = batch_item['label'].to(opt.device)\n",
    "            csv_feature = batch_item['csv_feature'].to(opt.device)\n",
    "            seq_lens = batch_item['seq_len'].to(opt.device)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model(img, csv_feature, seq_lens)\n",
    "                loss = criterion(output, label)\n",
    "            score = accuracy_function(label, output)\n",
    "            \n",
    "            valid_loss += loss\n",
    "            valid_macro_f1 += score\n",
    "\n",
    "            mean_valid_loss = valid_loss / (batch + 1)\n",
    "            tqdm_valid.set_postfix({\"Mean valid loss\": \"{:06f}\".format(mean_valid_loss),\n",
    "                                    \"Mean valid f1\": \"{:06f}\".format(valid_macro_f1 / (batch + 1))\n",
    "                                    })\n",
    "\n",
    "            \n",
    "#             print(f\"Valid Epoch : [{epoch}/{opt.epochs}] loss : {valid_loss}  f1 : {valid_macro_f1}\",end='\\r')\n",
    "            \n",
    "#         print(f\"Valid Epoch : [{epoch}/{opt.epochs}] loss : {valid_loss}  f1 : {valid_macro_f1}\")\n",
    "\n",
    "        if mean_valid_loss < best_loss :\n",
    "            early_stopping_step = 0\n",
    "            best_loss = mean_valid_loss\n",
    "            os.makedirs(opt.save_path, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(opt.save_path, f'{epoch}E_{mean_valid_loss:0.4f}_{opt.model_name}.pt'))\n",
    "        \n",
    "        elif mean_valid_loss > best_loss and epoch != 0 :\n",
    "            early_stopping_step += 1\n",
    "            print(f\"Early Stopping Step : [{early_stopping_step} / {opt.early_stopping}]\")\n",
    "            \n",
    "        if early_stopping_step == opt.early_stopping :\n",
    "            print(\"=== Early Stop ===\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cc477d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5766/5766 [00:24<00:00, 236.85it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5767/5767 [00:00<00:00, 2890946.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# 변수 설명 csv 파일 참조\n",
    "crop = {'1':'딸기','2':'토마토','3':'파프리카','4':'오이','5':'고추','6':'시설포도'}\n",
    "disease = {'1':{'a1':'딸기잿빛곰팡이병','a2':'딸기흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '2':{'a5':'토마토흰가루병','a6':'토마토잿빛곰팡이병','b2':'열과','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '3':{'a9':'파프리카흰가루병','a10':'파프리카잘록병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '4':{'a3':'오이노균병','a4':'오이흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '5':{'a7':'고추탄저병','a8':'고추흰가루병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '6':{'a11':'시설포도탄저병','a12':'시설포도노균병','b4':'일소피해','b5':'축과병'}}\n",
    "risk = {'1':'초기','2':'중기','3':'말기'}\n",
    "\n",
    "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
    "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
    "\n",
    "\n",
    "opt = {\"dataset_path\" : \"../data/train\",\n",
    "       \"label_path\" : \"../data/train.csv\",\n",
    "       \"save_path\" : \"../pretrain\",\n",
    "       \"pretrained_path\" : None,\n",
    "       \"batch_size\" : 16,\n",
    "       \"use_kfold\" : False,\n",
    "       \"kfold_splits\" : 4,\n",
    "       \"model_name\" : 'deit_small_patch16_224',\n",
    "       \"resize\" : 224,\n",
    "       \"num_classes\" : 38,\n",
    "       \"learning_rate\" : 1e-4,\n",
    "       \"early_stopping\" : 5,\n",
    "       \"device\" : \"cuda\", \n",
    "       \"csv_features\" : csv_features,\n",
    "       \"max_len\" : 590,\n",
    "       \"num_features\" : len(csv_features),\n",
    "       \"epochs\" : 3}\n",
    "\n",
    "opt = EasyDict(opt)\n",
    "\n",
    "# csv_feature_dict 옵션 추가\n",
    "opt.csv_feature_dict = csv_feature_dict(opt.dataset_path, opt.csv_features)\n",
    "\n",
    "# label_enc, dec 및 trasnforms 설정\n",
    "label_encoder, label_decoder = label_preprocessing(opt.label_path)\n",
    "train_transforms, valid_transforms = transform(size=opt.resize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf06b2c0",
   "metadata": {},
   "source": [
    "# Train Without kfold training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "323d20ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< option >>\n",
      "dataset_path : ../data/train\n",
      "label_path : ../data/train.csv\n",
      "save_path : ../pretrain\n",
      "pretrained_path : None\n",
      "batch_size : 16\n",
      "use_kfold : False\n",
      "kfold_splits : 4\n",
      "model_name : deit_small_patch16_224\n",
      "resize : 224\n",
      "num_classes : 38\n",
      "learning_rate : 0.0001\n",
      "early_stopping : 5\n",
      "device : cuda\n",
      "csv_features : ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
      "max_len : 590\n",
      "num_features : 9\n",
      "epochs : 3\n",
      "csv_feature_dict : {'내부 온도 1 평균': [3.4, 47.3], '내부 온도 1 최고': [3.4, 47.6], '내부 온도 1 최저': [3.3, 47.0], '내부 습도 1 평균': [23.7, 100.0], '내부 습도 1 최고': [25.9, 100.0], '내부 습도 1 최저': [0.0, 100.0], '내부 이슬점 평균': [0.1, 34.5], '내부 이슬점 최고': [0.2, 34.7], '내부 이슬점 최저': [0.0, 34.4]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 5767/5767 [00:00<00:00, 13901.63it/s]\n",
      "100%|█████████████████████| 289/289 [01:19<00:00,  3.61it/s, Epoch=1, Mean train loss=2.296077, Mean train f1=0.247283]\n",
      "100%|████████████████████████████████| 73/73 [00:17<00:00,  4.14it/s, Mean valid loss=1.542666, Mean valid f1=0.363203]\n",
      "100%|█████████████████████| 289/289 [01:20<00:00,  3.61it/s, Epoch=2, Mean train loss=1.904850, Mean train f1=0.362329]\n",
      "100%|████████████████████████████████| 73/73 [00:17<00:00,  4.17it/s, Mean valid loss=1.142041, Mean valid f1=0.504638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████| 289/289 [01:20<00:00,  3.60it/s, Epoch=3, Mean train loss=1.718108, Mean train f1=0.427098]\n",
      "100%|████████████████████████████████| 73/73 [00:17<00:00,  4.17it/s, Mean valid loss=0.950463, Mean valid f1=0.542767]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# option 출력\n",
    "print(\"<< option >>\")\n",
    "print(*[\"{} : {}\".format(k, v) for k, v in opt.items()], sep='\\n')\n",
    "\n",
    "# data split with stratify\n",
    "train, valid = data_split(opt.dataset_path, label_decoder)\n",
    "\n",
    "train_dataset = CustomDataset(train, train_transforms, label_decoder, opt)\n",
    "valid_dataset = CustomDataset(valid, valid_transforms, label_decoder, opt)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=opt.batch_size, shuffle=False)\n",
    "\n",
    "run(train_loader, valid_loader, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e0713",
   "metadata": {},
   "source": [
    "# Train With kfold training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01ac1cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<< option >>\n",
      "dataset_path : ../data/train\n",
      "label_path : ../data/train.csv\n",
      "save_path : ../pretrain\n",
      "pretrained_path : None\n",
      "batch_size : 16\n",
      "use_kfold : True\n",
      "kfold_splits : 4\n",
      "model_name : deit_small_patch16_224\n",
      "resize : 224\n",
      "num_classes : 38\n",
      "learning_rate : 0.0001\n",
      "early_stopping : 5\n",
      "device : cuda\n",
      "csv_features : ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
      "max_len : 590\n",
      "num_features : 9\n",
      "epochs : 1\n",
      "csv_feature_dict : {'내부 온도 1 평균': [3.4, 47.3], '내부 온도 1 최고': [3.4, 47.6], '내부 온도 1 최저': [3.3, 47.0], '내부 습도 1 평균': [23.7, 100.0], '내부 습도 1 최고': [25.9, 100.0], '내부 습도 1 최저': [0.0, 100.0], '내부 이슬점 평균': [0.1, 34.5], '내부 이슬점 최고': [0.2, 34.7], '내부 이슬점 최저': [0.0, 34.4]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 5767/5767 [00:00<00:00, 14896.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "===== k_fold : 1 / 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████| 271/271 [01:17<00:00,  3.49it/s, Epoch=1, Mean train loss=2.348999, Mean train f1=0.250484]\n",
      "100%|████████████████████████████████| 91/91 [00:22<00:00,  4.01it/s, Mean valid loss=1.650510, Mean valid f1=0.351820]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 5]\n",
      "\n",
      "\n",
      "\n",
      "===== k_fold : 2 / 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████| 271/271 [01:16<00:00,  3.53it/s, Epoch=1, Mean train loss=2.339868, Mean train f1=0.237967]\n",
      "100%|████████████████████████████████| 91/91 [00:22<00:00,  4.03it/s, Mean valid loss=1.611374, Mean valid f1=0.368475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 5]\n",
      "\n",
      "\n",
      "\n",
      "===== k_fold : 3 / 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████| 271/271 [01:16<00:00,  3.53it/s, Epoch=1, Mean train loss=2.326114, Mean train f1=0.249759]\n",
      "100%|████████████████████████████████| 91/91 [00:21<00:00,  4.24it/s, Mean valid loss=1.560356, Mean valid f1=0.390233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 5]\n",
      "\n",
      "\n",
      "\n",
      "===== k_fold : 4 / 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████| 271/271 [01:16<00:00,  3.55it/s, Epoch=1, Mean train loss=2.357530, Mean train f1=0.244086]\n",
      "100%|████████████████████████████████| 91/91 [00:22<00:00,  4.07it/s, Mean valid loss=1.667482, Mean valid f1=0.347218]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# option 출력\n",
    "print(\"<< option >>\")\n",
    "print(*[\"{} : {}\".format(k, v) for k, v in opt.items()], sep='\\n')\n",
    "\n",
    "img_list, label_list = data_split(opt.dataset_path, label_decoder, kfold=True)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=opt.kfold_splits, random_state=13, shuffle=True)\n",
    "for k, (fold_train, fold_valid) in enumerate(kfold.split(img_list, label_list), 1) :\n",
    "    \n",
    "    print(f\"\\n\\n\\n===== k_fold : {k} / {opt.kfold_splits} =====\")\n",
    "    train_dataset = CustomDataset(fold_train, train_transforms, label_decoder, opt)\n",
    "    valid_dataset = CustomDataset(fold_valid, valid_transforms, label_decoder, opt)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=opt.batch_size, shuffle=False)\n",
    "\n",
    "    run(train_loader, valid_loader, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab8338a",
   "metadata": {},
   "source": [
    "# Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c453e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softvoting(models, item, opt) :\n",
    "    img = item['img'].to(opt.device)\n",
    "    seq_len = item['seq_len'].to(opt.device)\n",
    "    csv_feature = item['csv_feature'].to(opt.device)\n",
    "    \n",
    "    predicts = torch.zeros(img.size(0), opt.num_classes)\n",
    "    with torch.no_grad() :\n",
    "        for idx, model in enumerate(models) :\n",
    "            \n",
    "            output = model(img, csv_feature, seq_len)\n",
    "\n",
    "            output = F.softmax(output.cpu(), dim=1)\n",
    "            predicts += output\n",
    "\n",
    "    # 둘다 값은 똑같이 나옴.\n",
    "    # pred_avg = predicts / len(models)\n",
    "    # answer = pred_avg.argmax(dim=-1)\n",
    "    # _, answer2 = torch.max(pred_avg, 1)\n",
    "\n",
    "    return predicts.detach().cpu() / len(models)\n",
    "\n",
    "def hardvoting(models, item, opt) :\n",
    "    img = item['img'].to(opt.device)\n",
    "    seq_len = item['seq_len'].to(opt.device)\n",
    "    csv_feature = item['csv_feature'].to(opt.device)\n",
    "    \n",
    "    predicts = torch.zeros(img.size(0), opt.num_classes)\n",
    "    results = {}\n",
    "    tmp_vals = [0] * 16\n",
    "    tmp_inds = [0] * 16\n",
    "    with torch.no_grad() :\n",
    "        for idx, model in enumerate(models) :\n",
    "            \n",
    "            outputs = model(img, csv_feature, seq_len)\n",
    "                \n",
    "            outputs = F.softmax(outputs.cpu(), dim=1)\n",
    "\n",
    "            vals, indices = torch.max(outputs, 1)\n",
    "            \n",
    "            for i, (val, idx) in enumerate(zip(vals, indices)) :\n",
    "                if tmp_vals[i] < val :\n",
    "                    tmp_vals[i] = val.item()\n",
    "                    tmp_inds[i] = idx.item()\n",
    "                    \n",
    "                \n",
    "#             print(tmp_vals)\n",
    "#             print(tmp_inds)\n",
    "\n",
    "\n",
    "            predicts += outputs\n",
    "\n",
    "    # 둘다 값은 똑같이 나옴.\n",
    "    # pred_avg = predicts / len(models)\n",
    "    # answer = pred_avg.argmax(dim=-1)\n",
    "    # _, answer2 = torch.max(pred_avg, 1)\n",
    "\n",
    "    return predicts.detach().cpu() / len(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4561fa30",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "21058526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_loader, models, opt, voting=\"soft\") :\n",
    "    tqdm_dataset = tqdm(test_loader)\n",
    "    results = []\n",
    "    for batch, batch_item in enumerate(tqdm_dataset) :\n",
    "        \n",
    "        if voting == \"soft\" :\n",
    "            predictions = softvoting(models, batch_item, opt)\n",
    "            \n",
    "        elif voting == \"hard\" :\n",
    "            predictions = hardvoting(models, batch_item, opt)\n",
    "            \n",
    "        else :\n",
    "            # single model\n",
    "            img = batch_item['img'].to(opt.device)\n",
    "            csv_feature = batch_item['csv_feature'].to(opt.device)\n",
    "            seq_len = batch_item['seq_len'].to(opt.device)\n",
    "            \n",
    "            predictions = models[0](img, csv_feature, seq_len)\n",
    "            predictions = F.softmax(predictions.cpu(), dim=1)\n",
    "            \n",
    "        batch_result = [int(torch.argmax(prediction)) for prediction in predictions]\n",
    "        results.extend(batch_result)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53525447",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2632301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {\"dataset_path\" : \"../data/test\",\n",
    "       \"label_path\" : \"../data/train.csv\",\n",
    "       \"save_path\" : \"../pretrain\",\n",
    "       \"pretrained_path\" : \"../code/0E_64.7687_deit_small_patch16_224.pt\",\n",
    "       \"model_path\" : \"../code/0E_1.9202_deit_small_patch16_224.pt\",\n",
    "       \"voting\" : None,\n",
    "       \"batch_size\" : 16,\n",
    "       \"use_kfold\" : False,\n",
    "       \"kfold_splits\" : 4,\n",
    "       \"model_name\" : 'deit_small_patch16_224',\n",
    "       \"resize\" : 224,\n",
    "       \"num_classes\" : 25,\n",
    "       \"device\" : \"cuda\", \n",
    "       \"csv_features\" : csv_features,\n",
    "       \"max_len\" : 590,\n",
    "       \"num_features\" : len(csv_features),\n",
    "       \"epochs\" : 3}\n",
    "\n",
    "opt = EasyDict(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2d1019a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 51905/51905 [03:22<00:00, 256.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# csv_feature_dict 옵션 추가\n",
    "opt.csv_feature_dict = csv_feature_dict(opt.dataset_path, opt.csv_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d0ee5772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 5767/5767 [00:00<00:00, 2885084.82it/s]\n"
     ]
    }
   ],
   "source": [
    "label_encoder, label_decoder = label_preprocessing(opt.label_path)\n",
    "_, valid_transform = transform()\n",
    "\n",
    "test = data_split(opt.dataset_path, label_decoder, mode=\"test\")\n",
    "test_dataset = CustomDataset(test, valid_transforms, label_decoder, opt, mode='test')\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=opt.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e883c910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../code/0E_1.9202_deit_small_patch16_224.pt\n",
      "['../code/0E_1.9202_deit_small_patch16_224.pt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3245/3245 [15:37<00:00,  3.46it/s]\n"
     ]
    }
   ],
   "source": [
    "print(opt.model_path)\n",
    "models_path = glob(opt.model_path)\n",
    "\n",
    "print(models_path)\n",
    "models = []\n",
    "if len(models_path) != 1 :\n",
    "    for idx, kfold_model_path in enumerate(models_path) :\n",
    "\n",
    "        model = CNN2RNN(opt).to(opt.device)\n",
    "\n",
    "        model.load_state_dict(torch.load(kfold_model_path, map_location=opt.device))\n",
    "        model.to(opt.device).eval()\n",
    "        models.append(model)\n",
    "        \n",
    "elif len(models_path) == 1 :\n",
    "    model = CNN2RNN(opt).to(opt.device)\n",
    "    model.load_state_dict(torch.load(models_path[0], map_location=opt.device))\n",
    "    model.to(opt.device).eval()\n",
    "    models.append(model)\n",
    "\n",
    "# models[0]\n",
    "preds = predict(test_loader, models, opt, voting=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f5eee",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c94f1161",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_answ = np.array([label_encoder[int(val)] for val in preds])\n",
    "submission_csv = pd.read_csv('../data/sample_submission.csv')\n",
    "submission_csv['label'] = preds_answ\n",
    "submission_csv.to_csv('./testtestestsets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da68aff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
