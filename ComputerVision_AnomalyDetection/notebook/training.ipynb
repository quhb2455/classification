{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4844775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from easydict import EasyDict\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d2e2c2",
   "metadata": {},
   "source": [
    "# custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa994213",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_list, label_list=None, transforms=None, mode=\"train\") :\n",
    "        self.img_list = img_list\n",
    "        \n",
    "        if mode == \"train\" : \n",
    "            self.label_list = self.label_encoder(label_list)\n",
    "            \n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_list[idx]\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        try :\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        except :\n",
    "            print(img_path)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            print(img)\n",
    "            print(img.shape)\n",
    "            \n",
    "        if self.transforms:            \n",
    "            img = self.transforms(image=img)['image']\n",
    "        \n",
    "        if self.mode == \"train\" :\n",
    "            label = self.label_list[idx]\n",
    "            return img, torch.tensor(label)\n",
    "        \n",
    "        elif self.mode == \"test\" :\n",
    "            return img\n",
    "    \n",
    "    def label_encoder(self, label_list) :\n",
    "        label_enc = {k : i for i, k in enumerate(sorted(list(set(label_list))))}\n",
    "#         display(label_enc)\n",
    "        return [label_enc[label] for label in label_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d536b0c7",
   "metadata": {},
   "source": [
    "#### test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3b5b4e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/aug_train_df.csv')\n",
    "# transforms = A.Compose([\n",
    "#     A.Resize(224,224),\n",
    "#     A.Normalize(),\n",
    "#     A.Rotate(),\n",
    "#     ToTensorV2()\n",
    "# ])\n",
    "# db = CustomDataset(list(df['file_name']), list(df['label']), transforms, mode=\"train\")\n",
    "# db_loader = DataLoader(db, batch_size=16, shuffle=True)\n",
    "# for img, label in db_loader : \n",
    "#     print(img.shape)\n",
    "#     print(label.shape)\n",
    "#     print(label)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7529bbf6",
   "metadata": {},
   "source": [
    "# Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce0c907f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class FocalLoss(nn.Module) :\n",
    "#     def __init__(self, alpha=2, gamma=2, logits=False, reduction='none') :\n",
    "#         super(FocalLoss, self).__init__()\n",
    "#         self.alpha = alpha\n",
    "#         self.gamma = gamma\n",
    "#         self.logits = logits\n",
    "#         self.reduction = reduction\n",
    "\n",
    "#     def forward(self, inputs, targets) :\n",
    "#         ce_loss = nn.CrossEntropyLoss(reduction=self.reduction)(inputs, targets)\n",
    "#         pt = torch.exp(-ce_loss)\n",
    "#         F_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "\n",
    "#         if self.reduction :\n",
    "#             return torch.mean(F_loss)\n",
    "#         else :\n",
    "#             return F_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c96004",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63704164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module) :\n",
    "    def __init__(self, model_name, num_classes) :\n",
    "        super(CNN, self).__init__()\n",
    "        self.model = timm.create_model(model_name=model_name, num_classes=num_classes, pretrained=True)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b915b40",
   "metadata": {},
   "source": [
    "# Custom SwinTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a393a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackBone(nn.Module) :\n",
    "    def __init__(self, model_name, backbone_output) :\n",
    "        super(BackBone, self).__init__()\n",
    "        self.model = timm.create_model(model_name=model_name, num_classes=backbone_output, pretrained=True)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "    \n",
    "class MLP(nn.Module) :\n",
    "    def __init__(self, in_features, dropout_rate, num_state) :\n",
    "        super(MLP, self).__init__()\n",
    "        #forward_features 시 LayerNorm까지 통과한 결과임\n",
    "        # 따라서 LayerNorm 와 AdaptiveAvgPool1d는 필요없음    \n",
    "\n",
    "        self.linear_1 = nn.Linear(in_features, in_features//2, bias=True)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate, inplace=False)\n",
    "        self.linear_2 = nn.Linear(in_features//2, num_state, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "    \n",
    "class CustomSwinTransformer(nn.Module) :\n",
    "    def __init__(self, \n",
    "                 model_path, \n",
    "                 model_name, \n",
    "                 backbone_output, \n",
    "                 num_class, \n",
    "                 num_state,\n",
    "                 dropout_rate=0.5) :\n",
    "        super(CustomSwinTransformer, self).__init__()\n",
    "        \n",
    "        self.backbone = self.get_backbone(model_path,\n",
    "                                         model_name,\n",
    "                                         backbone_output)\n",
    "        \n",
    "        # num_state + 1을 해준 이유 = None Class를 추가할 예정이기 때문\n",
    "        self.mlps = nn.ModuleList([MLP(in_features=1024, \n",
    "                         dropout_rate=dropout_rate, \n",
    "                         num_state = num_state[i] + 1) for i in range(num_class)])\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        preds = []\n",
    "        feature_map = self.backbone.forward_features(x)\n",
    "        for mlp in self.mlps :\n",
    "            preds.append(mlp(feature_map))\n",
    "        return preds\n",
    "    \n",
    "    def get_backbone(self, model_path, model_name, backbone_output) :\n",
    "        checkpoint = torch.load(model_path)\n",
    "        backbone = BackBone(model_name, backbone_output)\n",
    "        backbone.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        return backbone.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86207f15",
   "metadata": {},
   "source": [
    "#### test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "994ab65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN(\"efficientnetv2_s\", 88)\n",
    "# test_data = torch.randn(1, 3, 666,666)\n",
    "# output = model(test_data)\n",
    "# output = F.softmax(output)\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b2d98b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d0bc62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def cutmix(imgs, labels):\n",
    "    lam = np.random.beta(1.0, 1.0)\n",
    "    rand_index = torch.randperm(imgs.size()[0]).cuda()\n",
    "    target_a = labels\n",
    "    target_b = labels[rand_index]\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(imgs.size(), lam)\n",
    "    imgs[:, :, bbx1:bbx2, bby1:bby2] = imgs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "    # adjust lambda to exactly match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (imgs.size()[-1] * imgs.size()[-2]))\n",
    "\n",
    "    return imgs, target_a, target_b, lam\n",
    "\n",
    "def mixup(imgs, labels, alpha=1.0) :\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    rand_index = torch.randperm(imgs.size()[0]).cuda()\n",
    "    mixed_imgs = lam * imgs + (1 - lam) * imgs[rand_index, :]\n",
    "    target_a, target_b = labels, labels[rand_index]\n",
    "    \n",
    "    return mixed_imgs, lam, target_a, target_b\n",
    "\n",
    "def accuracy_function(real, pred):    \n",
    "    real = real.cpu()\n",
    "    pred = torch.argmax(pred, dim=1).cpu()\n",
    "    score = f1_score(real, pred, average='macro')\n",
    "    return score\n",
    "\n",
    "def training(model, train_loader, valid_loader, opt) :\n",
    "        \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr= opt.learning_rate,\n",
    "                                 weight_decay=opt.weight_decay)\n",
    "    scheduler = CosineAnnealingLR(optimizer, \n",
    "                                  T_max=opt.cosine_lr_Tmax, \n",
    "                                  eta_min=opt.cosine_lr_eta_min)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "#     criterion = FocalLoss()\n",
    "    \n",
    "    if opt.resume : \n",
    "        model_checkpoint = torch.load(opt.model_path)\n",
    "        model.load_state_dict(model_checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(model_checkpoint[\"optimizer_state_dict\"])\n",
    "        scheduler = CosineAnnealingLR(optimizer, \n",
    "                                  T_max=opt.cosine_lr_Tmax, \n",
    "                                  eta_min=opt.cosine_lr_eta_min)\n",
    "        \n",
    "        opt.start_epoch = model_checkpoint[\"epoch\"]\n",
    "    else : \n",
    "        opt.start_epoch = 0\n",
    "        \n",
    "    early_stop_step = 0\n",
    "    best_loss = 10\n",
    "    for E in range(opt.start_epoch + 1, opt.epochs + 1) :\n",
    "            # train\n",
    "            tqdm_train = tqdm(train_loader)\n",
    "            train_loss, train_f1 = 0, 0\n",
    "            for batch, (img, label) in enumerate(tqdm_train, start=1) :\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                img = img.to(opt.device)\n",
    "                label = label.to(opt.device)\n",
    "                \n",
    "                if opt.cutmix :\n",
    "                    imgs, target_a, target_b, lam = cutmix(img, label)\n",
    "                    output = model(imgs)\n",
    "                    loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1. - lam)\n",
    "                \n",
    "                elif opt.mixup :\n",
    "                    mixed_imgs, lam, target_a, target_b = mixup(img, label)\n",
    "                    output = model(mixed_imgs)\n",
    "                    loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1. - lam)                    \n",
    "                \n",
    "                else:\n",
    "                    output = model(img)\n",
    "                    loss = criterion(output, label)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                score = accuracy_function(label, output)\n",
    "                train_loss += loss.item()\n",
    "                train_f1 += score\n",
    "                tqdm_train.set_postfix({\"Epoch\" : E,\n",
    "                                \"Mean train loss\" : \"{:06f}\".format(train_loss / (batch)),\n",
    "                                \"Mean train f1\" : \"{:06f}\".format(train_f1 / (batch))\n",
    "                               })\n",
    "            # validation\n",
    "            tqdm_valid = tqdm(valid_loader)\n",
    "            valid_loss, valid_f1 = 0, 0\n",
    "            for batch, (img, label) in enumerate(tqdm_valid, start=1) :\n",
    "                model.eval()\n",
    "                \n",
    "                img = img.to(opt.device)\n",
    "                label = label.to(opt.device)\n",
    "                \n",
    "                with torch.no_grad() :\n",
    "                    output = model(img)\n",
    "                    loss = criterion(output, label)\n",
    "                    \n",
    "                score = accuracy_function(label, output)\n",
    "                valid_loss += loss.item()\n",
    "                valid_f1 += score\n",
    "                tqdm_valid.set_postfix({\n",
    "                    \"Mean valid loss\": \"{:06f}\".format(valid_loss / (batch)),\n",
    "                    \"Mean valid f1\": \"{:06f}\".format(valid_f1 / (batch))\n",
    "                    })\n",
    "            \n",
    "            # scheduler\n",
    "            scheduler.step()\n",
    "            \n",
    "            mean_valid_loss = valid_loss / batch\n",
    "            if mean_valid_loss < best_loss :\n",
    "                early_stop_step = 0\n",
    "                best_loss = mean_valid_loss\n",
    "                torch.save({\n",
    "                    \"epoch\" : E,\n",
    "                    \"model_state_dict\" : model.state_dict(),\n",
    "                    \"optimizer_state_dict\" : optimizer.state_dict()\n",
    "                }, \n",
    "                           os.path.join(opt.save_path, f'{E}E_{mean_valid_loss:0.4f}_{opt.model_name}.pt'))\n",
    "            \n",
    "            elif mean_valid_loss > best_loss : \n",
    "                early_stop_step += 1\n",
    "                print(f\"Early Stopping Step : [{early_stop_step} / {opt.early_stopping}]\")\n",
    "            \n",
    "            if early_stop_step == opt.early_stopping :\n",
    "                print(\"=== Early Stop ===\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52f5d7d",
   "metadata": {},
   "source": [
    "# Run - efficientNetV2_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ff1977c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# opt = {\n",
    "#     \"df_path\" : \"../data/aug_train_df.csv\",\n",
    "#     \"save_path\" : \"../model\",\n",
    "#     \"model_name\" : \"efficientnetv2_s\",\n",
    "#     \"num_classes\" : 88,\n",
    "#     \"resize\" : 300,\n",
    "#     \"device\" : \"cuda:0\",\n",
    "#     \"early_stopping\" : 5,\n",
    "#     \"epochs\" : 100,\n",
    "#     \"batch_size\" : 32,\n",
    "#     \"learning_rate\" : 1e-4,\n",
    "#     \"cosine_lr_Tmax\" : 20,\n",
    "#     \"cosine_lr_eta_min\" : 1e-5,\n",
    "#     \"cutmix\" : False,\n",
    "#     \"resume\" : False,\n",
    "#     \"model_path\" : \"../model/6E_0.1491_coat_mini.pt\"\n",
    "# }\n",
    "# opt = EasyDict(opt)\n",
    "# os.makedirs(opt.save_path, exist_ok=True)\n",
    "\n",
    "# t_transforms = A.Compose([\n",
    "#     A.Normalize(),\n",
    "#     A.Resize(opt.resize, opt.resize),\n",
    "#     A.Blur(p=0.7),\n",
    "#     A.Rotate(limit=(-270, 270), p=1),\n",
    "#     A.OneOf([\n",
    "#         A.HorizontalFlip(),\n",
    "#         A.VerticalFlip()\n",
    "#     ], p=1),\n",
    "#     ToTensorV2()\n",
    "# ])\n",
    "\n",
    "# v_transforms = A.Compose([\n",
    "#     A.Normalize(),\n",
    "#     A.Resize(opt.resize, opt.resize),\n",
    "#     ToTensorV2()\n",
    "# ])\n",
    "\n",
    "# train_df = pd.read_csv(opt.df_path)\n",
    "# t_imgs, v_imgs, t_labels, v_labels = train_test_split(\n",
    "#     list(train_df['file_name']),\n",
    "#     list(train_df['label']),\n",
    "#     train_size=0.8,\n",
    "#     shuffle=True,\n",
    "#     random_state=51,\n",
    "#     stratify=list(train_df['label']))\n",
    "\n",
    "\n",
    "# train_data = CustomDataset(t_imgs, t_labels, t_transforms)\n",
    "# valid_data = CustomDataset(v_imgs, v_labels, v_transforms)\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=opt.batch_size, shuffle=True)\n",
    "# valid_loader = DataLoader(valid_data, batch_size=opt.batch_size, shuffle=True)\n",
    "\n",
    "# model = CNN(opt.model_name, opt.num_classes).to(opt.device)\n",
    "# training(model, train_loader, valid_loader, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71c8760",
   "metadata": {},
   "source": [
    "# Run - coat_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f2dfea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = {\n",
    "#     \"df_path\" : \"../data/aug_train_df.csv\",\n",
    "#     \"save_path\" : \"../model/coat_mini\",\n",
    "#     \"model_name\" : \"coat_mini\",\n",
    "#     \"num_classes\" : 88,\n",
    "#     \"resize\" : 224,\n",
    "#     \"device\" : \"cuda:0\",\n",
    "#     \"early_stopping\" : 5,\n",
    "#     \"epochs\" : 25,\n",
    "#     \"batch_size\" : 32,\n",
    "#     \"learning_rate\" : 1e-4,\n",
    "#     \"cosine_lr_Tmax\" : 20,\n",
    "#     \"cosine_lr_eta_min\" : 1e-5,\n",
    "#     \"cutmix\" : False,\n",
    "#     \"resume\" : True,\n",
    "#     \"model_path\" : \"../model/coat_mini/9E_0.0738_coat_mini.pt\"\n",
    "# }\n",
    "# opt = EasyDict(opt)\n",
    "# os.makedirs(opt.save_path, exist_ok=True)\n",
    "\n",
    "# t_transforms = A.Compose([\n",
    "#     A.Normalize(),\n",
    "#     A.Resize(opt.resize, opt.resize),\n",
    "#     A.Blur(p=0.7),\n",
    "#     A.Rotate(limit=(-270, 270), p=1),\n",
    "#     A.OneOf([\n",
    "#         A.HorizontalFlip(),\n",
    "#         A.VerticalFlip()\n",
    "#     ], p=1),\n",
    "#     ToTensorV2()\n",
    "# ])\n",
    "\n",
    "# v_transforms = A.Compose([\n",
    "#     A.Normalize(),\n",
    "#     A.Resize(opt.resize, opt.resize),\n",
    "#     ToTensorV2()\n",
    "# ])\n",
    "\n",
    "# train_df = pd.read_csv(opt.df_path)\n",
    "# t_imgs, v_imgs, t_labels, v_labels = train_test_split(\n",
    "#     list(train_df['file_name']),\n",
    "#     list(train_df['label']),\n",
    "#     train_size=0.8,\n",
    "#     shuffle=True,\n",
    "#     random_state=51,\n",
    "#     stratify=list(train_df['label']))\n",
    "\n",
    "\n",
    "# train_data = CustomDataset(t_imgs, t_labels, t_transforms)\n",
    "# valid_data = CustomDataset(v_imgs, v_labels, v_transforms)\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=opt.batch_size, shuffle=True)\n",
    "# valid_loader = DataLoader(valid_data, batch_size=opt.batch_size, shuffle=True)\n",
    "\n",
    "# model = CNN(opt.model_name, opt.num_classes).to(opt.device)\n",
    "    \n",
    "# training(model, train_loader, valid_loader, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1b6c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# model.cpu()\n",
    "# del model\n",
    "# del train_data\n",
    "# del valid_data\n",
    "# del train_loader\n",
    "# del valid_loader\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f4dc2c",
   "metadata": {},
   "source": [
    "# Run - coat_mini w/ aug_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e922611a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████| 441/441 [11:52<00:00,  1.61s/it, Epoch=6, Mean train loss=0.094364, Mean train f1=0.948913]\n",
      "100%|██████████████████████████████| 111/111 [01:57<00:00,  1.06s/it, Mean valid loss=0.062684, Mean valid f1=0.964179]\n",
      "100%|█████████████████████| 441/441 [11:41<00:00,  1.59s/it, Epoch=7, Mean train loss=0.086221, Mean train f1=0.952773]\n",
      "100%|██████████████████████████████| 111/111 [01:58<00:00,  1.07s/it, Mean valid loss=0.080635, Mean valid f1=0.954950]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████| 441/441 [11:52<00:00,  1.62s/it, Epoch=8, Mean train loss=0.071946, Mean train f1=0.962618]\n",
      "100%|██████████████████████████████| 111/111 [02:00<00:00,  1.09s/it, Mean valid loss=0.074153, Mean valid f1=0.958979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████| 441/441 [11:52<00:00,  1.61s/it, Epoch=9, Mean train loss=0.068379, Mean train f1=0.961057]\n",
      "100%|██████████████████████████████| 111/111 [02:00<00:00,  1.08s/it, Mean valid loss=0.053328, Mean valid f1=0.971511]\n",
      "100%|████████████████████| 441/441 [11:54<00:00,  1.62s/it, Epoch=10, Mean train loss=0.070953, Mean train f1=0.963441]\n",
      " 86%|██████████████████████████▊    | 96/111 [01:44<00:16,  1.09s/it, Mean valid loss=0.070124, Mean valid f1=0.969440]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train\\aug_v2\\screw-thread_side\\mixup_2_10019.png\n",
      "[[[201 201 201]\n",
      "  [203 203 203]\n",
      "  [201 201 201]\n",
      "  ...\n",
      "  [210 210 210]\n",
      "  [210 210 210]\n",
      "  [209 209 209]]\n",
      "\n",
      " [[200 200 200]\n",
      "  [202 202 202]\n",
      "  [200 200 200]\n",
      "  ...\n",
      "  [210 210 210]\n",
      "  [209 209 209]\n",
      "  [208 208 208]]\n",
      "\n",
      " [[200 200 200]\n",
      "  [201 201 201]\n",
      "  [202 202 202]\n",
      "  ...\n",
      "  [209 209 209]\n",
      "  [208 208 208]\n",
      "  [207 207 207]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[205 205 205]\n",
      "  [206 206 206]\n",
      "  [206 206 206]\n",
      "  ...\n",
      "  [199 199 199]\n",
      "  [199 199 199]\n",
      "  [199 199 199]]\n",
      "\n",
      " [[206 206 206]\n",
      "  [207 207 207]\n",
      "  [206 206 206]\n",
      "  ...\n",
      "  [200 200 200]\n",
      "  [200 200 200]\n",
      "  [199 199 199]]\n",
      "\n",
      " [[209 209 209]\n",
      "  [208 208 208]\n",
      "  [207 207 207]\n",
      "  ...\n",
      "  [199 199 199]\n",
      "  [200 200 200]\n",
      "  [200 200 200]]]\n",
      "(1024, 1024, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 111/111 [02:00<00:00,  1.08s/it, Mean valid loss=0.071714, Mean valid f1=0.968272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 441/441 [11:52<00:00,  1.62s/it, Epoch=11, Mean train loss=0.054935, Mean train f1=0.968551]\n",
      "100%|██████████████████████████████| 111/111 [01:59<00:00,  1.08s/it, Mean valid loss=0.046592, Mean valid f1=0.978910]\n",
      "100%|████████████████████| 441/441 [11:52<00:00,  1.61s/it, Epoch=12, Mean train loss=0.046091, Mean train f1=0.975138]\n",
      "100%|██████████████████████████████| 111/111 [01:57<00:00,  1.06s/it, Mean valid loss=0.042580, Mean valid f1=0.976637]\n",
      "100%|████████████████████| 441/441 [12:03<00:00,  1.64s/it, Epoch=13, Mean train loss=0.036532, Mean train f1=0.981590]\n",
      "100%|██████████████████████████████| 111/111 [02:04<00:00,  1.12s/it, Mean valid loss=0.037757, Mean valid f1=0.983826]\n",
      "100%|████████████████████| 441/441 [12:04<00:00,  1.64s/it, Epoch=14, Mean train loss=0.029939, Mean train f1=0.985718]\n",
      "100%|██████████████████████████████| 111/111 [02:00<00:00,  1.09s/it, Mean valid loss=0.052639, Mean valid f1=0.973456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 441/441 [11:55<00:00,  1.62s/it, Epoch=15, Mean train loss=0.030142, Mean train f1=0.982964]\n",
      "100%|██████████████████████████████| 111/111 [02:00<00:00,  1.08s/it, Mean valid loss=0.024858, Mean valid f1=0.985774]\n"
     ]
    }
   ],
   "source": [
    "# opt = {\n",
    "#     \"df_path\" : \"../data/aug_v2_train_df.csv\",\n",
    "#     \"save_path\" : \"../model/coat_mini_aug_v2_CEL\",\n",
    "#     \"model_name\" : \"coat_mini\",\n",
    "#     \"num_classes\" : 88,\n",
    "#     \"resize\" : 224,\n",
    "#     \"device\" : \"cuda:0\",\n",
    "#     \"early_stopping\" : 5,\n",
    "#     \"epochs\" : 15,\n",
    "#     \"batch_size\" : 32,\n",
    "#     \"learning_rate\" : 1e-4,\n",
    "#     \"cosine_lr_Tmax\" : 20,\n",
    "#     \"cosine_lr_eta_min\" : 1e-5,\n",
    "#     \"cutmix\" : False,\n",
    "#     \"resume\" : True,\n",
    "#     \"model_path\" : \"../model/coat_mini_aug_v2_CEL/5E_0.0848_coat_mini.pt\"\n",
    "# }\n",
    "# opt = EasyDict(opt)\n",
    "# os.makedirs(opt.save_path, exist_ok=True)\n",
    "\n",
    "# t_transforms = A.Compose([\n",
    "#     A.Normalize(),\n",
    "#     A.Resize(opt.resize, opt.resize),\n",
    "#     A.Blur(p=0.7),\n",
    "#     A.Rotate(limit=(-270, 270), p=1),\n",
    "#     A.OneOf([\n",
    "#         A.HorizontalFlip(),\n",
    "#         A.VerticalFlip()\n",
    "#     ], p=1),\n",
    "#     ToTensorV2()\n",
    "# ])\n",
    "\n",
    "# v_transforms = A.Compose([\n",
    "#     A.Normalize(),\n",
    "#     A.Resize(opt.resize, opt.resize),\n",
    "#     ToTensorV2()\n",
    "# ])\n",
    "\n",
    "# train_df = pd.read_csv(opt.df_path)\n",
    "# t_imgs, v_imgs, t_labels, v_labels = train_test_split(\n",
    "#     list(train_df['file_name']),\n",
    "#     list(train_df['label']),\n",
    "#     train_size=0.8,\n",
    "#     shuffle=True,\n",
    "#     random_state=51,\n",
    "#     stratify=list(train_df['label']))\n",
    "\n",
    "\n",
    "# train_data = CustomDataset(t_imgs, t_labels, t_transforms)\n",
    "# valid_data = CustomDataset(v_imgs, v_labels, v_transforms)\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=opt.batch_size, shuffle=True)\n",
    "# valid_loader = DataLoader(valid_data, batch_size=opt.batch_size, shuffle=True)\n",
    "\n",
    "# model = CNN(opt.model_name, opt.num_classes).to(opt.device)\n",
    "    \n",
    "# training(model, train_loader, valid_loader, opt)\n",
    "\n",
    "# print(\"==== Complete ====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e29afb",
   "metadata": {},
   "source": [
    "# Run - coat_mini w/ aug_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1caff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████| 512/512 [14:15<00:00,  1.67s/it, Epoch=1, Mean train loss=1.133451, Mean train f1=0.570951]\n",
      "100%|██████████████████████████████| 128/128 [02:30<00:00,  1.17s/it, Mean valid loss=0.338896, Mean valid f1=0.846434]\n",
      "100%|█████████████████████| 512/512 [13:46<00:00,  1.61s/it, Epoch=2, Mean train loss=0.189016, Mean train f1=0.912570]\n",
      "100%|██████████████████████████████| 128/128 [02:18<00:00,  1.08s/it, Mean valid loss=0.251873, Mean valid f1=0.879343]\n",
      "100%|█████████████████████| 512/512 [13:40<00:00,  1.60s/it, Epoch=3, Mean train loss=0.095582, Mean train f1=0.951464]\n",
      "100%|██████████████████████████████| 128/128 [02:18<00:00,  1.08s/it, Mean valid loss=0.126928, Mean valid f1=0.928785]\n",
      "100%|█████████████████████| 512/512 [13:38<00:00,  1.60s/it, Epoch=4, Mean train loss=0.058838, Mean train f1=0.971937]\n",
      "100%|██████████████████████████████| 128/128 [02:19<00:00,  1.09s/it, Mean valid loss=0.168830, Mean valid f1=0.912340]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████| 512/512 [13:37<00:00,  1.60s/it, Epoch=5, Mean train loss=0.042170, Mean train f1=0.978022]\n",
      "100%|██████████████████████████████| 128/128 [02:18<00:00,  1.08s/it, Mean valid loss=0.091073, Mean valid f1=0.955261]\n",
      "100%|█████████████████████| 512/512 [13:37<00:00,  1.60s/it, Epoch=6, Mean train loss=0.040044, Mean train f1=0.980119]\n",
      "100%|██████████████████████████████| 128/128 [02:18<00:00,  1.08s/it, Mean valid loss=0.056357, Mean valid f1=0.967770]\n",
      "100%|█████████████████████| 512/512 [13:39<00:00,  1.60s/it, Epoch=7, Mean train loss=0.025724, Mean train f1=0.987839]\n",
      "100%|██████████████████████████████| 128/128 [02:18<00:00,  1.08s/it, Mean valid loss=0.061417, Mean valid f1=0.966731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████| 512/512 [13:38<00:00,  1.60s/it, Epoch=8, Mean train loss=0.026787, Mean train f1=0.987034]\n",
      "100%|██████████████████████████████| 128/128 [02:18<00:00,  1.08s/it, Mean valid loss=0.065773, Mean valid f1=0.960392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████| 512/512 [13:37<00:00,  1.60s/it, Epoch=9, Mean train loss=0.026573, Mean train f1=0.987955]\n",
      "100%|██████████████████████████████| 128/128 [02:18<00:00,  1.08s/it, Mean valid loss=0.063512, Mean valid f1=0.968032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [3 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 512/512 [13:37<00:00,  1.60s/it, Epoch=10, Mean train loss=0.018340, Mean train f1=0.991154]\n",
      "100%|██████████████████████████████| 128/128 [02:18<00:00,  1.08s/it, Mean valid loss=0.079449, Mean valid f1=0.955973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [4 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 512/512 [13:37<00:00,  1.60s/it, Epoch=11, Mean train loss=0.008917, Mean train f1=0.995069]\n",
      "100%|██████████████████████████████| 128/128 [02:18<00:00,  1.08s/it, Mean valid loss=0.044160, Mean valid f1=0.972664]\n",
      "100%|████████████████████| 512/512 [13:37<00:00,  1.60s/it, Epoch=12, Mean train loss=0.008879, Mean train f1=0.996316]\n",
      " 74%|███████████████████████        | 95/128 [01:42<00:35,  1.09s/it, Mean valid loss=0.037978, Mean valid f1=0.978461]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train\\aug_v3\\cable-poke_insulation\\mixup_1_11637.png\n",
      "[[[ 82 112 139]\n",
      "  [ 83 113 144]\n",
      "  [ 84 113 147]\n",
      "  ...\n",
      "  [ 75 103 127]\n",
      "  [ 76 104 129]\n",
      "  [ 78 105 132]]\n",
      "\n",
      " [[ 83 113 141]\n",
      "  [ 83 113 140]\n",
      "  [ 84 112 143]\n",
      "  ...\n",
      "  [ 78 102 128]\n",
      "  [ 78 104 130]\n",
      "  [ 78 105 132]]\n",
      "\n",
      " [[ 84 114 139]\n",
      "  [ 85 114 139]\n",
      "  [ 84 113 140]\n",
      "  ...\n",
      "  [ 78 103 132]\n",
      "  [ 78 104 131]\n",
      "  [ 79 106 126]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 62  80 101]\n",
      "  [ 63  83 103]\n",
      "  [ 66  85 105]\n",
      "  ...\n",
      "  [ 97 139 168]\n",
      "  [ 95 137 172]\n",
      "  [ 96 137 170]]\n",
      "\n",
      " [[ 64  83 102]\n",
      "  [ 64  84 102]\n",
      "  [ 65  86 107]\n",
      "  ...\n",
      "  [ 98 138 167]\n",
      "  [ 97 138 172]\n",
      "  [ 96 137 167]]\n",
      "\n",
      " [[ 66  85 105]\n",
      "  [ 66  86 105]\n",
      "  [ 67  87 111]\n",
      "  ...\n",
      "  [ 97 137 167]\n",
      "  [ 96 137 176]\n",
      "  [ 95 134 166]]]\n",
      "(1024, 1024, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████| 128/128 [02:18<00:00,  1.08s/it, Mean valid loss=0.039812, Mean valid f1=0.977612]\n",
      "100%|████████████████████| 512/512 [13:38<00:00,  1.60s/it, Epoch=13, Mean train loss=0.006019, Mean train f1=0.997383]\n",
      "100%|██████████████████████████████| 128/128 [02:18<00:00,  1.08s/it, Mean valid loss=0.038480, Mean valid f1=0.979249]\n",
      "100%|████████████████████| 512/512 [13:37<00:00,  1.60s/it, Epoch=14, Mean train loss=0.009647, Mean train f1=0.996660]\n",
      "100%|██████████████████████████████| 128/128 [02:18<00:00,  1.08s/it, Mean valid loss=0.050805, Mean valid f1=0.970108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 512/512 [13:37<00:00,  1.60s/it, Epoch=15, Mean train loss=0.003375, Mean train f1=0.998583]\n",
      "100%|██████████████████████████████| 128/128 [02:18<00:00,  1.08s/it, Mean valid loss=0.042192, Mean valid f1=0.978982]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 512/512 [13:37<00:00,  1.60s/it, Epoch=16, Mean train loss=0.005947, Mean train f1=0.997608]\n",
      "100%|██████████████████████████████| 128/128 [02:18<00:00,  1.08s/it, Mean valid loss=0.037625, Mean valid f1=0.977259]\n",
      "100%|████████████████████| 512/512 [13:39<00:00,  1.60s/it, Epoch=17, Mean train loss=0.002849, Mean train f1=0.998541]\n",
      "100%|██████████████████████████████| 128/128 [02:19<00:00,  1.09s/it, Mean valid loss=0.030221, Mean valid f1=0.981207]\n",
      "100%|████████████████████| 512/512 [13:44<00:00,  1.61s/it, Epoch=18, Mean train loss=0.001050, Mean train f1=0.999720]\n",
      "100%|██████████████████████████████| 128/128 [02:18<00:00,  1.08s/it, Mean valid loss=0.032288, Mean valid f1=0.981493]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 512/512 [13:38<00:00,  1.60s/it, Epoch=19, Mean train loss=0.001040, Mean train f1=0.999552]\n",
      "100%|██████████████████████████████| 128/128 [02:18<00:00,  1.08s/it, Mean valid loss=0.041419, Mean valid f1=0.978189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████| 512/512 [13:38<00:00,  1.60s/it, Epoch=20, Mean train loss=0.000972, Mean train f1=0.999907]\n",
      "100%|██████████████████████████████| 128/128 [02:18<00:00,  1.08s/it, Mean valid loss=0.037915, Mean valid f1=0.981300]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [3 / 5]\n",
      "==== Complete ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# opt = {\n",
    "#     \"df_path\" : \"../data/aug_v3_train_df.csv\",\n",
    "#     \"save_path\" : \"../model/coat_mini_aug_v3_CEL\",\n",
    "#     \"model_name\" : \"coat_mini\",\n",
    "#     \"num_classes\" : 88,\n",
    "#     \"resize\" : 224,\n",
    "#     \"device\" : \"cuda:0\",\n",
    "#     \"early_stopping\" : 5,\n",
    "#     \"epochs\" : 20,\n",
    "#     \"batch_size\" : 32,\n",
    "#     \"learning_rate\" : 1e-4,\n",
    "#     \"cosine_lr_Tmax\" : 20,\n",
    "#     \"cosine_lr_eta_min\" : 1e-5,\n",
    "#     \"cutmix\" : False,\n",
    "#     \"resume\" : False,\n",
    "#     \"model_path\" : \"../model/coat_mini_aug_v2_CEL/5E_0.0848_coat_mini.pt\"\n",
    "# }\n",
    "# opt = EasyDict(opt)\n",
    "# os.makedirs(opt.save_path, exist_ok=True)\n",
    "\n",
    "# t_transforms = A.Compose([\n",
    "#     A.Normalize(),\n",
    "#     A.Resize(opt.resize, opt.resize),\n",
    "# #     A.Blur(p=0.7),\n",
    "#     A.Rotate(limit=(45), p=1),\n",
    "#     A.OneOf([\n",
    "#         A.HorizontalFlip(),\n",
    "#         A.VerticalFlip()\n",
    "#     ], p=1),\n",
    "#     ToTensorV2()\n",
    "# ])\n",
    "\n",
    "# v_transforms = A.Compose([\n",
    "#     A.Normalize(),\n",
    "#     A.Resize(opt.resize, opt.resize),\n",
    "#     ToTensorV2()\n",
    "# ])\n",
    "\n",
    "# train_df = pd.read_csv(opt.df_path)\n",
    "# t_imgs, v_imgs, t_labels, v_labels = train_test_split(\n",
    "#     list(train_df['file_name']),\n",
    "#     list(train_df['label']),\n",
    "#     train_size=0.8,\n",
    "#     shuffle=True,\n",
    "#     random_state=51,\n",
    "#     stratify=list(train_df['label']))\n",
    "\n",
    "\n",
    "# train_data = CustomDataset(t_imgs, t_labels, t_transforms)\n",
    "# valid_data = CustomDataset(v_imgs, v_labels, v_transforms)\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=opt.batch_size, shuffle=True)\n",
    "# valid_loader = DataLoader(valid_data, batch_size=opt.batch_size, shuffle=True)\n",
    "\n",
    "# model = CNN(opt.model_name, opt.num_classes).to(opt.device)\n",
    "    \n",
    "# training(model, train_loader, valid_loader, opt)\n",
    "\n",
    "# print(\"==== Complete ====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a806ee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "model.cpu()\n",
    "del model\n",
    "del train_data\n",
    "del valid_data\n",
    "del train_loader\n",
    "del valid_loader\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0303bae0",
   "metadata": {},
   "source": [
    "# Run - SwinTransformer w/ aug_v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e6ac5a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sub\\.conda\\envs\\torch-1.11\\lib\\site-packages\\torch\\functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|███████████████████| 1018/1018 [23:28<00:00,  1.38s/it, Epoch=1, Mean train loss=1.751818, Mean train f1=0.255129]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.397987, Mean valid f1=0.809560]\n",
      " 12%|██▍                 | 127/1018 [02:53<20:17,  1.37s/it, Epoch=2, Mean train loss=1.305180, Mean train f1=0.339653]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train\\aug_v5\\bottle-contamination\\aug_18_10759.png\n",
      "[[[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[117  75  62]\n",
      "  [124  77  63]\n",
      "  [124  79  60]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[113  76  62]\n",
      "  [120  78  62]\n",
      "  [119  80  61]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[111  76  64]\n",
      "  [114  78  63]\n",
      "  [115  79  63]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]]\n",
      "(900, 900, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████| 1018/1018 [23:22<00:00,  1.38s/it, Epoch=2, Mean train loss=1.241543, Mean train f1=0.372965]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.253121, Mean valid f1=0.872212]\n",
      "100%|███████████████████| 1018/1018 [23:24<00:00,  1.38s/it, Epoch=3, Mean train loss=1.118839, Mean train f1=0.398421]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.174458, Mean valid f1=0.913646]\n",
      "100%|███████████████████| 1018/1018 [23:23<00:00,  1.38s/it, Epoch=4, Mean train loss=1.036602, Mean train f1=0.402771]\n",
      "100%|██████████████████████████████| 255/255 [04:09<00:00,  1.02it/s, Mean valid loss=0.161981, Mean valid f1=0.924002]\n",
      "100%|███████████████████| 1018/1018 [23:21<00:00,  1.38s/it, Epoch=5, Mean train loss=1.000440, Mean train f1=0.438458]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.129871, Mean valid f1=0.934118]\n",
      "100%|███████████████████| 1018/1018 [23:23<00:00,  1.38s/it, Epoch=6, Mean train loss=0.945333, Mean train f1=0.437643]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.124815, Mean valid f1=0.948588]\n",
      "100%|███████████████████| 1018/1018 [23:22<00:00,  1.38s/it, Epoch=7, Mean train loss=0.918787, Mean train f1=0.417947]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.092129, Mean valid f1=0.967422]\n",
      "100%|███████████████████| 1018/1018 [23:21<00:00,  1.38s/it, Epoch=8, Mean train loss=0.877474, Mean train f1=0.441753]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.108119, Mean valid f1=0.954162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████| 1018/1018 [23:23<00:00,  1.38s/it, Epoch=9, Mean train loss=0.850858, Mean train f1=0.443841]\n",
      "100%|██████████████████████████████| 255/255 [04:07<00:00,  1.03it/s, Mean valid loss=0.076973, Mean valid f1=0.974431]\n",
      "100%|██████████████████| 1018/1018 [23:22<00:00,  1.38s/it, Epoch=10, Mean train loss=0.823434, Mean train f1=0.471229]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.084904, Mean valid f1=0.966505]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████| 1018/1018 [23:21<00:00,  1.38s/it, Epoch=11, Mean train loss=0.793115, Mean train f1=0.453100]\n",
      "100%|██████████████████████████████| 255/255 [04:09<00:00,  1.02it/s, Mean valid loss=0.064208, Mean valid f1=0.975928]\n",
      "100%|██████████████████| 1018/1018 [23:25<00:00,  1.38s/it, Epoch=12, Mean train loss=0.763741, Mean train f1=0.476994]\n",
      "100%|██████████████████████████████| 255/255 [04:09<00:00,  1.02it/s, Mean valid loss=0.050924, Mean valid f1=0.979366]\n",
      "100%|██████████████████| 1018/1018 [23:25<00:00,  1.38s/it, Epoch=13, Mean train loss=0.759044, Mean train f1=0.485229]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.050954, Mean valid f1=0.981786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████| 1018/1018 [23:21<00:00,  1.38s/it, Epoch=14, Mean train loss=0.746793, Mean train f1=0.474233]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.041918, Mean valid f1=0.987184]\n",
      "100%|██████████████████| 1018/1018 [23:22<00:00,  1.38s/it, Epoch=15, Mean train loss=0.734085, Mean train f1=0.486780]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.038461, Mean valid f1=0.987341]\n",
      "100%|██████████████████| 1018/1018 [23:22<00:00,  1.38s/it, Epoch=16, Mean train loss=0.713862, Mean train f1=0.482083]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.030427, Mean valid f1=0.990257]\n",
      "100%|██████████████████| 1018/1018 [23:22<00:00,  1.38s/it, Epoch=17, Mean train loss=0.689480, Mean train f1=0.472212]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.029990, Mean valid f1=0.990974]\n",
      "100%|██████████████████| 1018/1018 [23:22<00:00,  1.38s/it, Epoch=18, Mean train loss=0.686895, Mean train f1=0.484190]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.033736, Mean valid f1=0.990298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████| 1018/1018 [23:22<00:00,  1.38s/it, Epoch=19, Mean train loss=0.681280, Mean train f1=0.496608]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.030392, Mean valid f1=0.989260]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████| 1018/1018 [23:22<00:00,  1.38s/it, Epoch=20, Mean train loss=0.672969, Mean train f1=0.508400]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.029551, Mean valid f1=0.990935]\n",
      "100%|██████████████████| 1018/1018 [23:26<00:00,  1.38s/it, Epoch=21, Mean train loss=0.677278, Mean train f1=0.489742]\n",
      "100%|██████████████████████████████| 255/255 [04:09<00:00,  1.02it/s, Mean valid loss=0.027530, Mean valid f1=0.992056]\n",
      "100%|██████████████████| 1018/1018 [23:25<00:00,  1.38s/it, Epoch=22, Mean train loss=0.669404, Mean train f1=0.475047]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.026936, Mean valid f1=0.992148]\n",
      "100%|██████████████████| 1018/1018 [23:22<00:00,  1.38s/it, Epoch=23, Mean train loss=0.666457, Mean train f1=0.482464]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.029484, Mean valid f1=0.992262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████| 1018/1018 [23:21<00:00,  1.38s/it, Epoch=24, Mean train loss=0.654196, Mean train f1=0.499701]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.030132, Mean valid f1=0.991238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████| 1018/1018 [23:21<00:00,  1.38s/it, Epoch=25, Mean train loss=0.674930, Mean train f1=0.502389]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.029178, Mean valid f1=0.992022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [3 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████| 1018/1018 [23:23<00:00,  1.38s/it, Epoch=26, Mean train loss=0.678675, Mean train f1=0.487835]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.027453, Mean valid f1=0.992319]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [4 / 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████| 1018/1018 [23:21<00:00,  1.38s/it, Epoch=27, Mean train loss=0.683802, Mean train f1=0.479744]\n",
      "100%|██████████████████████████████| 255/255 [04:08<00:00,  1.03it/s, Mean valid loss=0.029503, Mean valid f1=0.991907]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [5 / 5]\n",
      "=== Early Stop ===\n",
      "==== Complete ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "opt = {\n",
    "    \"df_path\" : \"../data/aug_v5_train_df.csv\",\n",
    "    \"save_path\" : \"../model/swin_aug_v5_mixup\",\n",
    "    \"model_name\" : \"swin_base_patch4_window7_224_in22k\",\n",
    "    \"num_classes\" : 88,\n",
    "    \"resize\" : 224,\n",
    "    \"device\" : \"cuda:0\",\n",
    "    \"early_stopping\" : 5,\n",
    "    \"epochs\" : 30,\n",
    "    \"batch_size\" : 32,\n",
    "    \"learning_rate\" : 1e-4,\n",
    "    \"weight_decay\" : 0.01,\n",
    "    \"cosine_lr_Tmax\" : 20,\n",
    "    \"cosine_lr_eta_min\" : 1e-5,\n",
    "    \"cutmix\" : False,\n",
    "    \"mixup\" : True,\n",
    "    \"resume\" : False,\n",
    "    \"model_path\" : \"../model/swin_aug_v4_mixup/21E_0.0382_swin_base_patch4_window7_224_in22k.pt\"\n",
    "}\n",
    "opt = EasyDict(opt)\n",
    "os.makedirs(opt.save_path, exist_ok=True)\n",
    "\n",
    "t_transforms = A.Compose([\n",
    "    A.Normalize(),\n",
    "    A.Resize(opt.resize, opt.resize),\n",
    "    A.Blur(p=0.7),#blur_limit=(7, 7), p=0.7),\n",
    "    A.Rotate(limit=(45), p=1),\n",
    "    A.OneOf([\n",
    "        A.HorizontalFlip(),\n",
    "        A.VerticalFlip()\n",
    "    ], p=1),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "v_transforms = A.Compose([\n",
    "    A.Normalize(),\n",
    "    A.Resize(opt.resize, opt.resize),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "train_df = pd.read_csv(opt.df_path)\n",
    "t_imgs, v_imgs, t_labels, v_labels = train_test_split(\n",
    "    list(train_df['file_name']),\n",
    "    list(train_df['label']),\n",
    "    train_size=0.8,\n",
    "    shuffle=True,\n",
    "    random_state=51,\n",
    "    stratify=list(train_df['label']))\n",
    "\n",
    "\n",
    "train_data = CustomDataset(t_imgs, t_labels, t_transforms)\n",
    "valid_data = CustomDataset(v_imgs, v_labels, v_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=opt.batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=opt.batch_size, shuffle=True)\n",
    "\n",
    "model = CNN(opt.model_name, opt.num_classes).to(opt.device)\n",
    "    \n",
    "training(model, train_loader, valid_loader, opt)\n",
    "\n",
    "print(\"==== Complete ====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a3d980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
