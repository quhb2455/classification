{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03150c0",
   "metadata": {},
   "source": [
    "## Input layer 바꾸기 -> patch embedding to linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "920ffa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting easydict\n",
      "  Downloading easydict-1.9.tar.gz (6.4 kB)\n",
      "Building wheels for collected packages: easydict\n",
      "  Building wheel for easydict (setup.py): started\n",
      "  Building wheel for easydict (setup.py): finished with status 'done'\n",
      "  Created wheel for easydict: filename=easydict-1.9-py3-none-any.whl size=6361 sha256=488619f6087cb6d9a358dea508cf3e8ca1aab6b3e5aded296726d4065e113d9b\n",
      "  Stored in directory: c:\\users\\dlfwn\\appdata\\local\\pip\\cache\\wheels\\5d\\79\\e4\\4e55effe206295359b37e0f9db3e68a1197ba396682807dadb\n",
      "Successfully built easydict\n",
      "Installing collected packages: easydict\n",
      "Successfully installed easydict-1.9\n"
     ]
    }
   ],
   "source": [
    "!pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16d56ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "375695d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.19.5\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6141ce4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.4-cp36-cp36m-win_amd64.whl (8.5 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from matplotlib) (3.0.7)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from matplotlib) (1.19.5)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.1-cp36-cp36m-win_amd64.whl (51 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.4.0-cp36-cp36m-win_amd64.whl (3.2 MB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.16.0)\n",
      "Installing collected packages: pillow, kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 kiwisolver-1.3.1 matplotlib-3.3.4 pillow-8.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aa77afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\n",
      "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.11.3-cp36-cp36m-win_amd64.whl (985 kB)\n",
      "Requirement already satisfied: torch>=1.4 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from timm) (1.10.2)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from torch>=1.4->timm) (0.8)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from torch>=1.4->timm) (4.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from torchvision->timm) (8.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from torchvision->timm) (1.19.5)\n",
      "Installing collected packages: torchvision, timm\n",
      "Successfully installed timm-0.5.4 torchvision-0.11.3\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19dfa178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp36-cp36m-win_amd64.whl (6.8 MB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-win_amd64.whl (31.2 MB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=9015b017456c1eab2f20ec05ac5a2686271fa8d4d64c083494cf63273e7bb337\n",
      "  Stored in directory: c:\\users\\dlfwn\\appdata\\local\\pip\\cache\\wheels\\23\\9d\\42\\5ec745cbbb17517000a53cecc49d6a865450d1f5cb16dc8a9c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-0.24.2 scipy-1.5.4 sklearn-0.0 threadpoolctl-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57f2b859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting albumentations\n",
      "  Downloading albumentations-1.1.0-py3-none-any.whl (102 kB)\n",
      "Requirement already satisfied: numpy>=1.11.1 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from albumentations) (1.19.5)\n",
      "Collecting PyYAML\n",
      "  Downloading PyYAML-6.0-cp36-cp36m-win_amd64.whl (153 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from albumentations) (1.5.4)\n",
      "Collecting opencv-python-headless>=4.1.1\n",
      "  Downloading opencv_python_headless-4.5.5.62-cp36-abi3-win_amd64.whl (35.3 MB)\n",
      "Collecting qudida>=0.0.4\n",
      "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
      "Collecting scikit-image>=0.16.1\n",
      "  Downloading scikit_image-0.17.2-cp36-cp36m-win_amd64.whl (11.5 MB)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from qudida>=0.0.4->albumentations) (0.24.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from qudida>=0.0.4->albumentations) (4.0.1)\n",
      "Collecting networkx>=2.0\n",
      "  Downloading networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (8.4.0)\n",
      "Collecting imageio>=2.3.0\n",
      "  Downloading imageio-2.14.1-py3-none-any.whl (3.3 MB)\n",
      "Collecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2020.9.3-py3-none-any.whl (148 kB)\n",
      "Collecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.1.1-cp36-cp36m-win_amd64.whl (4.2 MB)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from scikit-image>=0.16.1->albumentations) (3.3.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
      "Collecting decorator<5,>=4.3\n",
      "  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
      "Installing collected packages: decorator, tifffile, PyWavelets, opencv-python-headless, networkx, imageio, scikit-image, qudida, PyYAML, albumentations\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "Successfully installed PyWavelets-1.1.1 PyYAML-6.0 albumentations-1.1.0 decorator-4.4.2 imageio-2.14.1 networkx-2.5.1 opencv-python-headless-4.5.5.62 qudida-0.0.4 scikit-image-0.17.2 tifffile-2020.9.3\n"
     ]
    }
   ],
   "source": [
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f50844bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.5.5.62-cp36-abi3-win_amd64.whl (35.4 MB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from opencv-python) (1.19.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.5.5.62\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e912e7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement glob (from versions: none)\n",
      "ERROR: No matching distribution found for glob\n"
     ]
    }
   ],
   "source": [
    "!pip install glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae2ca977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\dlfwn\\.conda\\envs\\sub\\lib\\site-packages (from tqdm) (0.4.4)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.62.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65f082df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "772fc011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm \n",
    "\n",
    "_model = timm.create_model('vit_base_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1234dc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "          Identity-2             [-1, 196, 768]               0\n",
      "        PatchEmbed-3             [-1, 196, 768]               0\n",
      "           Dropout-4             [-1, 197, 768]               0\n",
      "         LayerNorm-5             [-1, 197, 768]           1,536\n",
      "            Linear-6            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-7         [-1, 12, 197, 197]               0\n",
      "            Linear-8             [-1, 197, 768]         590,592\n",
      "           Dropout-9             [-1, 197, 768]               0\n",
      "        Attention-10             [-1, 197, 768]               0\n",
      "         Identity-11             [-1, 197, 768]               0\n",
      "        LayerNorm-12             [-1, 197, 768]           1,536\n",
      "           Linear-13            [-1, 197, 3072]       2,362,368\n",
      "             GELU-14            [-1, 197, 3072]               0\n",
      "          Dropout-15            [-1, 197, 3072]               0\n",
      "           Linear-16             [-1, 197, 768]       2,360,064\n",
      "          Dropout-17             [-1, 197, 768]               0\n",
      "              Mlp-18             [-1, 197, 768]               0\n",
      "         Identity-19             [-1, 197, 768]               0\n",
      "            Block-20             [-1, 197, 768]               0\n",
      "        LayerNorm-21             [-1, 197, 768]           1,536\n",
      "           Linear-22            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-23         [-1, 12, 197, 197]               0\n",
      "           Linear-24             [-1, 197, 768]         590,592\n",
      "          Dropout-25             [-1, 197, 768]               0\n",
      "        Attention-26             [-1, 197, 768]               0\n",
      "         Identity-27             [-1, 197, 768]               0\n",
      "        LayerNorm-28             [-1, 197, 768]           1,536\n",
      "           Linear-29            [-1, 197, 3072]       2,362,368\n",
      "             GELU-30            [-1, 197, 3072]               0\n",
      "          Dropout-31            [-1, 197, 3072]               0\n",
      "           Linear-32             [-1, 197, 768]       2,360,064\n",
      "          Dropout-33             [-1, 197, 768]               0\n",
      "              Mlp-34             [-1, 197, 768]               0\n",
      "         Identity-35             [-1, 197, 768]               0\n",
      "            Block-36             [-1, 197, 768]               0\n",
      "        LayerNorm-37             [-1, 197, 768]           1,536\n",
      "           Linear-38            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-39         [-1, 12, 197, 197]               0\n",
      "           Linear-40             [-1, 197, 768]         590,592\n",
      "          Dropout-41             [-1, 197, 768]               0\n",
      "        Attention-42             [-1, 197, 768]               0\n",
      "         Identity-43             [-1, 197, 768]               0\n",
      "        LayerNorm-44             [-1, 197, 768]           1,536\n",
      "           Linear-45            [-1, 197, 3072]       2,362,368\n",
      "             GELU-46            [-1, 197, 3072]               0\n",
      "          Dropout-47            [-1, 197, 3072]               0\n",
      "           Linear-48             [-1, 197, 768]       2,360,064\n",
      "          Dropout-49             [-1, 197, 768]               0\n",
      "              Mlp-50             [-1, 197, 768]               0\n",
      "         Identity-51             [-1, 197, 768]               0\n",
      "            Block-52             [-1, 197, 768]               0\n",
      "        LayerNorm-53             [-1, 197, 768]           1,536\n",
      "           Linear-54            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-55         [-1, 12, 197, 197]               0\n",
      "           Linear-56             [-1, 197, 768]         590,592\n",
      "          Dropout-57             [-1, 197, 768]               0\n",
      "        Attention-58             [-1, 197, 768]               0\n",
      "         Identity-59             [-1, 197, 768]               0\n",
      "        LayerNorm-60             [-1, 197, 768]           1,536\n",
      "           Linear-61            [-1, 197, 3072]       2,362,368\n",
      "             GELU-62            [-1, 197, 3072]               0\n",
      "          Dropout-63            [-1, 197, 3072]               0\n",
      "           Linear-64             [-1, 197, 768]       2,360,064\n",
      "          Dropout-65             [-1, 197, 768]               0\n",
      "              Mlp-66             [-1, 197, 768]               0\n",
      "         Identity-67             [-1, 197, 768]               0\n",
      "            Block-68             [-1, 197, 768]               0\n",
      "        LayerNorm-69             [-1, 197, 768]           1,536\n",
      "           Linear-70            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-71         [-1, 12, 197, 197]               0\n",
      "           Linear-72             [-1, 197, 768]         590,592\n",
      "          Dropout-73             [-1, 197, 768]               0\n",
      "        Attention-74             [-1, 197, 768]               0\n",
      "         Identity-75             [-1, 197, 768]               0\n",
      "        LayerNorm-76             [-1, 197, 768]           1,536\n",
      "           Linear-77            [-1, 197, 3072]       2,362,368\n",
      "             GELU-78            [-1, 197, 3072]               0\n",
      "          Dropout-79            [-1, 197, 3072]               0\n",
      "           Linear-80             [-1, 197, 768]       2,360,064\n",
      "          Dropout-81             [-1, 197, 768]               0\n",
      "              Mlp-82             [-1, 197, 768]               0\n",
      "         Identity-83             [-1, 197, 768]               0\n",
      "            Block-84             [-1, 197, 768]               0\n",
      "        LayerNorm-85             [-1, 197, 768]           1,536\n",
      "           Linear-86            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-87         [-1, 12, 197, 197]               0\n",
      "           Linear-88             [-1, 197, 768]         590,592\n",
      "          Dropout-89             [-1, 197, 768]               0\n",
      "        Attention-90             [-1, 197, 768]               0\n",
      "         Identity-91             [-1, 197, 768]               0\n",
      "        LayerNorm-92             [-1, 197, 768]           1,536\n",
      "           Linear-93            [-1, 197, 3072]       2,362,368\n",
      "             GELU-94            [-1, 197, 3072]               0\n",
      "          Dropout-95            [-1, 197, 3072]               0\n",
      "           Linear-96             [-1, 197, 768]       2,360,064\n",
      "          Dropout-97             [-1, 197, 768]               0\n",
      "              Mlp-98             [-1, 197, 768]               0\n",
      "         Identity-99             [-1, 197, 768]               0\n",
      "           Block-100             [-1, 197, 768]               0\n",
      "       LayerNorm-101             [-1, 197, 768]           1,536\n",
      "          Linear-102            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-103         [-1, 12, 197, 197]               0\n",
      "          Linear-104             [-1, 197, 768]         590,592\n",
      "         Dropout-105             [-1, 197, 768]               0\n",
      "       Attention-106             [-1, 197, 768]               0\n",
      "        Identity-107             [-1, 197, 768]               0\n",
      "       LayerNorm-108             [-1, 197, 768]           1,536\n",
      "          Linear-109            [-1, 197, 3072]       2,362,368\n",
      "            GELU-110            [-1, 197, 3072]               0\n",
      "         Dropout-111            [-1, 197, 3072]               0\n",
      "          Linear-112             [-1, 197, 768]       2,360,064\n",
      "         Dropout-113             [-1, 197, 768]               0\n",
      "             Mlp-114             [-1, 197, 768]               0\n",
      "        Identity-115             [-1, 197, 768]               0\n",
      "           Block-116             [-1, 197, 768]               0\n",
      "       LayerNorm-117             [-1, 197, 768]           1,536\n",
      "          Linear-118            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-119         [-1, 12, 197, 197]               0\n",
      "          Linear-120             [-1, 197, 768]         590,592\n",
      "         Dropout-121             [-1, 197, 768]               0\n",
      "       Attention-122             [-1, 197, 768]               0\n",
      "        Identity-123             [-1, 197, 768]               0\n",
      "       LayerNorm-124             [-1, 197, 768]           1,536\n",
      "          Linear-125            [-1, 197, 3072]       2,362,368\n",
      "            GELU-126            [-1, 197, 3072]               0\n",
      "         Dropout-127            [-1, 197, 3072]               0\n",
      "          Linear-128             [-1, 197, 768]       2,360,064\n",
      "         Dropout-129             [-1, 197, 768]               0\n",
      "             Mlp-130             [-1, 197, 768]               0\n",
      "        Identity-131             [-1, 197, 768]               0\n",
      "           Block-132             [-1, 197, 768]               0\n",
      "       LayerNorm-133             [-1, 197, 768]           1,536\n",
      "          Linear-134            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-135         [-1, 12, 197, 197]               0\n",
      "          Linear-136             [-1, 197, 768]         590,592\n",
      "         Dropout-137             [-1, 197, 768]               0\n",
      "       Attention-138             [-1, 197, 768]               0\n",
      "        Identity-139             [-1, 197, 768]               0\n",
      "       LayerNorm-140             [-1, 197, 768]           1,536\n",
      "          Linear-141            [-1, 197, 3072]       2,362,368\n",
      "            GELU-142            [-1, 197, 3072]               0\n",
      "         Dropout-143            [-1, 197, 3072]               0\n",
      "          Linear-144             [-1, 197, 768]       2,360,064\n",
      "         Dropout-145             [-1, 197, 768]               0\n",
      "             Mlp-146             [-1, 197, 768]               0\n",
      "        Identity-147             [-1, 197, 768]               0\n",
      "           Block-148             [-1, 197, 768]               0\n",
      "       LayerNorm-149             [-1, 197, 768]           1,536\n",
      "          Linear-150            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-151         [-1, 12, 197, 197]               0\n",
      "          Linear-152             [-1, 197, 768]         590,592\n",
      "         Dropout-153             [-1, 197, 768]               0\n",
      "       Attention-154             [-1, 197, 768]               0\n",
      "        Identity-155             [-1, 197, 768]               0\n",
      "       LayerNorm-156             [-1, 197, 768]           1,536\n",
      "          Linear-157            [-1, 197, 3072]       2,362,368\n",
      "            GELU-158            [-1, 197, 3072]               0\n",
      "         Dropout-159            [-1, 197, 3072]               0\n",
      "          Linear-160             [-1, 197, 768]       2,360,064\n",
      "         Dropout-161             [-1, 197, 768]               0\n",
      "             Mlp-162             [-1, 197, 768]               0\n",
      "        Identity-163             [-1, 197, 768]               0\n",
      "           Block-164             [-1, 197, 768]               0\n",
      "       LayerNorm-165             [-1, 197, 768]           1,536\n",
      "          Linear-166            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-167         [-1, 12, 197, 197]               0\n",
      "          Linear-168             [-1, 197, 768]         590,592\n",
      "         Dropout-169             [-1, 197, 768]               0\n",
      "       Attention-170             [-1, 197, 768]               0\n",
      "        Identity-171             [-1, 197, 768]               0\n",
      "       LayerNorm-172             [-1, 197, 768]           1,536\n",
      "          Linear-173            [-1, 197, 3072]       2,362,368\n",
      "            GELU-174            [-1, 197, 3072]               0\n",
      "         Dropout-175            [-1, 197, 3072]               0\n",
      "          Linear-176             [-1, 197, 768]       2,360,064\n",
      "         Dropout-177             [-1, 197, 768]               0\n",
      "             Mlp-178             [-1, 197, 768]               0\n",
      "        Identity-179             [-1, 197, 768]               0\n",
      "           Block-180             [-1, 197, 768]               0\n",
      "       LayerNorm-181             [-1, 197, 768]           1,536\n",
      "          Linear-182            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-183         [-1, 12, 197, 197]               0\n",
      "          Linear-184             [-1, 197, 768]         590,592\n",
      "         Dropout-185             [-1, 197, 768]               0\n",
      "       Attention-186             [-1, 197, 768]               0\n",
      "        Identity-187             [-1, 197, 768]               0\n",
      "       LayerNorm-188             [-1, 197, 768]           1,536\n",
      "          Linear-189            [-1, 197, 3072]       2,362,368\n",
      "            GELU-190            [-1, 197, 3072]               0\n",
      "         Dropout-191            [-1, 197, 3072]               0\n",
      "          Linear-192             [-1, 197, 768]       2,360,064\n",
      "         Dropout-193             [-1, 197, 768]               0\n",
      "             Mlp-194             [-1, 197, 768]               0\n",
      "        Identity-195             [-1, 197, 768]               0\n",
      "           Block-196             [-1, 197, 768]               0\n",
      "       LayerNorm-197             [-1, 197, 768]           1,536\n",
      "        Identity-198                  [-1, 768]               0\n",
      "          Linear-199                 [-1, 1000]         769,000\n",
      "================================================================\n",
      "Total params: 86,415,592\n",
      "Trainable params: 86,415,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 408.54\n",
      "Params size (MB): 329.65\n",
      "Estimated Total Size (MB): 738.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn([3,224,224])\n",
    "# data.shape\n",
    "summary(_model, (3,224,224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "316ef763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class custom(nn.Module) :\n",
    "    def __init__(self, out) :\n",
    "        super(custom, self).__init__()\n",
    "        self.out = out\n",
    "        self.model = nn.Sequential(nn.Linear(777, 768), _model.blocks, _model.norm, _model.pre_logits, _model.head)\n",
    "        self.linear = nn.Linear(1000, out)\n",
    "        self.avgpool1d = nn.AdaptiveAvgPool1d((1))\n",
    "        \n",
    "        \n",
    "    def forward(self, x) :\n",
    "        x = self.model(x)\n",
    "        print(x.shape)\n",
    "        x = x.transpose(2,1)\n",
    "        print(x.shape)\n",
    "        x = self.avgpool1d(x)\n",
    "        print(x.shape)\n",
    "        x = x.squeeze(2)\n",
    "        print(x.shape)\n",
    "        x = self.linear(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "modelcus = custom(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bd5a99a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768, 1000])\n",
      "torch.Size([2, 1000, 768])\n",
      "torch.Size([2, 1000, 1])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 25])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1             [-1, 768, 768]         597,504\n",
      "         LayerNorm-2             [-1, 768, 768]           1,536\n",
      "            Linear-3            [-1, 768, 2304]       1,771,776\n",
      "           Dropout-4         [-1, 12, 768, 768]               0\n",
      "            Linear-5             [-1, 768, 768]         590,592\n",
      "           Dropout-6             [-1, 768, 768]               0\n",
      "         Attention-7             [-1, 768, 768]               0\n",
      "          Identity-8             [-1, 768, 768]               0\n",
      "         LayerNorm-9             [-1, 768, 768]           1,536\n",
      "           Linear-10            [-1, 768, 3072]       2,362,368\n",
      "             GELU-11            [-1, 768, 3072]               0\n",
      "          Dropout-12            [-1, 768, 3072]               0\n",
      "           Linear-13             [-1, 768, 768]       2,360,064\n",
      "          Dropout-14             [-1, 768, 768]               0\n",
      "              Mlp-15             [-1, 768, 768]               0\n",
      "         Identity-16             [-1, 768, 768]               0\n",
      "            Block-17             [-1, 768, 768]               0\n",
      "        LayerNorm-18             [-1, 768, 768]           1,536\n",
      "           Linear-19            [-1, 768, 2304]       1,771,776\n",
      "          Dropout-20         [-1, 12, 768, 768]               0\n",
      "           Linear-21             [-1, 768, 768]         590,592\n",
      "          Dropout-22             [-1, 768, 768]               0\n",
      "        Attention-23             [-1, 768, 768]               0\n",
      "         Identity-24             [-1, 768, 768]               0\n",
      "        LayerNorm-25             [-1, 768, 768]           1,536\n",
      "           Linear-26            [-1, 768, 3072]       2,362,368\n",
      "             GELU-27            [-1, 768, 3072]               0\n",
      "          Dropout-28            [-1, 768, 3072]               0\n",
      "           Linear-29             [-1, 768, 768]       2,360,064\n",
      "          Dropout-30             [-1, 768, 768]               0\n",
      "              Mlp-31             [-1, 768, 768]               0\n",
      "         Identity-32             [-1, 768, 768]               0\n",
      "            Block-33             [-1, 768, 768]               0\n",
      "        LayerNorm-34             [-1, 768, 768]           1,536\n",
      "           Linear-35            [-1, 768, 2304]       1,771,776\n",
      "          Dropout-36         [-1, 12, 768, 768]               0\n",
      "           Linear-37             [-1, 768, 768]         590,592\n",
      "          Dropout-38             [-1, 768, 768]               0\n",
      "        Attention-39             [-1, 768, 768]               0\n",
      "         Identity-40             [-1, 768, 768]               0\n",
      "        LayerNorm-41             [-1, 768, 768]           1,536\n",
      "           Linear-42            [-1, 768, 3072]       2,362,368\n",
      "             GELU-43            [-1, 768, 3072]               0\n",
      "          Dropout-44            [-1, 768, 3072]               0\n",
      "           Linear-45             [-1, 768, 768]       2,360,064\n",
      "          Dropout-46             [-1, 768, 768]               0\n",
      "              Mlp-47             [-1, 768, 768]               0\n",
      "         Identity-48             [-1, 768, 768]               0\n",
      "            Block-49             [-1, 768, 768]               0\n",
      "        LayerNorm-50             [-1, 768, 768]           1,536\n",
      "           Linear-51            [-1, 768, 2304]       1,771,776\n",
      "          Dropout-52         [-1, 12, 768, 768]               0\n",
      "           Linear-53             [-1, 768, 768]         590,592\n",
      "          Dropout-54             [-1, 768, 768]               0\n",
      "        Attention-55             [-1, 768, 768]               0\n",
      "         Identity-56             [-1, 768, 768]               0\n",
      "        LayerNorm-57             [-1, 768, 768]           1,536\n",
      "           Linear-58            [-1, 768, 3072]       2,362,368\n",
      "             GELU-59            [-1, 768, 3072]               0\n",
      "          Dropout-60            [-1, 768, 3072]               0\n",
      "           Linear-61             [-1, 768, 768]       2,360,064\n",
      "          Dropout-62             [-1, 768, 768]               0\n",
      "              Mlp-63             [-1, 768, 768]               0\n",
      "         Identity-64             [-1, 768, 768]               0\n",
      "            Block-65             [-1, 768, 768]               0\n",
      "        LayerNorm-66             [-1, 768, 768]           1,536\n",
      "           Linear-67            [-1, 768, 2304]       1,771,776\n",
      "          Dropout-68         [-1, 12, 768, 768]               0\n",
      "           Linear-69             [-1, 768, 768]         590,592\n",
      "          Dropout-70             [-1, 768, 768]               0\n",
      "        Attention-71             [-1, 768, 768]               0\n",
      "         Identity-72             [-1, 768, 768]               0\n",
      "        LayerNorm-73             [-1, 768, 768]           1,536\n",
      "           Linear-74            [-1, 768, 3072]       2,362,368\n",
      "             GELU-75            [-1, 768, 3072]               0\n",
      "          Dropout-76            [-1, 768, 3072]               0\n",
      "           Linear-77             [-1, 768, 768]       2,360,064\n",
      "          Dropout-78             [-1, 768, 768]               0\n",
      "              Mlp-79             [-1, 768, 768]               0\n",
      "         Identity-80             [-1, 768, 768]               0\n",
      "            Block-81             [-1, 768, 768]               0\n",
      "        LayerNorm-82             [-1, 768, 768]           1,536\n",
      "           Linear-83            [-1, 768, 2304]       1,771,776\n",
      "          Dropout-84         [-1, 12, 768, 768]               0\n",
      "           Linear-85             [-1, 768, 768]         590,592\n",
      "          Dropout-86             [-1, 768, 768]               0\n",
      "        Attention-87             [-1, 768, 768]               0\n",
      "         Identity-88             [-1, 768, 768]               0\n",
      "        LayerNorm-89             [-1, 768, 768]           1,536\n",
      "           Linear-90            [-1, 768, 3072]       2,362,368\n",
      "             GELU-91            [-1, 768, 3072]               0\n",
      "          Dropout-92            [-1, 768, 3072]               0\n",
      "           Linear-93             [-1, 768, 768]       2,360,064\n",
      "          Dropout-94             [-1, 768, 768]               0\n",
      "              Mlp-95             [-1, 768, 768]               0\n",
      "         Identity-96             [-1, 768, 768]               0\n",
      "            Block-97             [-1, 768, 768]               0\n",
      "        LayerNorm-98             [-1, 768, 768]           1,536\n",
      "           Linear-99            [-1, 768, 2304]       1,771,776\n",
      "         Dropout-100         [-1, 12, 768, 768]               0\n",
      "          Linear-101             [-1, 768, 768]         590,592\n",
      "         Dropout-102             [-1, 768, 768]               0\n",
      "       Attention-103             [-1, 768, 768]               0\n",
      "        Identity-104             [-1, 768, 768]               0\n",
      "       LayerNorm-105             [-1, 768, 768]           1,536\n",
      "          Linear-106            [-1, 768, 3072]       2,362,368\n",
      "            GELU-107            [-1, 768, 3072]               0\n",
      "         Dropout-108            [-1, 768, 3072]               0\n",
      "          Linear-109             [-1, 768, 768]       2,360,064\n",
      "         Dropout-110             [-1, 768, 768]               0\n",
      "             Mlp-111             [-1, 768, 768]               0\n",
      "        Identity-112             [-1, 768, 768]               0\n",
      "           Block-113             [-1, 768, 768]               0\n",
      "       LayerNorm-114             [-1, 768, 768]           1,536\n",
      "          Linear-115            [-1, 768, 2304]       1,771,776\n",
      "         Dropout-116         [-1, 12, 768, 768]               0\n",
      "          Linear-117             [-1, 768, 768]         590,592\n",
      "         Dropout-118             [-1, 768, 768]               0\n",
      "       Attention-119             [-1, 768, 768]               0\n",
      "        Identity-120             [-1, 768, 768]               0\n",
      "       LayerNorm-121             [-1, 768, 768]           1,536\n",
      "          Linear-122            [-1, 768, 3072]       2,362,368\n",
      "            GELU-123            [-1, 768, 3072]               0\n",
      "         Dropout-124            [-1, 768, 3072]               0\n",
      "          Linear-125             [-1, 768, 768]       2,360,064\n",
      "         Dropout-126             [-1, 768, 768]               0\n",
      "             Mlp-127             [-1, 768, 768]               0\n",
      "        Identity-128             [-1, 768, 768]               0\n",
      "           Block-129             [-1, 768, 768]               0\n",
      "       LayerNorm-130             [-1, 768, 768]           1,536\n",
      "          Linear-131            [-1, 768, 2304]       1,771,776\n",
      "         Dropout-132         [-1, 12, 768, 768]               0\n",
      "          Linear-133             [-1, 768, 768]         590,592\n",
      "         Dropout-134             [-1, 768, 768]               0\n",
      "       Attention-135             [-1, 768, 768]               0\n",
      "        Identity-136             [-1, 768, 768]               0\n",
      "       LayerNorm-137             [-1, 768, 768]           1,536\n",
      "          Linear-138            [-1, 768, 3072]       2,362,368\n",
      "            GELU-139            [-1, 768, 3072]               0\n",
      "         Dropout-140            [-1, 768, 3072]               0\n",
      "          Linear-141             [-1, 768, 768]       2,360,064\n",
      "         Dropout-142             [-1, 768, 768]               0\n",
      "             Mlp-143             [-1, 768, 768]               0\n",
      "        Identity-144             [-1, 768, 768]               0\n",
      "           Block-145             [-1, 768, 768]               0\n",
      "       LayerNorm-146             [-1, 768, 768]           1,536\n",
      "          Linear-147            [-1, 768, 2304]       1,771,776\n",
      "         Dropout-148         [-1, 12, 768, 768]               0\n",
      "          Linear-149             [-1, 768, 768]         590,592\n",
      "         Dropout-150             [-1, 768, 768]               0\n",
      "       Attention-151             [-1, 768, 768]               0\n",
      "        Identity-152             [-1, 768, 768]               0\n",
      "       LayerNorm-153             [-1, 768, 768]           1,536\n",
      "          Linear-154            [-1, 768, 3072]       2,362,368\n",
      "            GELU-155            [-1, 768, 3072]               0\n",
      "         Dropout-156            [-1, 768, 3072]               0\n",
      "          Linear-157             [-1, 768, 768]       2,360,064\n",
      "         Dropout-158             [-1, 768, 768]               0\n",
      "             Mlp-159             [-1, 768, 768]               0\n",
      "        Identity-160             [-1, 768, 768]               0\n",
      "           Block-161             [-1, 768, 768]               0\n",
      "       LayerNorm-162             [-1, 768, 768]           1,536\n",
      "          Linear-163            [-1, 768, 2304]       1,771,776\n",
      "         Dropout-164         [-1, 12, 768, 768]               0\n",
      "          Linear-165             [-1, 768, 768]         590,592\n",
      "         Dropout-166             [-1, 768, 768]               0\n",
      "       Attention-167             [-1, 768, 768]               0\n",
      "        Identity-168             [-1, 768, 768]               0\n",
      "       LayerNorm-169             [-1, 768, 768]           1,536\n",
      "          Linear-170            [-1, 768, 3072]       2,362,368\n",
      "            GELU-171            [-1, 768, 3072]               0\n",
      "         Dropout-172            [-1, 768, 3072]               0\n",
      "          Linear-173             [-1, 768, 768]       2,360,064\n",
      "         Dropout-174             [-1, 768, 768]               0\n",
      "             Mlp-175             [-1, 768, 768]               0\n",
      "        Identity-176             [-1, 768, 768]               0\n",
      "           Block-177             [-1, 768, 768]               0\n",
      "       LayerNorm-178             [-1, 768, 768]           1,536\n",
      "          Linear-179            [-1, 768, 2304]       1,771,776\n",
      "         Dropout-180         [-1, 12, 768, 768]               0\n",
      "          Linear-181             [-1, 768, 768]         590,592\n",
      "         Dropout-182             [-1, 768, 768]               0\n",
      "       Attention-183             [-1, 768, 768]               0\n",
      "        Identity-184             [-1, 768, 768]               0\n",
      "       LayerNorm-185             [-1, 768, 768]           1,536\n",
      "          Linear-186            [-1, 768, 3072]       2,362,368\n",
      "            GELU-187            [-1, 768, 3072]               0\n",
      "         Dropout-188            [-1, 768, 3072]               0\n",
      "          Linear-189             [-1, 768, 768]       2,360,064\n",
      "         Dropout-190             [-1, 768, 768]               0\n",
      "             Mlp-191             [-1, 768, 768]               0\n",
      "        Identity-192             [-1, 768, 768]               0\n",
      "           Block-193             [-1, 768, 768]               0\n",
      "       LayerNorm-194             [-1, 768, 768]           1,536\n",
      "        Identity-195             [-1, 768, 768]               0\n",
      "          Linear-196            [-1, 768, 1000]         769,000\n",
      "AdaptiveAvgPool1d-197              [-1, 1000, 1]               0\n",
      "          Linear-198                   [-1, 25]          25,025\n",
      "================================================================\n",
      "Total params: 86,447,529\n",
      "Trainable params: 86,447,529\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.28\n",
      "Forward/backward pass size (MB): 2071.37\n",
      "Params size (MB): 329.77\n",
      "Estimated Total Size (MB): 2403.41\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(modelcus, (768, 777), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9ad74e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 1000])\n",
      "torch.Size([1, 1000, 768])\n",
      "torch.Size([1, 1000, 1])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 25])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3285,  0.5938,  0.9646,  0.9012,  0.3262,  1.2143,  0.5092, -0.6902,\n",
       "         -0.4887,  0.2381,  1.1416, -0.4408, -0.3450,  0.9816, -0.2160,  0.2707,\n",
       "          0.0627, -0.7833,  1.3075,  0.5925,  1.2748,  0.6680, -0.3639,  0.6835,\n",
       "         -0.3724]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = torch.randn((1, 768, 777))\n",
    "pred = modelcus(data)\n",
    "display(pred.shape)\n",
    "display(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e0b97c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
