{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4636868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from easydict import EasyDict\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import json \n",
    "import timm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa2ce4",
   "metadata": {},
   "source": [
    "# Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd44a54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def label_preprocessing(path) :\n",
    "#     path = os.path.join(path, 'train.csv')\n",
    "    labels = pd.read_csv(path)\n",
    "\n",
    "    cnt = 0\n",
    "    label_encoder = {}\n",
    "    for i, label in enumerate(tqdm(sorted(labels['label']))) :\n",
    "        \n",
    "        if label not in label_encoder.values() :\n",
    "            label_encoder[cnt] = label\n",
    "            cnt += 1\n",
    "        \n",
    "    label_decoder = {val : key for key, val in label_encoder.items()}\n",
    "    \n",
    "    return label_encoder, label_decoder\n",
    "\n",
    "# enc, dec = label_preprocessing(\"../data/train.csv\")\n",
    "# display(enc)\n",
    "# display(dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45adac9d",
   "metadata": {},
   "source": [
    "# CSV feature - min, max value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195590dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_feature_dict(path, csv_features) :\n",
    "    \n",
    "    csv_files = sorted(glob(os.path.join(path, '*/*.csv')))\n",
    "\n",
    "    temp_csv = pd.read_csv(csv_files[0])[csv_features]\n",
    "    max_arr, min_arr = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "\n",
    "    # feature 별 최대값, 최솟값 계산\n",
    "    for csv in tqdm(csv_files[1:]):\n",
    "        temp_csv = pd.read_csv(csv)[csv_features]\n",
    "        temp_csv = temp_csv.replace('-',np.nan).dropna()\n",
    "        if len(temp_csv) == 0:\n",
    "            continue\n",
    "        temp_csv = temp_csv.astype(float)\n",
    "        temp_max, temp_min = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "        max_arr = np.max([max_arr,temp_max], axis=0)\n",
    "        min_arr = np.min([min_arr,temp_min], axis=0)\n",
    "\n",
    "    # feature 별 최대값, 최솟값 dictionary return\n",
    "    return {csv_features[i]:[min_arr[i], max_arr[i]] for i in range(len(csv_features))}\n",
    "\n",
    "# csv_feature_dict = csv_feature_dict('../data/train', csv_features)\n",
    "# csv_feature_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420ac20a",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a55be8e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_split(path, label_decoder, kfold=False, test_size=0.2) : \n",
    "    imgs = glob(os.path.join(path, '*/*.jpg'))\n",
    "    json_files = glob(os.path.join(path, '*/*.json'))\n",
    "    \n",
    "    label_list = []\n",
    "    for json_path in tqdm(json_files) :\n",
    "        json_file = json.load(open(json_path, 'r'))\n",
    "        \n",
    "        crop = json_file[\"annotations\"][\"crop\"]\n",
    "        disease = json_file[\"annotations\"][\"disease\"]\n",
    "        risk = json_file[\"annotations\"][\"risk\"]\n",
    "        \n",
    "        label = f'{crop}_{disease}_{risk}'\n",
    "        label_list.append(label_decoder[label])\n",
    "    \n",
    "    if kfold :\n",
    "        return imgs, label_list\n",
    "    else :\n",
    "        return train_test_split(imgs, test_size=test_size, shuffle=True, stratify=label_list)\n",
    "# a, b = data_split('../data/train', label_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a210c4",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f283bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(size=224):\n",
    "    train_transforms = A.Compose([\n",
    "                A.Resize(size ,size),\n",
    "                A.OneOf([\n",
    "                    A.Rotate(),\n",
    "                    A.HorizontalFlip(),\n",
    "                    A.VerticalFlip()\n",
    "                ], p=1)\n",
    "            ])\n",
    "\n",
    "    val_transforms = A.Compose([\n",
    "        A.Resize(size,size)\n",
    "    ])\n",
    "    \n",
    "    return train_transforms, val_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69b5c3",
   "metadata": {},
   "source": [
    "# Custom Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02b2705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 files, \n",
    "                 transforms, \n",
    "                 label_decoder, \n",
    "                 opt,\n",
    "                 mode='train'):\n",
    "        \n",
    "        if opt.use_kfold :\n",
    "            self.files = self.kfold_files(files, opt)\n",
    "        else : \n",
    "            self.files = files\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.label_decoder = label_decoder #label_encoder\n",
    "        self.csv_feature_dict = opt.csv_feature_dict\n",
    "        self.max_len = opt.max_len\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        file = self.files[i]\n",
    "        \n",
    "        # CSV\n",
    "        csv_data, seq_len = self.csv_preprocessing(file)\n",
    "        \n",
    "        # image\n",
    "        img = self.img_preprocessing(file)\n",
    "        \n",
    "        if self.mode == 'train':         \n",
    "            # Label\n",
    "            label = self.label_preprocessing(file)\n",
    "            \n",
    "            return {\n",
    "                'img' : torch.tensor(img, dtype=torch.float32),\n",
    "                'label' : torch.tensor(self.label_decoder[label], dtype=torch.long),\n",
    "                'csv_feature': torch.tensor(csv_data, dtype=torch.float32),\n",
    "                'seq_len' : seq_len\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            return {\n",
    "                'img' : torch.tensor(img, dtype=torch.float32),\n",
    "                'csv_feature': torch.tensor(csv_data, dtype=torch.float32),\n",
    "                'seq_len' : seq_len\n",
    "            }\n",
    "        \n",
    "    def kfold_files(self, data_index, opt) :\n",
    "        file_list = glob(os.path.join(opt.dataset_path, \"*/*.jpg\"))\n",
    "        return [file_list[idx] for idx in data_index]\n",
    "        \n",
    "    \n",
    "    def csv_preprocessing(self, file) :\n",
    "        # CSV\n",
    "        csv_path = file.replace(\"jpg\",\"csv\")\n",
    "        df = pd.read_csv(csv_path)[self.csv_feature_dict.keys()]\n",
    "        df = df.replace('-', 0)\n",
    "        \n",
    "        # MinMax scaling\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].astype(float) - self.csv_feature_dict[col][0]\n",
    "            df[col] = df[col] / (self.csv_feature_dict[col][1]-self.csv_feature_dict[col][0])\n",
    "\n",
    "        # pack_padded_sequence 하기 위한 len 추가\n",
    "        seq_len = len(df)\n",
    "\n",
    "        df_np = df.to_numpy()\n",
    "        df_len, df_features = df_np.shape\n",
    "        \n",
    "        csv_data = np.zeros([self.max_len, df_features])\n",
    "        csv_data[0:df_len, :] = df_np\n",
    "        \n",
    "        return csv_data, seq_len\n",
    "\n",
    "    def img_preprocessing(self, file) :\n",
    "        image_path = file\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = self.transforms(image=img)[\"image\"]\n",
    "        img = img.transpose(2,0,1)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def label_preprocessing(self, file) :\n",
    "        json_path = file.replace(\"jpg\",\"json\")\n",
    "        with open(json_path, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "\n",
    "        crop = json_file['annotations']['crop']\n",
    "        disease = json_file['annotations']['disease']\n",
    "        risk = json_file['annotations']['risk']\n",
    "        \n",
    "        return f'{crop}_{disease}_{risk}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce6704d",
   "metadata": {},
   "source": [
    "# Model - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1580252",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CNN_Encoder(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, pretrained_path=None):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        \n",
    "        if pretrained_path :\n",
    "            # no use pretrained model trained with Public dataset\n",
    "            self.model = self.create_pretrained_model(model_name, num_classes, pretrained_path)\n",
    "            \n",
    "        else :            \n",
    "            self.model = timm.create_model(model_name, num_classes=num_classes, pretrained=True)\n",
    "            \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        output = self.model(inputs)\n",
    "        return output\n",
    "\n",
    "    def create_pretrained_model(self, model_name, num_classes, pretrained_path):\n",
    "        pre_model = torch.load(pretrained_path, map_location=\"cpu\")\n",
    "        output_size = pre_model[list(pre_model.keys())[-1]].shape[0]\n",
    " \n",
    "        return nn.Sequential(\n",
    "                    timm.create_model(model_name, num_classes=output_size, pretrained=True),\n",
    "                    nn.Linear(output_size, num_classes)\n",
    "                )\n",
    "\n",
    "# model = CNN_Encoder(\"efficientnetv2_rw_s\", 1000)\n",
    "# model = CNN_Encoder(\"efficientnetv2_rw_s\", 1000, \"../model/k_fold_50k_pretrained_effiv2S/4_f9462_public_vill_50k_pretrain_efficientnetv2S.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5f419",
   "metadata": {},
   "source": [
    "# Model - RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0db46531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class MLSTMfcn(nn.Module):\n",
    "    def __init__(self, *, num_classes, max_seq_len, num_features,\n",
    "                 num_lstm_out=128, num_lstm_layers=1, \n",
    "                 conv1_nf=128, conv2_nf=256, conv3_nf=128,\n",
    "                 lstm_drop_p=0.8, fc_drop_p=0.3):\n",
    "        \n",
    "        super(MLSTMfcn, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_features = num_features\n",
    "\n",
    "        self.num_lstm_out = num_lstm_out\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "\n",
    "        self.conv1_nf = conv1_nf\n",
    "        self.conv2_nf = conv2_nf\n",
    "        self.conv3_nf = conv3_nf\n",
    "\n",
    "        self.lstm_drop_p = lstm_drop_p\n",
    "        self.fc_drop_p = fc_drop_p\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.num_features, \n",
    "                            hidden_size=self.num_lstm_out,\n",
    "                            num_layers=self.num_lstm_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(self.num_features, self.conv1_nf, 8)\n",
    "        self.conv2 = nn.Conv1d(self.conv1_nf, self.conv2_nf, 5)\n",
    "        self.conv3 = nn.Conv1d(self.conv2_nf, self.conv3_nf, 3)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(self.conv1_nf)\n",
    "        self.bn2 = nn.BatchNorm1d(self.conv2_nf)\n",
    "        self.bn3 = nn.BatchNorm1d(self.conv3_nf)\n",
    "\n",
    "        self.se1 = SELayer(self.conv1_nf)  # ex 128\n",
    "        self.se2 = SELayer(self.conv2_nf)  # ex 256\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstmDrop = nn.Dropout(self.lstm_drop_p)\n",
    "        self.convDrop = nn.Dropout(self.fc_drop_p)\n",
    "\n",
    "        self.fc = nn.Linear(self.conv3_nf+self.num_lstm_out, 128)\n",
    "\n",
    "        self.out_layer = nn.Linear(self.num_classes+128, self.num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, enc_out, x, seq_lens):\n",
    "        ''' input x should be in size [B,T,F], where \n",
    "            B = Batch size\n",
    "            T = Time samples\n",
    "            F = features\n",
    "        '''\n",
    "        x1 = nn.utils.rnn.pack_padded_sequence(x, seq_lens.cpu(), \n",
    "                                               batch_first=True, \n",
    "                                               enforce_sorted=False)\n",
    "        x1, (ht,ct) = self.lstm(x1)\n",
    "        x1, _ = nn.utils.rnn.pad_packed_sequence(x1, batch_first=True, \n",
    "                                                 padding_value=0.0)\n",
    "        x1 = x1[:,-1,:]\n",
    "        \n",
    "        x2 = x.transpose(2,1)\n",
    "        x2 = self.convDrop(self.relu(self.bn1(self.conv1(x2))))\n",
    "        x2 = self.se1(x2)\n",
    "        x2 = self.convDrop(self.relu(self.bn2(self.conv2(x2))))\n",
    "        x2 = self.se2(x2)\n",
    "        x2 = self.convDrop(self.relu(self.bn3(self.conv3(x2))))\n",
    "        x2 = torch.mean(x2,2)\n",
    "        \n",
    "        x_all = torch.cat((x1,x2),dim=1)\n",
    "        x_out = self.fc(x_all)\n",
    "        concat = torch.cat([enc_out, x_out], dim=1)  # enc_out + hidden \n",
    "        output = self.dropout(concat)\n",
    "        x_output = self.out_layer(output)\n",
    "        x_out = F.log_softmax(x_output, dim=1)\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "# model = MLSTMfcn(num_classes=38, max_seq_len=512, num_features=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100c687",
   "metadata": {},
   "source": [
    "# Model - CNN + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b59e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2RNN(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(CNN2RNN, self).__init__()\n",
    "        \n",
    "        self.cnn = CNN_Encoder(opt.model_name, opt.num_classes, opt.pretrained_path)\n",
    "        self.rnn = MLSTMfcn(num_classes=opt.num_classes, max_seq_len=opt.max_len, num_features=opt.num_features)\n",
    "\n",
    "\n",
    "    def forward(self, img, seq, seq_len):\n",
    "        cnn_output = self.cnn(img)\n",
    "        output = self.rnn(cnn_output, seq, seq_len)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# model = CNN2RNN(\"efficientnetv2_rw_s\", \n",
    "#                 1000, \n",
    "#                 512,\n",
    "#                 6,\n",
    "#                 \"../model/k_fold_50k_pretrained_effiv2S/4_f9462_public_vill_50k_pretrain_efficientnetv2S.pt\")\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13736063",
   "metadata": {},
   "source": [
    "# CutMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caedba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    " \n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e10041f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e49d8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_function(real, pred):    \n",
    "    real = real.cpu()\n",
    "    pred = torch.argmax(pred, dim=1).cpu()\n",
    "    score = f1_score(real, pred, average='macro')\n",
    "    return score\n",
    "\n",
    "def run(train_loader, valid_loader, opt) :\n",
    "    \n",
    "    model = CNN2RNN(opt).to(opt.device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr= opt.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    early_stopping_step = 0\n",
    "    best_loss = 10\n",
    "    for epoch in range(opt.epochs) : \n",
    "\n",
    "        # training\n",
    "        tqdm_train = tqdm(train_loader)\n",
    "        train_loss, train_macro_f1 = 0, 0\n",
    "        for batch, batch_item in enumerate(tqdm_train) :\n",
    "            model.train()\n",
    "            \n",
    "            img = batch_item['img'].to(opt.device)\n",
    "            label = batch_item['label'].to(opt.device)\n",
    "            csv_feature = batch_item['csv_feature'].to(opt.device)\n",
    "            seq_lens = batch_item['seq_len'].to(opt.device)\n",
    "\n",
    "            lam = np.random.beta(1.0, 1.0)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # add - cutmix\n",
    "                rand_index = torch.randperm(img.size()[0])\n",
    "                target_a = label\n",
    "                target_b = label[rand_index]\n",
    "                bbx1, bby1, bbx2, bby2 = rand_bbox(img.size(), lam)\n",
    "                img[:, :, bbx1:bbx2, bby1:bby2] = img[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "                lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (img.size()[-1] * img.size()[-2]))\n",
    "\n",
    "                output = model(img, csv_feature, seq_lens)\n",
    "                loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1. - lam)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            score = accuracy_function(label, output)\n",
    "            \n",
    "            train_loss += loss\n",
    "            train_macro_f1 += score\n",
    "            \n",
    "            tqdm_train.set_postfix({\"Epoch\" : epoch+1,\n",
    "                                    \"Mean train loss\" : \"{:06f}\".format(train_loss/(batch+1)),\n",
    "                                    \"Mean train f1\" : \"{:06f}\".format(train_macro_f1/(batch+1))\n",
    "                                   })\n",
    "            \n",
    "#             print(f\"Traing Epoch : [{epoch}/{opt.epochs}] loss : {train_loss}  f1 : {train_macro_f1}\",end='\\r')\n",
    "            \n",
    "#         print(f\"Traing Epoch : [{epoch}/{opt.epochs}] loss : {train_loss}  f1 : {train_macro_f1}\")\n",
    "        \n",
    "        # validation\n",
    "        tqdm_valid = tqdm(valid_loader)\n",
    "        valid_loss, valid_macro_f1 = 0, 0\n",
    "        for batch, batch_item in enumerate(tqdm_valid) :\n",
    "            img = batch_item['img'].to(opt.device)\n",
    "            label = batch_item['label'].to(opt.device)\n",
    "            csv_feature = batch_item['csv_feature'].to(opt.device)\n",
    "            seq_lens = batch_item['seq_len'].to(opt.device)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model(img, csv_feature, seq_lens)\n",
    "                loss = criterion(output, label)\n",
    "            score = accuracy_function(label, output)\n",
    "            \n",
    "            valid_loss += loss\n",
    "            valid_macro_f1 += score\n",
    "            \n",
    "            tqdm_valid.set_postfix({\"Mean valid loss\" : \"{:06f}\".format(valid_loss/(batch+1)),\n",
    "                                    \"Mean valid f1\" : \"{:06f}\".format(valid_macro_f1/(batch+1))\n",
    "                                   })\n",
    "            \n",
    "#             print(f\"Valid Epoch : [{epoch}/{opt.epochs}] loss : {valid_loss}  f1 : {valid_macro_f1}\",end='\\r')\n",
    "            \n",
    "#         print(f\"Valid Epoch : [{epoch}/{opt.epochs}] loss : {valid_loss}  f1 : {valid_macro_f1}\")\n",
    "\n",
    "        if valid_loss < best_loss :\n",
    "            best_f1 = valid_macro_f1\n",
    "            os.makedirs(opt.save_path, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(opt.save_path, f'{epoch}E_{round(valid_loss, 4)}_{opt.model_name}.pt'))\n",
    "        \n",
    "        else :\n",
    "            early_stopping_step += 1\n",
    "            print(f\"Early Stopping Step : [{early_stopping_step} / {opt.early_stopping}]\")\n",
    "            \n",
    "        if early_stopping_step == opt.early_stopping :\n",
    "            print(\"=== Early Stop ===\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7cc477d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▍                                                                           | 320/5766 [00:04<01:24, 64.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-f6e9df659436>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# csv_feature_dict 옵션 추가\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv_feature_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsv_feature_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# label_enc, dec 및 trasnforms 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-6c38becc197a>\u001b[0m in \u001b[0;36mcsv_feature_dict\u001b[1;34m(path, csv_features)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# feature 별 최대값, 최솟값 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcsv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mtemp_csv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcsv_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mtemp_csv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_csv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_csv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1211\u001b[0m             \u001b[0mnew_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_currow\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnew_rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         ]\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0maxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_arrays\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   1690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1692\u001b[1;33m         \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mform_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1693\u001b[0m         \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mform_blocks\u001b[1;34m(arrays, names, axes)\u001b[0m\n\u001b[0;32m   1748\u001b[0m     \u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"FloatBlock\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1750\u001b[1;33m         \u001b[0mfloat_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_multi_blockify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"FloatBlock\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1751\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_multi_blockify\u001b[1;34m(tuples, dtype)\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup_block\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1843\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1844\u001b[1;33m         \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_stack_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup_block\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_stack_arrays\u001b[1;34m(tuples, dtype)\u001b[0m\n\u001b[0;32m   1872\u001b[0m     \u001b[0mstacked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1873\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1874\u001b[1;33m         \u001b[0mstacked\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1876\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstacked\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 변수 설명 csv 파일 참조\n",
    "crop = {'1':'딸기','2':'토마토','3':'파프리카','4':'오이','5':'고추','6':'시설포도'}\n",
    "disease = {'1':{'a1':'딸기잿빛곰팡이병','a2':'딸기흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '2':{'a5':'토마토흰가루병','a6':'토마토잿빛곰팡이병','b2':'열과','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '3':{'a9':'파프리카흰가루병','a10':'파프리카잘록병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '4':{'a3':'오이노균병','a4':'오이흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '5':{'a7':'고추탄저병','a8':'고추흰가루병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '6':{'a11':'시설포도탄저병','a12':'시설포도노균병','b4':'일소피해','b5':'축과병'}}\n",
    "risk = {'1':'초기','2':'중기','3':'말기'}\n",
    "\n",
    "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
    "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
    "\n",
    "\n",
    "opt = {\"dataset_path\" : \"../data/train\",\n",
    "       \"label_path\" : \"../data/train.csv\",\n",
    "       \"save_path\" : \"../pretrain\",\n",
    "       \"pretrained_path\" : None,\n",
    "       \"batch_size\" : 16,\n",
    "       \"use_kfold\" : True,\n",
    "       \"kfold_splits\" : 4,\n",
    "       \"model_name\" : 'deit_small_patch16_224',\n",
    "       \"resize\" : 224,\n",
    "       \"num_classes\" : 38,\n",
    "       \"learning_rate\" : 1e-4,\n",
    "       \"early_stopping\" : 5,\n",
    "       \"device\" : \"cuda\", \n",
    "       \"csv_features\" : csv_features,\n",
    "       \"max_len\" : 590,\n",
    "       \"num_features\" : len(csv_features),\n",
    "       \"epochs\" : 1}\n",
    "\n",
    "opt = EasyDict(opt)\n",
    "\n",
    "# csv_feature_dict 옵션 추가\n",
    "opt.csv_feature_dict = csv_feature_dict(opt.dataset_path, opt.csv_features)\n",
    "\n",
    "# label_enc, dec 및 trasnforms 설정\n",
    "label_encoder, label_decoder = label_preprocessing(opt.label_path)\n",
    "train_transforms, valid_transforms = transform(size=opt.resize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf06b2c0",
   "metadata": {},
   "source": [
    "# Train Without kfold training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "323d20ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optin :  {'dataset_path': '../data/train', 'label_path': '../data/train.csv', 'save_path': '../pretrain', 'pretrained_path': None, 'batch_size': 16, 'model_name': 'deit_small_patch16_224', 'resize': 224, 'num_classes': 38, 'learning_rate': 0.0001, 'device': 'cpu', 'csv_features': ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저'], 'max_len': 590, 'num_features': 9, 'epochs': 1, 'csv_feature_dict': {'내부 온도 1 평균': [3.4, 47.3], '내부 온도 1 최고': [3.4, 47.6], '내부 온도 1 최저': [3.3, 47.0], '내부 습도 1 평균': [23.7, 100.0], '내부 습도 1 최고': [25.9, 100.0], '내부 습도 1 최저': [0.0, 100.0], '내부 이슬점 평균': [0.1, 34.5], '내부 이슬점 최고': [0.2, 34.7], '내부 이슬점 최저': [0.0, 34.4]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/289 [00:00<?, ?it/s]c:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\torch\\cuda\\amp\\autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "  0%|                      | 1/289 [00:12<1:00:38, 12.63s/it, Epoch=1, Mean train loss=3.648806, Mean train f1=0.000000]c:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\torch\\cuda\\amp\\autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "  1%|▏                       | 2/289 [00:23<55:00, 11.50s/it, Epoch=1, Mean train loss=3.595712, Mean train f1=0.000000]c:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\torch\\cuda\\amp\\autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "  1%|▏                       | 3/289 [00:34<55:08, 11.57s/it, Epoch=1, Mean train loss=3.490332, Mean train f1=0.013158]c:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\torch\\cuda\\amp\\autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "  1%|▎                       | 4/289 [00:47<56:12, 11.83s/it, Epoch=1, Mean train loss=3.444139, Mean train f1=0.015216]c:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\torch\\cuda\\amp\\autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "  2%|▍                       | 5/289 [00:59<56:58, 12.04s/it, Epoch=1, Mean train loss=3.454366, Mean train f1=0.016213]c:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\torch\\cuda\\amp\\autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "  2%|▍                       | 6/289 [01:09<52:42, 11.17s/it, Epoch=1, Mean train loss=3.402708, Mean train f1=0.020178]c:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\torch\\cuda\\amp\\autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "  2%|▍                     | 6/289 [01:22<1:04:27, 13.67s/it, Epoch=1, Mean train loss=3.402708, Mean train f1=0.020178]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f9c6c5678681>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-3e8f4285a573>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(train_loader, valid_loader, opt)\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_a\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlam\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_b\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# option 출력\n",
    "print(\"<< option >>\")\n",
    "print(*[\"{} : {}\".format(k, v) for k, v in opt.items()], sep='\\n')\n",
    "\n",
    "# data split with stratify\n",
    "train, valid = data_split(opt.dataset_path, label_decoder)\n",
    "\n",
    "train_dataset = CustomDataset(train, train_transforms, label_decoder, opt)\n",
    "valid_dataset = CustomDataset(valid, valid_transforms, label_decoder, opt)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=opt.batch_size, shuffle=False)\n",
    "\n",
    "run(train_loader, valid_loader, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e0713",
   "metadata": {},
   "source": [
    "# Train With kfold training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01ac1cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5767/5767 [00:02<00:00, 2491.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "===== k_fold : 1 / 4 =====\n",
      "[   0    1    2 ... 5764 5765 5766]\n",
      "<< option >>\n",
      "dataset_path : ../data/train\n",
      "label_path : ../data/train.csv\n",
      "save_path : ../pretrain\n",
      "pretrained_path : None\n",
      "batch_size : 16\n",
      "use_kfold : True\n",
      "kfold_splits : 4\n",
      "model_name : deit_small_patch16_224\n",
      "resize : 224\n",
      "num_classes : 38\n",
      "learning_rate : 0.0001\n",
      "device : cpu\n",
      "csv_features : ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
      "max_len : 590\n",
      "num_features : 9\n",
      "epochs : 1\n",
      "csv_feature_dict : {'내부 온도 1 평균': [3.4, 47.3], '내부 온도 1 최고': [3.4, 47.6], '내부 온도 1 최저': [3.3, 47.0], '내부 습도 1 평균': [23.7, 100.0], '내부 습도 1 최고': [25.9, 100.0], '내부 습도 1 최저': [0.0, 100.0], '내부 이슬점 평균': [0.1, 34.5], '내부 이슬점 최고': [0.2, 34.7], '내부 이슬점 최저': [0.0, 34.4]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/271 [00:00<?, ?it/s]c:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\torch\\cuda\\amp\\autocast_mode.py:120: UserWarning: torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.autocast only affects CUDA ops, but CUDA is not available.  Disabling.\")\n",
      "  0%|                                                                                           | 0/271 [00:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-8ee0ac22f38a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mvalid_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-a38fc19410c6>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(train_loader, valid_loader, opt)\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_a\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlam\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_b\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sub\\anaconda3\\envs\\torch-sub\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# option 출력\n",
    "print(\"<< option >>\")\n",
    "print(*[\"{} : {}\".format(k, v) for k, v in opt.items()], sep='\\n')\n",
    "\n",
    "img_list, label_list = data_split(opt.dataset_path, label_decoder, kfold=True)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=opt.kfold_splits, random_state=13, shuffle=True)\n",
    "for k, (fold_train, fold_valid) in enumerate(kfold.split(img_list, label_list), 1) :\n",
    "    \n",
    "    print(f\"\\n\\n\\n===== k_fold : {k} / {opt.kfold_splits} =====\")\n",
    "    train_dataset = CustomDataset(fold_train, train_transforms, label_decoder, opt)\n",
    "    valid_dataset = CustomDataset(fold_valid, valid_transforms, label_decoder, opt)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=opt.batch_size, shuffle=False)\n",
    "\n",
    "    run(train_loader, valid_loader, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee5772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c453e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
