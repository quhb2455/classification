{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4636868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import json \n",
    "import timm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa2ce4",
   "metadata": {},
   "source": [
    "# Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd44a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 설명 csv 파일 참조\n",
    "crop = {'1':'딸기','2':'토마토','3':'파프리카','4':'오이','5':'고추','6':'시설포도'}\n",
    "disease = {'1':{'a1':'딸기잿빛곰팡이병','a2':'딸기흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '2':{'a5':'토마토흰가루병','a6':'토마토잿빛곰팡이병','b2':'열과','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '3':{'a9':'파프리카흰가루병','a10':'파프리카잘록병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '4':{'a3':'오이노균병','a4':'오이흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '5':{'a7':'고추탄저병','a8':'고추흰가루병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '6':{'a11':'시설포도탄저병','a12':'시설포도노균병','b4':'일소피해','b5':'축과병'}}\n",
    "risk = {'1':'초기','2':'중기','3':'말기'}\n",
    "\n",
    "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
    "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
    "\n",
    "def label_preprocessing(path) :\n",
    "    labels = pd.read_csv(path)\n",
    "\n",
    "    cnt = 0\n",
    "    label_encoder = {}\n",
    "    for i, label in enumerate(tqdm(sorted(labels['label']))) :\n",
    "        \n",
    "        if label not in label_encoder.values() :\n",
    "            label_encoder[cnt] = label\n",
    "            cnt += 1\n",
    "        \n",
    "    label_decoder = {val : key for key, val in label_encoder.items()}\n",
    "    \n",
    "    return label_encoder, label_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45adac9d",
   "metadata": {},
   "source": [
    "# CSV feature - min, max value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "195590dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_feature_dict(path, csv_features) :\n",
    "    \n",
    "    csv_files = sorted(glob(os.path.join(path, '*/*.csv')))\n",
    "\n",
    "    temp_csv = pd.read_csv(csv_files[0])[csv_features]\n",
    "    max_arr, min_arr = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "\n",
    "    # feature 별 최대값, 최솟값 계산\n",
    "    for csv in tqdm(csv_files[1:]):\n",
    "        temp_csv = pd.read_csv(csv)[csv_features]\n",
    "        temp_csv = temp_csv.replace('-',np.nan).dropna()\n",
    "        if len(temp_csv) == 0:\n",
    "            continue\n",
    "        temp_csv = temp_csv.astype(float)\n",
    "        temp_max, temp_min = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "        max_arr = np.max([max_arr,temp_max], axis=0)\n",
    "        min_arr = np.min([min_arr,temp_min], axis=0)\n",
    "\n",
    "    # feature 별 최대값, 최솟값 dictionary return\n",
    "    return {csv_features[i]:[min_arr[i], max_arr[i]] for i in range(len(csv_features))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85eeaead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 5766/5766 [02:26<00:00, 39.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'내부 온도 1 평균': [3.4, 47.3],\n",
       " '내부 온도 1 최고': [3.4, 47.6],\n",
       " '내부 온도 1 최저': [3.3, 47.0],\n",
       " '내부 습도 1 평균': [23.7, 100.0],\n",
       " '내부 습도 1 최고': [25.9, 100.0],\n",
       " '내부 습도 1 최저': [0.0, 100.0],\n",
       " '내부 이슬점 평균': [0.1, 34.5],\n",
       " '내부 이슬점 최고': [0.2, 34.7],\n",
       " '내부 이슬점 최저': [0.0, 34.4]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_feature_dict = csv_feature_dict('../data/train', csv_features)\n",
    "csv_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9299d9ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(294, 9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(512, 9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_path = '../data/train/10037/10037.csv'\n",
    "df = pd.read_csv(csv_path)[csv_feature_dict.keys()]\n",
    "df = df.replace('-', 0)\n",
    "\n",
    "# MinMax scaling\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].astype(float) - csv_feature_dict[col][0]\n",
    "    df[col] = df[col] / (csv_feature_dict[col][1]-csv_feature_dict[col][0])\n",
    "\n",
    "display(len(df))\n",
    "\n",
    "df_dict = {}\n",
    "for key in csv_feature_dict.keys() :\n",
    "    df_dict[key] = np.max(df[key].to_numpy(), -1)\n",
    "\n",
    "after_df = df.to_numpy()\n",
    "df_len, df_features = after_df.shape\n",
    "\n",
    "display(df.to_numpy().shape)\n",
    "x        = np.zeros([512, df_features])\n",
    "display(x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69b5c3",
   "metadata": {},
   "source": [
    "# Custom Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "02b2705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 files, \n",
    "                 transforms, \n",
    "                 label_encoder, \n",
    "                 csv_feature_dict, \n",
    "                 max_len,\n",
    "                 num_features,\n",
    "                 mode='train'):\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.files = files\n",
    "        self.label_encoder = label_encoder #label_encoder\n",
    "        self.csv_feature_dict = csv_feature_dict\n",
    "        self.max_len = max_len\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        file = self.files[i]\n",
    "        file_name = file.split('\\\\')[-1]\n",
    "        \n",
    "        # CSV\n",
    "        csv_data = csv_preprocessing(file, file_name)\n",
    "        \n",
    "        # image\n",
    "        img = img_preprocessing(file, file_name)\n",
    "        \n",
    "        if self.mode == 'train':         \n",
    "            # Label\n",
    "            label = label_preprocessing(file, file_name)\n",
    "            \n",
    "            return {\n",
    "                'img' : torch.tensor(img, dtype=torch.float32),\n",
    "                'label' : torch.tensor(self.label_encoder[label], dtype=torch.long),\n",
    "                'csv_feature': torch.tensor(x, dtype=torch.float32)\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            return {\n",
    "                'img' : torch.tensor(img, dtype=torch.float32),\n",
    "                'csv_feature': torch.tensor(x, dtype=torch.float32)\n",
    "            }\n",
    "\n",
    "    def csv_preprocessing(self, file, file_name) :\n",
    "        # CSV\n",
    "        csv_path = f'{file}/{file_name}.csv'\n",
    "        df = pd.read_csv(csv_path)[self.csv_feature_dict.keys()]\n",
    "        df = df.replace('-', 0)\n",
    "        \n",
    "        # MinMax scaling\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].astype(float) - self.csv_feature_dict[col][0]\n",
    "            df[col] = df[col] / (self.csv_feature_dict[col][1]-self.csv_feature_dict[col][0])\n",
    "\n",
    "        seq_len = len(df)\n",
    "        df_np = df.to_numpy()\n",
    "        df_len, df_features = df_np.shape\n",
    "        \n",
    "        csv_data = np.zeros([self.max_len, df_features])\n",
    "        csv_data[0:df_len, :] = df_np\n",
    "        \n",
    "        return csv_data\n",
    "\n",
    "    def img_preprocessing(self, file, file_name) :\n",
    "        image_path = f'{file}/{file_name}.jpg'\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = self.transforms(image=img)[\"image\"]\n",
    "        img = img.transpose(2,0,1)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def label_preprocessing(self, file, file_name) :\n",
    "        json_path = f'{file}/{file_name}.json'\n",
    "        with open(json_path, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "\n",
    "        crop = json_file['annotations']['crop']\n",
    "        disease = json_file['annotations']['disease']\n",
    "        risk = json_file['annotations']['risk']\n",
    "        \n",
    "        return f'{crop}_{disease}_{risk}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce6704d",
   "metadata": {},
   "source": [
    "# Model - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e1580252",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CNN_Encoder(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, pretrained_path=None):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        \n",
    "        if pretrained_path :\n",
    "            # no use pretrained model trained with Public dataset\n",
    "            self.model = self.create_pretrained_model(model_name, num_classes, pretrained_path)\n",
    "            \n",
    "        else :            \n",
    "            self.model = timm.create_model(model_name, num_classes=num_classes, pretrained=True)\n",
    "            \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        output = self.model(inputs)\n",
    "        return output\n",
    "\n",
    "    def create_pretrained_model(self, model_name, num_classes, pretrained_path):\n",
    "        pre_model = torch.load(pretrained_path, map_location=\"cpu\")\n",
    "        output_size = pre_model[list(pre_model.keys())[-1]].shape[0]\n",
    " \n",
    "        return nn.Sequential(\n",
    "                    timm.create_model(model_name, num_classes=output_size, pretrained=True),\n",
    "                    nn.Linear(output_size, num_classes)\n",
    "                )\n",
    "\n",
    "# model = CNN_Encoder(\"efficientnetv2_rw_s\", 1000)\n",
    "# model = CNN_Encoder(\"efficientnetv2_rw_s\", 1000, \"../model/k_fold_50k_pretrained_effiv2S/4_f9462_public_vill_50k_pretrain_efficientnetv2S.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5f419",
   "metadata": {},
   "source": [
    "# Model - RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0db46531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class MLSTMfcn(nn.Module):\n",
    "    def __init__(self, *, num_classes, max_seq_len, num_features,\n",
    "                 num_lstm_out=128, num_lstm_layers=1, \n",
    "                 conv1_nf=128, conv2_nf=256, conv3_nf=128,\n",
    "                 lstm_drop_p=0.8, fc_drop_p=0.3, cnn_out=1000):\n",
    "        \n",
    "        super(MLSTMfcn, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_features = num_features\n",
    "\n",
    "        self.num_lstm_out = num_lstm_out\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "\n",
    "        self.conv1_nf = conv1_nf\n",
    "        self.conv2_nf = conv2_nf\n",
    "        self.conv3_nf = conv3_nf\n",
    "\n",
    "        self.lstm_drop_p = lstm_drop_p\n",
    "        self.fc_drop_p = fc_drop_p\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.num_features, \n",
    "                            hidden_size=self.num_lstm_out,\n",
    "                            num_layers=self.num_lstm_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(self.num_features, self.conv1_nf, 8)\n",
    "        self.conv2 = nn.Conv1d(self.conv1_nf, self.conv2_nf, 5)\n",
    "        self.conv3 = nn.Conv1d(self.conv2_nf, self.conv3_nf, 3)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(self.conv1_nf)\n",
    "        self.bn2 = nn.BatchNorm1d(self.conv2_nf)\n",
    "        self.bn3 = nn.BatchNorm1d(self.conv3_nf)\n",
    "\n",
    "        self.se1 = SELayer(self.conv1_nf)  # ex 128\n",
    "        self.se2 = SELayer(self.conv2_nf)  # ex 256\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstmDrop = nn.Dropout(self.lstm_drop_p)\n",
    "        self.convDrop = nn.Dropout(self.fc_drop_p)\n",
    "\n",
    "        self.fc = nn.Linear(self.conv3_nf+self.num_lstm_out, 128)\n",
    "\n",
    "        self.out_layer = nn.Linear(cnn_out+128, self.num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, enc_out, x, seq_lens):\n",
    "        ''' input x should be in size [B,T,F], where \n",
    "            B = Batch size\n",
    "            T = Time samples\n",
    "            F = features\n",
    "        '''\n",
    "        x1 = nn.utils.rnn.pack_padded_sequence(x, seq_lens.cpu(), \n",
    "                                               batch_first=True, \n",
    "                                               enforce_sorted=False)\n",
    "        x1, (ht,ct) = self.lstm(x1)\n",
    "        x1, _ = nn.utils.rnn.pad_packed_sequence(x1, batch_first=True, \n",
    "                                                 padding_value=0.0)\n",
    "        x1 = x1[:,-1,:]\n",
    "        \n",
    "        x2 = x.transpose(2,1)\n",
    "        x2 = self.convDrop(self.relu(self.bn1(self.conv1(x2))))\n",
    "        x2 = self.se1(x2)\n",
    "        x2 = self.convDrop(self.relu(self.bn2(self.conv2(x2))))\n",
    "        x2 = self.se2(x2)\n",
    "        x2 = self.convDrop(self.relu(self.bn3(self.conv3(x2))))\n",
    "        x2 = torch.mean(x2,2)\n",
    "        \n",
    "        x_all = torch.cat((x1,x2),dim=1)\n",
    "        x_out = self.fc(x_all)\n",
    "        concat = torch.cat([enc_out, x_out], dim=1)  # enc_out + hidden \n",
    "        output = self.dropout(concat)\n",
    "        x_output = self.out_layer(output)\n",
    "        x_out = F.log_softmax(x_output, dim=1)\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "# model = MLSTMfcn(num_classes=38, max_seq_len=512, num_features=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100c687",
   "metadata": {},
   "source": [
    "# Model - CNN + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7b59e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2RNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        num_classes,\n",
    "        max_len,\n",
    "        num_features,\n",
    "        pretrained_path=None,\n",
    "    ):\n",
    "        super(CNN2RNN, self).__init__()\n",
    "        \n",
    "        self.cnn = CNN_Encoder(model_name, num_classes, pretrained_path)\n",
    "        self.rnn = MLSTMfcn(num_classes=num_classes, max_seq_len=max_len, num_features=num_features)\n",
    "\n",
    "\n",
    "    def forward(self, img, seq, seq_len):\n",
    "        cnn_output = self.cnn(img)\n",
    "        output = self.rnn(cnn_output, seq, seq_len)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# model = CNN2RNN(\"efficientnetv2_rw_s\", \n",
    "#                 1000, \n",
    "#                 512,\n",
    "#                 6,\n",
    "#                 \"../model/k_fold_50k_pretrained_effiv2S/4_f9462_public_vill_50k_pretrain_efficientnetv2S.pt\")\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc477d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
