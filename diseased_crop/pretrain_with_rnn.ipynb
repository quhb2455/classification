{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7759e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import json \n",
    "import timm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "# from torchsampler import ImbalancedDatasetSampler\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e9e49",
   "metadata": {},
   "source": [
    "# Train dataset 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c080e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 설명 csv 파일 참조\n",
    "crop = {'1':'딸기','2':'토마토','3':'파프리카','4':'오이','5':'고추','6':'시설포도'}\n",
    "disease = {'1':{'a1':'딸기잿빛곰팡이병','a2':'딸기흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '2':{'a5':'토마토흰가루병','a6':'토마토잿빛곰팡이병','b2':'열과','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '3':{'a9':'파프리카흰가루병','a10':'파프리카잘록병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '4':{'a3':'오이노균병','a4':'오이흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '5':{'a7':'고추탄저병','a8':'고추흰가루병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '6':{'a11':'시설포도탄저병','a12':'시설포도노균병','b4':'일소피해','b5':'축과병'}}\n",
    "risk = {'1':'초기','2':'중기','3':'말기'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea5b2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1_00_0', '딸기_정상'),\n",
       " ('1_a1_1', '딸기_딸기잿빛곰팡이병_초기'),\n",
       " ('1_a1_2', '딸기_딸기잿빛곰팡이병_중기'),\n",
       " ('1_a1_3', '딸기_딸기잿빛곰팡이병_말기'),\n",
       " ('1_a2_1', '딸기_딸기흰가루병_초기'),\n",
       " ('1_a2_2', '딸기_딸기흰가루병_중기'),\n",
       " ('1_a2_3', '딸기_딸기흰가루병_말기'),\n",
       " ('1_b1_1', '딸기_냉해피해_초기'),\n",
       " ('1_b1_2', '딸기_냉해피해_중기'),\n",
       " ('1_b1_3', '딸기_냉해피해_말기')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_description = {}\n",
    "for key, value in disease.items():\n",
    "    label_description[f'{key}_00_0'] = f'{crop[key]}_정상'\n",
    "    for disease_code in value:\n",
    "        for risk_code in risk:\n",
    "            label = f'{key}_{disease_code}_{risk_code}'\n",
    "            label_description[label] = f'{crop[key]}_{disease[key][disease_code]}_{risk[risk_code]}'\n",
    "list(label_description.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e87034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_feature_dict = {\n",
    "    '내부 온도 1 평균': [3.4, 47.3],\n",
    "    '내부 온도 1 최고': [3.4, 47.6],\n",
    "    '내부 온도 1 최저': [3.3, 47.0],\n",
    "    '내부 습도 1 평균': [23.7, 100.0],\n",
    "    '내부 습도 1 최고': [25.9, 100.0],\n",
    "    '내부 습도 1 최저': [0.0, 100.0],\n",
    "    '내부 이슬점 평균': [0.1, 34.5],\n",
    "    '내부 이슬점 최고': [0.2, 34.7],\n",
    "    '내부 이슬점 최저': [0.0, 34.4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "380c69ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 5767/5767 [00:00<00:00, 963764.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: '1_00_0',\n",
       " 1: '2_00_0',\n",
       " 2: '2_a5_2',\n",
       " 3: '3_00_0',\n",
       " 4: '3_a9_1',\n",
       " 5: '3_a9_2',\n",
       " 6: '3_a9_3',\n",
       " 7: '3_b3_1',\n",
       " 8: '3_b6_1',\n",
       " 9: '3_b7_1',\n",
       " 10: '3_b8_1',\n",
       " 11: '4_00_0',\n",
       " 12: '5_00_0',\n",
       " 13: '5_a7_2',\n",
       " 14: '5_b6_1',\n",
       " 15: '5_b7_1',\n",
       " 16: '5_b8_1',\n",
       " 17: '6_00_0',\n",
       " 18: '6_a11_1',\n",
       " 19: '6_a11_2',\n",
       " 20: '6_a12_1',\n",
       " 21: '6_a12_2',\n",
       " 22: '6_b4_1',\n",
       " 23: '6_b4_3',\n",
       " 24: '6_b5_1'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'1_00_0': 0,\n",
       " '2_00_0': 1,\n",
       " '2_a5_2': 2,\n",
       " '3_00_0': 3,\n",
       " '3_a9_1': 4,\n",
       " '3_a9_2': 5,\n",
       " '3_a9_3': 6,\n",
       " '3_b3_1': 7,\n",
       " '3_b6_1': 8,\n",
       " '3_b7_1': 9,\n",
       " '3_b8_1': 10,\n",
       " '4_00_0': 11,\n",
       " '5_00_0': 12,\n",
       " '5_a7_2': 13,\n",
       " '5_b6_1': 14,\n",
       " '5_b7_1': 15,\n",
       " '5_b8_1': 16,\n",
       " '6_00_0': 17,\n",
       " '6_a11_1': 18,\n",
       " '6_a11_2': 19,\n",
       " '6_a12_1': 20,\n",
       " '6_a12_2': 21,\n",
       " '6_b4_1': 22,\n",
       " '6_b4_3': 23,\n",
       " '6_b5_1': 24}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============= add\n",
    "labels = pd.read_csv('./data/train.csv')\n",
    "\n",
    "train_label_encoder = {}\n",
    "label_cnt = 0\n",
    "previous_label = '0_00_0'\n",
    "for i, label in enumerate(tqdm(sorted(labels['label']))) :\n",
    "    crop_val = label.split('_')[0] # crop\n",
    "    disease_val = label.split('_')[1] # disease\n",
    "    risk_val = label.split('_')[2] # risk\n",
    "    \n",
    "    tmp_label = f'{crop_val}_{disease_val}_{risk_val}'\n",
    "    if previous_label != tmp_label :\n",
    "        train_label_encoder[tmp_label] = label_cnt\n",
    "        previous_label = tmp_label\n",
    "        label_cnt += 1\n",
    "        \n",
    "train_label_decoder = {val : key for key, val in train_label_encoder.items()}\n",
    "display(train_label_decoder)\n",
    "display(train_label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d313c9",
   "metadata": {},
   "source": [
    "# Custom dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b22048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        files, \n",
    "        csv_feature_dict, \n",
    "        label_encoder,\n",
    "        labels=None,\n",
    "        transforms=None,\n",
    "        mode='train',\n",
    "    ):\n",
    "        self.mode = mode\n",
    "        self.files = files\n",
    "        \n",
    "        self.csv_feature_dict = csv_feature_dict\n",
    "        \n",
    "        if files is not None:\n",
    "            self.csv_feature_check = [0]*len(self.files)\n",
    "            self.csv_features = [None]*len(self.files)\n",
    "            \n",
    "        self.max_len = MAX_LEN\n",
    "        self.num_features = NUM_FEATURES\n",
    "        \n",
    "        self.label_encoder = label_encoder\n",
    "        \n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        file = self.files[i]\n",
    "        file_name = file.split(os.sep)[-1]\n",
    "        \n",
    "        # csv\n",
    "        csv_path = f'{file}/{file_name}.csv'\n",
    "        df = pd.read_csv(csv_path)[self.csv_feature_dict.keys()]\n",
    "        df = df.replace('-', 0)\n",
    "\n",
    "        # MinMax scaling\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].astype(float) - self.csv_feature_dict[col][0]\n",
    "            df[col] = df[col] / (self.csv_feature_dict[col][1]-self.csv_feature_dict[col][0])\n",
    "\n",
    "        df_dict = {}\n",
    "        df_dict['내부 온도 1 최저'] = np.max(df[['내부 온도 1 최저']].to_numpy(), -1)\n",
    "        df_dict['내부 온도 1 평균'] = np.max(df[['내부 온도 1 평균']].to_numpy(), -1)\n",
    "        df_dict['내부 온도 1 최고'] = np.max(df[['내부 온도 1 최고']].to_numpy(), -1)\n",
    "        df_dict['내부 습도 1 최저'] = np.max(df[['내부 습도 1 최저']].to_numpy(), -1)\n",
    "        df_dict['내부 습도 1 평균'] = np.max(df[['내부 습도 1 평균']].to_numpy(), -1)\n",
    "        df_dict['내부 습도 1 최고'] = np.max(df[['내부 습도 1 최고']].to_numpy(), -1)\n",
    "        df_dict['내부 이슬점 최저'] = np.max(df[['내부 이슬점 최저']].to_numpy(), -1)\n",
    "        df_dict['내부 이슬점 평균'] = np.max(df[['내부 이슬점 평균']].to_numpy(), -1)\n",
    "        df_dict['내부 이슬점 최고'] = np.max(df[['내부 이슬점 최고']].to_numpy(), -1)\n",
    "        seq_len = len(df_dict['내부 온도 1 최저'])\n",
    "        df = pd.DataFrame(df_dict)\n",
    "\n",
    "        after_df = pd.DataFrame(df_dict).to_numpy()\n",
    "        x        = np.zeros([self.max_len, self.num_features])\n",
    "\n",
    "        h, w      = after_df.shape\n",
    "        x[0:h, :] = after_df\n",
    "        \n",
    "        # image\n",
    "        image_path = f'{file}/{file_name}.jpg'\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        ### Image Padding 생략\n",
    "#         h,w,c = img.shape\n",
    "#         if w < h: # width padding\n",
    "#             pad_w = (h - w) // 2\n",
    "#             pad_img = np.zeros([h,h,3]).astype(np.uint8)\n",
    "#             pad_img[:,pad_w:pad_w+w] = img\n",
    "#             img = pad_img\n",
    "#         elif w > h: # height padding\n",
    "#             pad_h = (w - h) // 2\n",
    "#             pad_img = np.zeros([w,w,3]).astype(np.uint8)\n",
    "#             pad_img[pad_h:pad_h+h,:] = img\n",
    "#             img = pad_img\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(image=img)['image']\n",
    "        img = img.transpose(2,0,1)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            json_path = f'{file}/{file_name}.json'\n",
    "            with open(json_path, 'r') as f:\n",
    "                json_file = json.load(f)\n",
    "            \n",
    "            crop = json_file['annotations']['crop']\n",
    "            disease = json_file['annotations']['disease']\n",
    "            risk = json_file['annotations']['risk']\n",
    "            label = f'{crop}_{disease}_{risk}'\n",
    "            \n",
    "            return {\n",
    "                'img': torch.tensor(img, dtype=torch.float32) / 255.0,\n",
    "                'csv_feature': torch.tensor(x, dtype=torch.float32),\n",
    "                'seq_len' : seq_len,\n",
    "                'label': torch.tensor(self.label_encoder[label], dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'img': torch.tensor(img, dtype=torch.float32) / 255.0,\n",
    "                'seq_len' : seq_len,\n",
    "                'csv_feature': torch.tensor(x, dtype=torch.float32)\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bbab88",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343230c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EffiV2S(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EffiV2S, self).__init__()\n",
    "        self.model = timm.create_model('efficientnetv2_rw_s', pretrained=True, num_classes=38)\n",
    "#         self.model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output = self.model(inputs)\n",
    "        return output\n",
    "    \n",
    "class CNN_ENCODER(nn.Module) :\n",
    "    def __init__(self, model_path):\n",
    "        super(CNN_ENCODER, self).__init__()\n",
    "        self.model = EffiV2S()\n",
    "        self.model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        self.model.to(device)\n",
    "        self.model.model.classifier = nn.Linear(1792, 1000, bias=True).to(device)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output = self.model(inputs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1e7695",
   "metadata": {},
   "source": [
    "### model load test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b2579cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = torch.randn(1,3,288, 288, dtype=torch.float32).to(device)\n",
    "# model_path = './model/public_vill_50k_pretrain_EffiV2S.pt'\n",
    "# model = CNN_ENCODER('./model/public_vill_50k_pretrain_EffiV2S.pt').to(device)\n",
    "# a = model(data)\n",
    "# a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c4a26",
   "metadata": {},
   "source": [
    "# RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a6e0a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class MLSTMfcn(nn.Module):\n",
    "    def __init__(self, *, num_classes, max_seq_len, num_features,\n",
    "                 num_lstm_out=128, num_lstm_layers=1, \n",
    "                 conv1_nf=128, conv2_nf=256, conv3_nf=128,\n",
    "                 lstm_drop_p=0.8, fc_drop_p=0.3):\n",
    "        \n",
    "        super(MLSTMfcn, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_features = num_features\n",
    "\n",
    "        self.num_lstm_out = num_lstm_out\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "\n",
    "        self.conv1_nf = conv1_nf\n",
    "        self.conv2_nf = conv2_nf\n",
    "        self.conv3_nf = conv3_nf\n",
    "\n",
    "        self.lstm_drop_p = lstm_drop_p\n",
    "        self.fc_drop_p = fc_drop_p\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.num_features, \n",
    "                            hidden_size=self.num_lstm_out,\n",
    "                            num_layers=self.num_lstm_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(self.num_features, self.conv1_nf, 8)\n",
    "        self.conv2 = nn.Conv1d(self.conv1_nf, self.conv2_nf, 5)\n",
    "        self.conv3 = nn.Conv1d(self.conv2_nf, self.conv3_nf, 3)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(self.conv1_nf)\n",
    "        self.bn2 = nn.BatchNorm1d(self.conv2_nf)\n",
    "        self.bn3 = nn.BatchNorm1d(self.conv3_nf)\n",
    "\n",
    "        self.se1 = SELayer(self.conv1_nf)  # ex 128\n",
    "        self.se2 = SELayer(self.conv2_nf)  # ex 256\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstmDrop = nn.Dropout(self.lstm_drop_p)\n",
    "        self.convDrop = nn.Dropout(self.fc_drop_p)\n",
    "\n",
    "        self.fc = nn.Linear(self.conv3_nf+self.num_lstm_out, 128)\n",
    "\n",
    "        self.out_layer = nn.Linear(1000+128, self.num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, enc_out, x, seq_lens):\n",
    "        ''' input x should be in size [B,T,F], where \n",
    "            B = Batch size\n",
    "            T = Time samples\n",
    "            F = features\n",
    "        '''\n",
    "        x1 = nn.utils.rnn.pack_padded_sequence(x, seq_lens.cpu(), \n",
    "                                               batch_first=True, \n",
    "                                               enforce_sorted=False)\n",
    "        x1, (ht,ct) = self.lstm(x1)\n",
    "        x1, _ = nn.utils.rnn.pad_packed_sequence(x1, batch_first=True, \n",
    "                                                 padding_value=0.0)\n",
    "        x1 = x1[:,-1,:]\n",
    "        \n",
    "        x2 = x.transpose(2,1)\n",
    "        x2 = self.convDrop(self.relu(self.bn1(self.conv1(x2))))\n",
    "        x2 = self.se1(x2)\n",
    "        x2 = self.convDrop(self.relu(self.bn2(self.conv2(x2))))\n",
    "        x2 = self.se2(x2)\n",
    "        x2 = self.convDrop(self.relu(self.bn3(self.conv3(x2))))\n",
    "        x2 = torch.mean(x2,2)\n",
    "        \n",
    "        x_all = torch.cat((x1,x2),dim=1)\n",
    "        x_out = self.fc(x_all)\n",
    "        concat = torch.cat([enc_out, x_out], dim=1)  # enc_out + hidden \n",
    "        output = self.dropout(concat)\n",
    "        x_output = self.out_layer(output)\n",
    "        x_out = F.log_softmax(x_output, dim=1)\n",
    "\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b366065",
   "metadata": {},
   "source": [
    "# CNN + RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a18b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2RNNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path,\n",
    "        n_classes\n",
    "    ):\n",
    "        super(CNN2RNNModel, self).__init__()\n",
    "        \n",
    "        self.cnn = CNN_ENCODER(model_path)\n",
    "        self.rnn = MLSTMfcn(num_classes=n_classes, max_seq_len=MAX_LEN, num_features=NUM_FEATURES)\n",
    "\n",
    "\n",
    "    def forward(self, img, seq, seq_len):\n",
    "        cnn_output = self.cnn(img)\n",
    "        output = self.rnn(cnn_output, seq, seq_len)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd690b46",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eeba8e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    " \n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1667c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_function(real, pred):    \n",
    "    real = real.cpu()\n",
    "    pred = torch.argmax(pred, dim=1).cpu()\n",
    "    score = f1_score(real, pred, average='macro')\n",
    "    return score\n",
    "\n",
    "def train_step(batch_item, training):\n",
    "    img = batch_item['img'].to(device)\n",
    "    csv_feature  = batch_item['csv_feature'].to(device)\n",
    "    seq_len  = batch_item['seq_len'].to(device)\n",
    "    label = batch_item['label'].to(device)\n",
    "\n",
    "    lam = np.random.beta(1.0, 1.0)\n",
    "    \n",
    "    if training is True:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # add - cutmix\n",
    "            rand_index = torch.randperm(img.size()[0])\n",
    "            target_a = label\n",
    "            target_b = label[rand_index]\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(img.size(), lam)\n",
    "            img[:, :, bbx1:bbx2, bby1:bby2] = img[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (img.size()[-1] * img.size()[-2]))\n",
    "            \n",
    "            output = model(img, csv_feature, seq_len)\n",
    "#             loss = criterion(output, label)\n",
    "            loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1. - lam)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        score = accuracy_function(label, output)\n",
    "        return loss, score\n",
    "    \n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(img, csv_feature, seq_len)\n",
    "            loss = criterion(output, label)\n",
    "        score = accuracy_function(label, output)\n",
    "        return loss, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663938c2",
   "metadata": {},
   "source": [
    "# Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd294abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "n_classes = len(train_label_encoder)\n",
    "learning_rate = 1e-3\n",
    "NUM_FEATURES  = len(csv_feature_dict)\n",
    "MAX_LEN  = 590\n",
    "DROPOUT_RATE  = 0.1\n",
    "epochs = 25\n",
    "device = torch.device(\"cuda:0\")#(\"cpu\")\n",
    "early_stopping_cnt = 4\n",
    "save_path = 'public_vill_50k_pretrain_Effi_1000_CNN2RNN.pt'\n",
    "model_path = './model/public_vill_50k_pretrain_EffiV2S.pt'\n",
    "fold_n = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a81ad2",
   "metadata": {},
   "source": [
    "# model이랑 loss 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d15aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN2RNNModel(model_path, n_classes)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=5e-2)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=0.0001) \n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "897c4b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 5767/5767 [00:01<00:00, 3144.68it/s]\n"
     ]
    }
   ],
   "source": [
    "json_path = glob('./data/train/*/*.json')\n",
    "\n",
    "label_list = []\n",
    "for path in tqdm(json_path) :\n",
    "    json_file = json.load(open(path, 'r'))\n",
    "    \n",
    "    crop = json_file['annotations']['crop']\n",
    "    disease = json_file['annotations']['disease']\n",
    "    risk = json_file['annotations']['risk']\n",
    "    \n",
    "    label = f'{crop}_{disease}_{risk}'\n",
    "    label_list.append(train_label_encoder[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7db9ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transforms = A.Compose([\n",
    "#                 A.Resize(288, 288),\n",
    "#                 A.OneOf([\n",
    "#                     A.Rotate(),\n",
    "#                     A.HorizontalFlip(),\n",
    "#                     A.VerticalFlip()\n",
    "#                 ], p=1)\n",
    "#             ])\n",
    "\n",
    "# val_transforms = A.Compose([\n",
    "#     A.Resize(288, 288)\n",
    "# ])\n",
    "\n",
    "# label_encoder = train_label_encoder\n",
    "\n",
    "# data_list = glob('./data/train/*')\n",
    "# print(\"total : \", len(data_list))\n",
    "\n",
    "# train, val = train_test_split(data_list, test_size=0.2, shuffle=True, stratify=label_list)\n",
    "\n",
    "# print(\"train : \", len(train))\n",
    "# print(\"val : \", len(val))\n",
    "\n",
    "# train_dataset = CustomDataset(\n",
    "#             train, \n",
    "#             csv_feature_dict,\n",
    "#             label_encoder,\n",
    "#             transforms=train_transforms,\n",
    "#         )\n",
    "# val_dataset = CustomDataset(\n",
    "#             val, \n",
    "#             csv_feature_dict,\n",
    "#             label_encoder,\n",
    "#             transforms=val_transforms,\n",
    "#         )\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset, \n",
    "#     batch_size=batch_size, \n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "# val_loader = DataLoader(\n",
    "#     val_dataset, \n",
    "#     batch_size=batch_size, \n",
    "#     shuffle=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bfa9001",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose([\n",
    "                A.Resize(288, 288),\n",
    "                A.OneOf([\n",
    "                    A.Rotate(),\n",
    "                    A.HorizontalFlip(),\n",
    "                    A.VerticalFlip()\n",
    "                ], p=1)\n",
    "            ])\n",
    "\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize(288, 288)\n",
    "])\n",
    "label_encoder = train_label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2e629e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = glob('./data/train/*')\n",
    "kfold = StratifiedKFold(n_splits=4, random_state=13, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095ea04",
   "metadata": {},
   "source": [
    "# K fold Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22ac98db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "===== k_fold : 1 / 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [02:40,  1.69it/s, Epoch=1, Loss=0.696115, Mean Loss=1.431567, Mean F-1=0.474266]\n",
      "91it [00:42,  2.14it/s, Epoch=1, Val Loss=1.955474, Mean Val Loss=0.416796, Mean Val F-1=0.719961]\n",
      "271it [02:34,  1.75it/s, Epoch=2, Loss=1.739481, Mean Loss=1.166044, Mean F-1=0.578668]\n",
      "91it [00:42,  2.14it/s, Epoch=2, Val Loss=0.519295, Mean Val Loss=0.483337, Mean Val F-1=0.736528]\n",
      "271it [02:35,  1.74it/s, Epoch=3, Loss=0.724580, Mean Loss=1.048112, Mean F-1=0.592015]\n",
      "91it [00:45,  2.02it/s, Epoch=3, Val Loss=0.570880, Mean Val Loss=0.316277, Mean Val F-1=0.776346]\n",
      "271it [02:40,  1.69it/s, Epoch=4, Loss=1.174053, Mean Loss=1.017561, Mean F-1=0.596064]\n",
      "91it [00:44,  2.05it/s, Epoch=4, Val Loss=0.043030, Mean Val Loss=0.270884, Mean Val F-1=0.775114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [02:41,  1.68it/s, Epoch=5, Loss=1.154722, Mean Loss=0.945627, Mean F-1=0.622562]\n",
      "91it [00:44,  2.06it/s, Epoch=5, Val Loss=1.275321, Mean Val Loss=0.219081, Mean Val F-1=0.830978]\n",
      "271it [02:41,  1.68it/s, Epoch=6, Loss=0.712279, Mean Loss=0.909005, Mean F-1=0.646492]\n",
      "91it [00:44,  2.04it/s, Epoch=6, Val Loss=0.721532, Mean Val Loss=0.180704, Mean Val F-1=0.849334]\n",
      "271it [02:42,  1.66it/s, Epoch=7, Loss=0.962823, Mean Loss=0.855021, Mean F-1=0.689815]\n",
      "91it [00:45,  2.01it/s, Epoch=7, Val Loss=0.055825, Mean Val Loss=0.150960, Mean Val F-1=0.859999]\n",
      "271it [02:45,  1.64it/s, Epoch=8, Loss=1.318704, Mean Loss=0.845946, Mean F-1=0.651970]\n",
      "91it [00:46,  1.97it/s, Epoch=8, Val Loss=0.341998, Mean Val Loss=0.157554, Mean Val F-1=0.870604]\n",
      "271it [02:48,  1.61it/s, Epoch=9, Loss=0.339760, Mean Loss=0.780437, Mean F-1=0.695388]\n",
      "91it [00:48,  1.89it/s, Epoch=9, Val Loss=1.426675, Mean Val Loss=0.154697, Mean Val F-1=0.883031]\n",
      "271it [02:50,  1.59it/s, Epoch=10, Loss=0.869062, Mean Loss=0.797110, Mean F-1=0.692973]\n",
      "91it [00:48,  1.89it/s, Epoch=10, Val Loss=0.024631, Mean Val Loss=0.137630, Mean Val F-1=0.899129]\n",
      "271it [02:53,  1.56it/s, Epoch=11, Loss=0.631414, Mean Loss=0.754940, Mean F-1=0.662797]\n",
      "91it [00:49,  1.85it/s, Epoch=11, Val Loss=0.030955, Mean Val Loss=0.119696, Mean Val F-1=0.895639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [02:57,  1.53it/s, Epoch=12, Loss=0.393348, Mean Loss=0.721996, Mean F-1=0.696704]\n",
      "91it [00:50,  1.80it/s, Epoch=12, Val Loss=0.051585, Mean Val Loss=0.108682, Mean Val F-1=0.917405]\n",
      "271it [03:01,  1.49it/s, Epoch=13, Loss=0.459046, Mean Loss=0.666658, Mean F-1=0.725043]\n",
      "91it [00:54,  1.67it/s, Epoch=13, Val Loss=0.097976, Mean Val Loss=0.109208, Mean Val F-1=0.922318]\n",
      "271it [03:08,  1.44it/s, Epoch=14, Loss=1.624780, Mean Loss=0.643076, Mean F-1=0.753538]\n",
      "91it [00:54,  1.68it/s, Epoch=14, Val Loss=0.049940, Mean Val Loss=0.086009, Mean Val F-1=0.928376]\n",
      "271it [03:12,  1.41it/s, Epoch=15, Loss=1.002817, Mean Loss=0.647742, Mean F-1=0.734108]\n",
      "91it [00:56,  1.61it/s, Epoch=15, Val Loss=0.042530, Mean Val Loss=0.088779, Mean Val F-1=0.931467]\n",
      "271it [03:18,  1.37it/s, Epoch=16, Loss=0.075517, Mean Loss=0.606510, Mean F-1=0.758586]\n",
      "91it [00:58,  1.57it/s, Epoch=16, Val Loss=0.041163, Mean Val Loss=0.086669, Mean Val F-1=0.931122]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [03:52,  1.16it/s, Epoch=17, Loss=0.970153, Mean Loss=0.589358, Mean F-1=0.759975]\n",
      "91it [01:14,  1.23it/s, Epoch=17, Val Loss=0.036311, Mean Val Loss=0.076314, Mean Val F-1=0.937112]\n",
      "271it [04:17,  1.05it/s, Epoch=18, Loss=0.569910, Mean Loss=0.590339, Mean F-1=0.740619]\n",
      "91it [01:20,  1.13it/s, Epoch=18, Val Loss=0.071337, Mean Val Loss=0.086315, Mean Val F-1=0.927350]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [04:44,  1.05s/it, Epoch=19, Loss=0.642491, Mean Loss=0.583797, Mean F-1=0.749022]\n",
      "91it [01:23,  1.09it/s, Epoch=19, Val Loss=0.042140, Mean Val Loss=0.072082, Mean Val F-1=0.936339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [04:55,  1.09s/it, Epoch=20, Loss=1.005052, Mean Loss=0.573492, Mean F-1=0.771150]\n",
      "91it [01:26,  1.05it/s, Epoch=20, Val Loss=0.071991, Mean Val Loss=0.073233, Mean Val F-1=0.946767]\n",
      "271it [04:58,  1.10s/it, Epoch=21, Loss=0.590973, Mean Loss=0.548670, Mean F-1=0.779605]\n",
      "91it [01:28,  1.03it/s, Epoch=21, Val Loss=0.098516, Mean Val Loss=0.080959, Mean Val F-1=0.951152]\n",
      "271it [05:13,  1.16s/it, Epoch=22, Loss=0.455247, Mean Loss=0.565924, Mean F-1=0.781208]\n",
      "91it [01:29,  1.01it/s, Epoch=22, Val Loss=0.070609, Mean Val Loss=0.071776, Mean Val F-1=0.949321]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [05:26,  1.20s/it, Epoch=23, Loss=1.112584, Mean Loss=0.549028, Mean F-1=0.748528]\n",
      "91it [01:37,  1.08s/it, Epoch=23, Val Loss=0.050117, Mean Val Loss=0.070825, Mean Val F-1=0.946559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [05:15,  1.16s/it, Epoch=24, Loss=0.556913, Mean Loss=0.567319, Mean F-1=0.782828]\n",
      "91it [01:21,  1.12it/s, Epoch=24, Val Loss=0.010654, Mean Val Loss=0.088901, Mean Val F-1=0.929145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [3 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [04:28,  1.01it/s, Epoch=25, Loss=0.549209, Mean Loss=0.557251, Mean F-1=0.794989]\n",
      "91it [01:22,  1.11it/s, Epoch=25, Val Loss=0.003387, Mean Val Loss=0.078799, Mean Val F-1=0.937980]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [4 / 4]\n",
      "== Early Stop ==\n",
      "\n",
      "\n",
      "\n",
      "===== k_fold : 2 / 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [02:45,  1.64it/s, Epoch=1, Loss=1.193419, Mean Loss=1.394189, Mean F-1=0.484463]\n",
      "91it [00:45,  2.00it/s, Epoch=1, Val Loss=0.462191, Mean Val Loss=0.653252, Mean Val F-1=0.640316]\n",
      "271it [02:44,  1.65it/s, Epoch=2, Loss=1.163081, Mean Loss=1.112839, Mean F-1=0.566000]\n",
      "91it [00:44,  2.03it/s, Epoch=2, Val Loss=0.172038, Mean Val Loss=0.306795, Mean Val F-1=0.789913]\n",
      "271it [02:43,  1.66it/s, Epoch=3, Loss=1.873655, Mean Loss=1.060597, Mean F-1=0.600854]\n",
      "91it [00:44,  2.02it/s, Epoch=3, Val Loss=0.089939, Mean Val Loss=0.267797, Mean Val F-1=0.777639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [02:44,  1.65it/s, Epoch=4, Loss=1.010631, Mean Loss=0.950442, Mean F-1=0.626070]\n",
      "91it [00:45,  1.99it/s, Epoch=4, Val Loss=0.095127, Mean Val Loss=0.247601, Mean Val F-1=0.798906]\n",
      "271it [02:45,  1.63it/s, Epoch=5, Loss=0.945606, Mean Loss=0.923674, Mean F-1=0.654870]\n",
      "91it [00:46,  1.98it/s, Epoch=5, Val Loss=0.049833, Mean Val Loss=0.204349, Mean Val F-1=0.830722]\n",
      "271it [02:49,  1.60it/s, Epoch=6, Loss=0.962103, Mean Loss=0.907337, Mean F-1=0.633414]\n",
      "91it [00:46,  1.94it/s, Epoch=6, Val Loss=0.065727, Mean Val Loss=0.204922, Mean Val F-1=0.832480]\n",
      "271it [02:52,  1.57it/s, Epoch=7, Loss=1.045993, Mean Loss=0.849125, Mean F-1=0.666920]\n",
      "91it [00:48,  1.87it/s, Epoch=7, Val Loss=0.042823, Mean Val Loss=0.154672, Mean Val F-1=0.871404]\n",
      "271it [02:55,  1.54it/s, Epoch=8, Loss=1.425965, Mean Loss=0.809496, Mean F-1=0.682869]\n",
      "91it [00:49,  1.84it/s, Epoch=8, Val Loss=0.015412, Mean Val Loss=0.162945, Mean Val F-1=0.838996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [02:59,  1.51it/s, Epoch=9, Loss=0.686982, Mean Loss=0.779941, Mean F-1=0.701166]\n",
      "91it [00:51,  1.76it/s, Epoch=9, Val Loss=0.035412, Mean Val Loss=0.159516, Mean Val F-1=0.853888]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [03:04,  1.47it/s, Epoch=10, Loss=1.664356, Mean Loss=0.778557, Mean F-1=0.684127]\n",
      "91it [00:53,  1.69it/s, Epoch=10, Val Loss=0.013471, Mean Val Loss=0.125958, Mean Val F-1=0.878890]\n",
      "271it [03:10,  1.42it/s, Epoch=11, Loss=0.642303, Mean Loss=0.733784, Mean F-1=0.707908]\n",
      "91it [00:54,  1.66it/s, Epoch=11, Val Loss=0.017255, Mean Val Loss=0.149363, Mean Val F-1=0.889972]\n",
      "271it [03:15,  1.39it/s, Epoch=12, Loss=0.466152, Mean Loss=0.714747, Mean F-1=0.711950]\n",
      "91it [00:56,  1.62it/s, Epoch=12, Val Loss=0.003814, Mean Val Loss=0.129261, Mean Val F-1=0.888317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [03:16,  1.38it/s, Epoch=13, Loss=0.466214, Mean Loss=0.691281, Mean F-1=0.703553]\n",
      "91it [00:57,  1.58it/s, Epoch=13, Val Loss=0.016440, Mean Val Loss=0.125504, Mean Val F-1=0.879063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [03:23,  1.33it/s, Epoch=14, Loss=0.677230, Mean Loss=0.655900, Mean F-1=0.724268]\n",
      "91it [00:59,  1.54it/s, Epoch=14, Val Loss=0.035181, Mean Val Loss=0.111458, Mean Val F-1=0.900732]\n",
      "271it [03:30,  1.29it/s, Epoch=15, Loss=1.159975, Mean Loss=0.621150, Mean F-1=0.761980]\n",
      "91it [01:01,  1.48it/s, Epoch=15, Val Loss=0.007324, Mean Val Loss=0.095968, Mean Val F-1=0.902558]\n",
      "271it [03:36,  1.25it/s, Epoch=16, Loss=0.374476, Mean Loss=0.618846, Mean F-1=0.739538]\n",
      "91it [01:04,  1.42it/s, Epoch=16, Val Loss=0.006354, Mean Val Loss=0.083222, Mean Val F-1=0.917558]\n",
      "271it [03:43,  1.21it/s, Epoch=17, Loss=0.411399, Mean Loss=0.595368, Mean F-1=0.773878]\n",
      "91it [01:06,  1.37it/s, Epoch=17, Val Loss=0.004393, Mean Val Loss=0.085945, Mean Val F-1=0.904785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [03:48,  1.18it/s, Epoch=18, Loss=1.212831, Mean Loss=0.579312, Mean F-1=0.772782]\n",
      "91it [01:06,  1.36it/s, Epoch=18, Val Loss=0.005641, Mean Val Loss=0.080330, Mean Val F-1=0.919319]\n",
      "271it [03:55,  1.15it/s, Epoch=19, Loss=1.009541, Mean Loss=0.563296, Mean F-1=0.778591]\n",
      "91it [01:09,  1.31it/s, Epoch=19, Val Loss=0.008138, Mean Val Loss=0.077212, Mean Val F-1=0.921865]\n",
      "271it [04:02,  1.12it/s, Epoch=20, Loss=1.627387, Mean Loss=0.549545, Mean F-1=0.788173]\n",
      "91it [01:12,  1.25it/s, Epoch=20, Val Loss=0.004697, Mean Val Loss=0.082640, Mean Val F-1=0.913412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [04:08,  1.09it/s, Epoch=21, Loss=0.423065, Mean Loss=0.567174, Mean F-1=0.792149]\n",
      "91it [01:12,  1.25it/s, Epoch=21, Val Loss=0.003087, Mean Val Loss=0.084076, Mean Val F-1=0.909292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [04:14,  1.07it/s, Epoch=22, Loss=0.504836, Mean Loss=0.569013, Mean F-1=0.772809]\n",
      "91it [01:15,  1.20it/s, Epoch=22, Val Loss=0.006326, Mean Val Loss=0.089780, Mean Val F-1=0.912633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [3 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [04:21,  1.04it/s, Epoch=23, Loss=0.291150, Mean Loss=0.550967, Mean F-1=0.787183]\n",
      "91it [01:18,  1.16it/s, Epoch=23, Val Loss=0.003844, Mean Val Loss=0.092491, Mean Val F-1=0.916185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [4 / 4]\n",
      "== Early Stop ==\n",
      "\n",
      "\n",
      "\n",
      "===== k_fold : 3 / 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [02:43,  1.66it/s, Epoch=1, Loss=1.515357, Mean Loss=1.430301, Mean F-1=0.478090]\n",
      "91it [00:44,  2.04it/s, Epoch=1, Val Loss=0.001991, Mean Val Loss=0.396378, Mean Val F-1=0.727099]\n",
      "271it [02:41,  1.67it/s, Epoch=2, Loss=0.856359, Mean Loss=1.134151, Mean F-1=0.561843]\n",
      "91it [00:44,  2.06it/s, Epoch=2, Val Loss=0.009191, Mean Val Loss=0.303536, Mean Val F-1=0.782070]\n",
      "271it [02:41,  1.68it/s, Epoch=3, Loss=1.420507, Mean Loss=1.019497, Mean F-1=0.601129]\n",
      "91it [00:44,  2.04it/s, Epoch=3, Val Loss=0.020753, Mean Val Loss=0.306989, Mean Val F-1=0.779297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [02:42,  1.67it/s, Epoch=4, Loss=1.467582, Mean Loss=0.995311, Mean F-1=0.615914]\n",
      "91it [00:44,  2.03it/s, Epoch=4, Val Loss=0.006346, Mean Val Loss=0.221783, Mean Val F-1=0.813670]\n",
      "271it [02:44,  1.65it/s, Epoch=5, Loss=1.310707, Mean Loss=0.937442, Mean F-1=0.627705]\n",
      "91it [00:45,  2.00it/s, Epoch=5, Val Loss=0.012911, Mean Val Loss=0.189046, Mean Val F-1=0.850582]\n",
      "271it [02:46,  1.63it/s, Epoch=6, Loss=2.373494, Mean Loss=0.877903, Mean F-1=0.668942]\n",
      "91it [00:46,  1.94it/s, Epoch=6, Val Loss=0.027511, Mean Val Loss=0.192255, Mean Val F-1=0.843078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [02:49,  1.60it/s, Epoch=7, Loss=0.910030, Mean Loss=0.856352, Mean F-1=0.649232]\n",
      "91it [00:47,  1.91it/s, Epoch=7, Val Loss=0.002686, Mean Val Loss=0.167786, Mean Val F-1=0.851629]\n",
      "271it [02:53,  1.56it/s, Epoch=8, Loss=0.551767, Mean Loss=0.813206, Mean F-1=0.662006]\n",
      "91it [00:48,  1.86it/s, Epoch=8, Val Loss=0.004853, Mean Val Loss=0.163143, Mean Val F-1=0.857527]\n",
      "271it [02:58,  1.52it/s, Epoch=9, Loss=0.990842, Mean Loss=0.782713, Mean F-1=0.698511]\n",
      "91it [00:51,  1.77it/s, Epoch=9, Val Loss=0.008465, Mean Val Loss=0.141998, Mean Val F-1=0.882371]\n",
      "271it [03:04,  1.47it/s, Epoch=10, Loss=0.677934, Mean Loss=0.761798, Mean F-1=0.721194]\n",
      "91it [00:56,  1.62it/s, Epoch=10, Val Loss=0.002497, Mean Val Loss=0.164443, Mean Val F-1=0.863381]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [03:10,  1.42it/s, Epoch=11, Loss=1.062029, Mean Loss=0.719421, Mean F-1=0.713204]\n",
      "91it [00:54,  1.66it/s, Epoch=11, Val Loss=0.011356, Mean Val Loss=0.151504, Mean Val F-1=0.872841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [03:17,  1.37it/s, Epoch=12, Loss=2.044736, Mean Loss=0.693559, Mean F-1=0.738165]\n",
      "91it [00:57,  1.57it/s, Epoch=12, Val Loss=0.007415, Mean Val Loss=0.147661, Mean Val F-1=0.874325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [3 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [03:22,  1.34it/s, Epoch=13, Loss=1.828229, Mean Loss=0.682916, Mean F-1=0.727040]\n",
      "91it [00:59,  1.54it/s, Epoch=13, Val Loss=0.007277, Mean Val Loss=0.111676, Mean Val F-1=0.904724]\n",
      "271it [03:29,  1.30it/s, Epoch=14, Loss=0.361990, Mean Loss=0.656987, Mean F-1=0.730932]\n",
      "91it [01:01,  1.49it/s, Epoch=14, Val Loss=0.002697, Mean Val Loss=0.123726, Mean Val F-1=0.895566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [03:36,  1.25it/s, Epoch=15, Loss=0.979997, Mean Loss=0.632759, Mean F-1=0.754100]\n",
      "91it [01:04,  1.41it/s, Epoch=15, Val Loss=0.000682, Mean Val Loss=0.117916, Mean Val F-1=0.890969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [03:42,  1.22it/s, Epoch=16, Loss=0.608932, Mean Loss=0.618685, Mean F-1=0.741384]\n",
      "91it [01:05,  1.39it/s, Epoch=16, Val Loss=0.007532, Mean Val Loss=0.118978, Mean Val F-1=0.905384]\n",
      "271it [03:47,  1.19it/s, Epoch=17, Loss=0.277074, Mean Loss=0.577368, Mean F-1=0.760305]\n",
      "91it [01:06,  1.37it/s, Epoch=17, Val Loss=0.004460, Mean Val Loss=0.106364, Mean Val F-1=0.903561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [03:53,  1.16it/s, Epoch=18, Loss=1.008022, Mean Loss=0.588373, Mean F-1=0.746878]\n",
      "91it [01:10,  1.29it/s, Epoch=18, Val Loss=0.001670, Mean Val Loss=0.101180, Mean Val F-1=0.904786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [04:00,  1.13it/s, Epoch=19, Loss=0.969614, Mean Loss=0.578759, Mean F-1=0.732513]\n",
      "91it [01:11,  1.27it/s, Epoch=19, Val Loss=0.000518, Mean Val Loss=0.098520, Mean Val F-1=0.908689]\n",
      "271it [04:06,  1.10it/s, Epoch=20, Loss=1.142852, Mean Loss=0.561126, Mean F-1=0.788164]\n",
      "91it [01:12,  1.26it/s, Epoch=20, Val Loss=0.001011, Mean Val Loss=0.099447, Mean Val F-1=0.907737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [04:12,  1.07it/s, Epoch=21, Loss=1.082069, Mean Loss=0.529997, Mean F-1=0.802520]\n",
      "91it [01:14,  1.23it/s, Epoch=21, Val Loss=0.000641, Mean Val Loss=0.098976, Mean Val F-1=0.918320]\n",
      "271it [04:19,  1.04it/s, Epoch=22, Loss=0.649692, Mean Loss=0.551127, Mean F-1=0.769923]\n",
      "91it [01:17,  1.18it/s, Epoch=22, Val Loss=0.001178, Mean Val Loss=0.107910, Mean Val F-1=0.923542]\n",
      "271it [04:24,  1.02it/s, Epoch=23, Loss=0.615253, Mean Loss=0.542815, Mean F-1=0.787502]\n",
      "91it [01:21,  1.11it/s, Epoch=23, Val Loss=0.003876, Mean Val Loss=0.110496, Mean Val F-1=0.912524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [04:31,  1.00s/it, Epoch=24, Loss=0.453651, Mean Loss=0.533434, Mean F-1=0.801265]\n",
      "91it [01:22,  1.11it/s, Epoch=24, Val Loss=0.000914, Mean Val Loss=0.113444, Mean Val F-1=0.903436]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [04:37,  1.02s/it, Epoch=25, Loss=0.311887, Mean Loss=0.557553, Mean F-1=0.773588]\n",
      "91it [01:23,  1.09it/s, Epoch=25, Val Loss=0.001061, Mean Val Loss=0.117495, Mean Val F-1=0.911077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [3 / 4]\n",
      "\n",
      "\n",
      "\n",
      "===== k_fold : 4 / 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [02:45,  1.64it/s, Epoch=1, Loss=1.708018, Mean Loss=1.415026, Mean F-1=0.488214]\n",
      "91it [00:45,  2.01it/s, Epoch=1, Val Loss=0.956454, Mean Val Loss=0.391304, Mean Val F-1=0.703273]\n",
      "271it [02:43,  1.65it/s, Epoch=2, Loss=0.849125, Mean Loss=1.138924, Mean F-1=0.551794]\n",
      "91it [00:45,  2.01it/s, Epoch=2, Val Loss=1.716672, Mean Val Loss=0.288223, Mean Val F-1=0.766511]\n",
      "271it [02:44,  1.65it/s, Epoch=3, Loss=1.231426, Mean Loss=1.056585, Mean F-1=0.581693]\n",
      "91it [00:45,  2.02it/s, Epoch=3, Val Loss=0.079570, Mean Val Loss=0.260532, Mean Val F-1=0.808746]\n",
      "271it [02:44,  1.65it/s, Epoch=4, Loss=0.812325, Mean Loss=1.029253, Mean F-1=0.590379]\n",
      "91it [00:45,  1.98it/s, Epoch=4, Val Loss=1.197335, Mean Val Loss=0.269338, Mean Val F-1=0.798357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [02:44,  1.64it/s, Epoch=5, Loss=1.164494, Mean Loss=0.966837, Mean F-1=0.616465]\n",
      "91it [00:45,  1.99it/s, Epoch=5, Val Loss=1.084004, Mean Val Loss=0.252294, Mean Val F-1=0.843539]\n",
      "271it [02:47,  1.62it/s, Epoch=6, Loss=1.057140, Mean Loss=0.930308, Mean F-1=0.637114]\n",
      "91it [00:46,  1.94it/s, Epoch=6, Val Loss=0.067927, Mean Val Loss=0.312092, Mean Val F-1=0.850603]\n",
      "271it [02:50,  1.59it/s, Epoch=7, Loss=0.743463, Mean Loss=0.835778, Mean F-1=0.673759]\n",
      "91it [00:47,  1.90it/s, Epoch=7, Val Loss=1.370171, Mean Val Loss=0.192732, Mean Val F-1=0.845908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [02:54,  1.55it/s, Epoch=8, Loss=0.279097, Mean Loss=0.790987, Mean F-1=0.699963]\n",
      "91it [00:49,  1.84it/s, Epoch=8, Val Loss=0.179630, Mean Val Loss=0.188544, Mean Val F-1=0.871737]\n",
      "271it [02:59,  1.51it/s, Epoch=9, Loss=1.056091, Mean Loss=0.790191, Mean F-1=0.692904]\n",
      "91it [00:51,  1.78it/s, Epoch=9, Val Loss=0.042407, Mean Val Loss=0.150655, Mean Val F-1=0.880289]\n",
      "271it [03:04,  1.47it/s, Epoch=10, Loss=0.852421, Mean Loss=0.729725, Mean F-1=0.706369]\n",
      "91it [00:53,  1.69it/s, Epoch=10, Val Loss=0.072195, Mean Val Loss=0.131826, Mean Val F-1=0.892181]\n",
      "271it [03:10,  1.42it/s, Epoch=11, Loss=0.849649, Mean Loss=0.732002, Mean F-1=0.688069]\n",
      "91it [00:55,  1.65it/s, Epoch=11, Val Loss=0.176945, Mean Val Loss=0.459027, Mean Val F-1=0.901298]\n",
      "271it [03:17,  1.37it/s, Epoch=12, Loss=1.141965, Mean Loss=0.698102, Mean F-1=0.724850]\n",
      "91it [00:56,  1.60it/s, Epoch=12, Val Loss=0.016501, Mean Val Loss=0.302652, Mean Val F-1=0.865076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [03:24,  1.32it/s, Epoch=13, Loss=0.596957, Mean Loss=0.687160, Mean F-1=0.732038]\n",
      "91it [00:59,  1.52it/s, Epoch=13, Val Loss=0.015078, Mean Val Loss=0.170024, Mean Val F-1=0.889519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [03:32,  1.27it/s, Epoch=14, Loss=0.374410, Mean Loss=0.654846, Mean F-1=0.741451]\n",
      "91it [01:02,  1.45it/s, Epoch=14, Val Loss=0.003870, Mean Val Loss=0.208717, Mean Val F-1=0.888049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [3 / 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "271it [03:39,  1.23it/s, Epoch=15, Loss=0.651731, Mean Loss=0.636249, Mean F-1=0.724978]\n",
      "91it [01:02,  1.45it/s, Epoch=15, Val Loss=0.011037, Mean Val Loss=0.217308, Mean Val F-1=0.899436]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [4 / 4]\n",
      "== Early Stop ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k, (fold_train, fold_val) in enumerate(kfold.split(data_list, label_list), 1) :\n",
    "    model = CNN2RNNModel(model_path, n_classes)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=5e-2)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=0.0001) \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_data_list = []\n",
    "    val_data_list = []    \n",
    "    for k_train in fold_train :\n",
    "        train_data_list.append(data_list[k_train])\n",
    "    \n",
    "    for k_val in fold_val :\n",
    "        val_data_list.append(data_list[k_val])\n",
    "        \n",
    "        \n",
    "    print(f\"\\n\\n\\n===== k_fold : {k} / {fold_n} =====\")\n",
    "    train_dataset = CustomDataset(\n",
    "                train_data_list, \n",
    "                csv_feature_dict,\n",
    "                label_encoder,\n",
    "                transforms=train_transforms,\n",
    "            )\n",
    "    val_dataset = CustomDataset(\n",
    "                val_data_list, \n",
    "                csv_feature_dict,\n",
    "                label_encoder,\n",
    "                transforms=val_transforms,\n",
    "            )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    loss_plot, val_loss_plot = [], []\n",
    "    metric_plot, val_metric_plot = [], []\n",
    "\n",
    "    early_stopping = 0\n",
    "    for epoch in range(epochs):\n",
    "        total_loss, total_val_loss = 0, 0\n",
    "        total_acc, total_val_acc = 0, 0\n",
    "\n",
    "        tqdm_dataset = tqdm(enumerate(train_loader))\n",
    "        training = True\n",
    "        for batch, batch_item in tqdm_dataset:\n",
    "            batch_loss, batch_acc = train_step(batch_item, training)\n",
    "            total_loss += batch_loss\n",
    "            total_acc += batch_acc\n",
    "\n",
    "            tqdm_dataset.set_postfix({\n",
    "                'Epoch': epoch + 1,\n",
    "                'Loss': '{:06f}'.format(batch_loss.item()),\n",
    "                'Mean Loss' : '{:06f}'.format(total_loss/(batch+1)),\n",
    "                'Mean F-1' : '{:06f}'.format(total_acc/(batch+1))\n",
    "            })\n",
    "        loss_plot.append(total_loss/(batch+1))\n",
    "        metric_plot.append(total_acc/(batch+1))\n",
    "\n",
    "        tqdm_dataset = tqdm(enumerate(val_loader))\n",
    "        training = False\n",
    "        for batch, batch_item in tqdm_dataset:\n",
    "            batch_loss, batch_acc = train_step(batch_item, training)\n",
    "            total_val_loss += batch_loss\n",
    "            total_val_acc += batch_acc\n",
    "\n",
    "            tqdm_dataset.set_postfix({\n",
    "                'Epoch': epoch + 1,\n",
    "                'Val Loss': '{:06f}'.format(batch_loss.item()),\n",
    "                'Mean Val Loss' : '{:06f}'.format(total_val_loss/(batch+1)),\n",
    "                'Mean Val F-1' : '{:06f}'.format(total_val_acc/(batch+1))\n",
    "            })\n",
    "        val_loss_plot.append(total_val_loss/(batch+1))\n",
    "        val_metric_plot.append(total_val_acc/(batch+1))\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if np.max(val_metric_plot) == val_metric_plot[-1]:\n",
    "            torch.save(model.state_dict(), f'{k}_{save_path}')\n",
    "            early_stopping = 0\n",
    "\n",
    "        elif np.max(val_metric_plot) > val_metric_plot[-1]: \n",
    "            early_stopping += 1\n",
    "            print(f\"Early Stopping Step : [{early_stopping} / {early_stopping_cnt}]\")\n",
    "\n",
    "        if early_stopping == early_stopping_cnt :\n",
    "            print(\"== Early Stop ==\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c165b",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ff516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_plot, val_loss_plot = [], []\n",
    "# metric_plot, val_metric_plot = [], []\n",
    "\n",
    "# early_stopping = 0\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss, total_val_loss = 0, 0\n",
    "#     total_acc, total_val_acc = 0, 0\n",
    "    \n",
    "#     tqdm_dataset = tqdm(enumerate(train_loader))\n",
    "#     training = True\n",
    "#     for batch, batch_item in tqdm_dataset:\n",
    "#         batch_loss, batch_acc = train_step(batch_item, training)\n",
    "#         total_loss += batch_loss\n",
    "#         total_acc += batch_acc\n",
    "        \n",
    "#         tqdm_dataset.set_postfix({\n",
    "#             'Epoch': epoch + 1,\n",
    "#             'Loss': '{:06f}'.format(batch_loss.item()),\n",
    "#             'Mean Loss' : '{:06f}'.format(total_loss/(batch+1)),\n",
    "#             'Mean F-1' : '{:06f}'.format(total_acc/(batch+1))\n",
    "#         })\n",
    "#     loss_plot.append(total_loss/(batch+1))\n",
    "#     metric_plot.append(total_acc/(batch+1))\n",
    "    \n",
    "#     tqdm_dataset = tqdm(enumerate(val_loader))\n",
    "#     training = False\n",
    "#     for batch, batch_item in tqdm_dataset:\n",
    "#         batch_loss, batch_acc = train_step(batch_item, training)\n",
    "#         total_val_loss += batch_loss\n",
    "#         total_val_acc += batch_acc\n",
    "        \n",
    "#         tqdm_dataset.set_postfix({\n",
    "#             'Epoch': epoch + 1,\n",
    "#             'Val Loss': '{:06f}'.format(batch_loss.item()),\n",
    "#             'Mean Val Loss' : '{:06f}'.format(total_val_loss/(batch+1)),\n",
    "#             'Mean Val F-1' : '{:06f}'.format(total_val_acc/(batch+1))\n",
    "#         })\n",
    "#     val_loss_plot.append(total_val_loss/(batch+1))\n",
    "#     val_metric_plot.append(total_val_acc/(batch+1))\n",
    "    \n",
    "#     if np.max(val_metric_plot) == val_metric_plot[-1]:\n",
    "#         torch.save(model.state_dict(), save_path)\n",
    "        \n",
    "#     elif np.max(val_metric_plot) > val_metric_plot[-1]: \n",
    "#         early_stopping += 1\n",
    "#         print(f\"Early Stopping Step : [{early_stopping} / 4]\")\n",
    "\n",
    "#     if early_stopping == early_stopping_cnt :\n",
    "#         print(\"== Early Stop ==\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "554f9bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _pretrained_EffiV2S(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_pretrained_EffiV2S, self).__init__()\n",
    "        self.model = timm.create_model('efficientnetv2_rw_s', num_classes=38, pretrained=False)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        output = self.model(inputs)\n",
    "        return output\n",
    "    \n",
    "class _EffiV2S(nn.Module) :\n",
    "    def __init__(self, model_path, test=False) :\n",
    "        super(_EffiV2S, self).__init__()\n",
    "        self.pre_model = _pretrained_EffiV2S()\n",
    "        if test == False :\n",
    "            self.pre_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        \n",
    "        self.fc = nn.Linear(38, 25)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        x = self.pre_model(x)\n",
    "        x = self.fc(x)\n",
    "        return x   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de0a870",
   "metadata": {},
   "source": [
    "# Test Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "903d42ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transforms = A.Compose([\n",
    "    A.Resize(288,288)\n",
    "])\n",
    "\n",
    "test = sorted(glob('data/test/*'))\n",
    "test_dataset = CustomDataset(\n",
    "            test, \n",
    "            csv_feature_dict,\n",
    "            label_encoder,\n",
    "            transforms=val_transforms,\n",
    "            mode = 'test'\n",
    "        )\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4586a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def softvoting(models, item, n_classes=25) :\n",
    "    img = item['img'].to(device)\n",
    "    seq_len = item['seq_len'].to(device)\n",
    "    csv_feature = item['csv_feature'].to(device)\n",
    "    \n",
    "    predicts = torch.zeros(img.size(0), n_classes)\n",
    "    with torch.no_grad() :\n",
    "        for idx, model in enumerate(models) :\n",
    "            if idx < 4 :\n",
    "                output = model(img, csv_feature, seq_len)\n",
    "            elif idx > 3 :\n",
    "                output = model(img)\n",
    "            output = F.softmax(output.cpu(), dim=1)\n",
    "            predicts += output\n",
    "\n",
    "    # 둘다 값은 똑같이 나옴.\n",
    "    # pred_avg = predicts / len(models)\n",
    "    # answer = pred_avg.argmax(dim=-1)\n",
    "    # _, answer2 = torch.max(pred_avg, 1)\n",
    "\n",
    "    return predicts.detach().cpu() / len(models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86713857",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3245it [49:09,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "def predict(dataset, models) :\n",
    "    tqdm_dataset = tqdm(enumerate(dataset))\n",
    "    results = []\n",
    "    for batch, batch_item in tqdm_dataset :\n",
    "        item = batch_item\n",
    "#         print(img.shape)\n",
    "        predictions = softvoting(models, item)\n",
    "        batch_result = [int(torch.argmax(prediction)) for prediction in predictions]\n",
    "#         print(batch_result)\n",
    "#         for prediction in predictions :\n",
    "            \n",
    "#         results.append(int(torch.argmax(predictions[0])))\n",
    "#             output = model(img)\n",
    "#         output = torch.tensor(torch.argmax(output, dim=1), dtype=torch.int32).cpu().numpy()\n",
    "        results.extend(batch_result)\n",
    "    return results\n",
    "\n",
    "kfold_models_path = glob('./model/k_public_vill_50k_pretrain_Effi_1000_CNN2RNN/*.pt') + glob('./model/k_fold_50k_pretrained_effiv2S/*.pt')\n",
    "models = []\n",
    "for idx, kfold_model_path in enumerate(kfold_models_path) :\n",
    "    if idx < 4 :\n",
    "        model = CNN2RNNModel(model_path, n_classes)    \n",
    "    elif idx > 3 :\n",
    "        model = _EffiV2S('',test=True)\n",
    "\n",
    "    model.load_state_dict(torch.load(kfold_model_path, map_location=device))\n",
    "    model.to(device).eval()\n",
    "    models.append(model)\n",
    "\n",
    "    \n",
    "preds = predict(test_dataloader, models)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e12254fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cp = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41162d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cp = np.array([train_label_decoder[int(val)] for val in preds_cp])\n",
    "submission_csv = pd.read_csv('./data/sample_submission.csv')\n",
    "submission_csv['label'] = preds_cp\n",
    "submission_csv.to_csv('./data/k_public_vill_50k_pretrain_KFOLD_Effi_Effi_1000_CNN2RNN.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a044ce46",
   "metadata": {},
   "source": [
    "# hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4a7df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hardvoting(models, item) :\n",
    "    img = item['img'].to(device)\n",
    "    seq_len = item['seq_len'].to(device)\n",
    "    csv_feature = item['csv_feature'].to(device)\n",
    "    \n",
    "    predicts = torch.zeros(img.size(0), n_classes)\n",
    "    results = {}\n",
    "    tmp_vals = [0] * 16\n",
    "    tmp_inds = [0] * 16\n",
    "    with torch.no_grad() :\n",
    "        for idx, model in enumerate(models) :\n",
    "            if idx < 4 :\n",
    "                outputs = model(img, csv_feature, seq_len)\n",
    "            elif idx > 3 :\n",
    "                outputs = model(img)\n",
    "                \n",
    "#             outputs = model(img, csv_feature, seq_len)\n",
    "            outputs = F.softmax(outputs.cpu(), dim=1)\n",
    "\n",
    "            vals, indices = torch.max(outputs, 1)\n",
    "            \n",
    "            for i, (val, idx) in enumerate(zip(vals, indices)) :\n",
    "                if tmp_vals[i] < val :\n",
    "                    tmp_vals[i] = val.item()\n",
    "                    tmp_inds[i] = idx.item()\n",
    "                    \n",
    "                \n",
    "#             print(tmp_vals)\n",
    "#             print(tmp_inds)\n",
    "\n",
    "\n",
    "            predicts += outputs\n",
    "\n",
    "    # 둘다 값은 똑같이 나옴.\n",
    "    # pred_avg = predicts / len(models)\n",
    "    # answer = pred_avg.argmax(dim=-1)\n",
    "    # _, answer2 = torch.max(pred_avg, 1)\n",
    "\n",
    "    return predicts.detach().cpu() / len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a713b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3245it [50:23,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "def predict(dataset, models) :\n",
    "    tqdm_dataset = tqdm(enumerate(dataset))\n",
    "    results = []\n",
    "    for batch, batch_item in tqdm_dataset :\n",
    "        item = batch_item\n",
    "#         print(img.shape)\n",
    "        predictions = hardvoting(models, item)\n",
    "        batch_result = [int(torch.argmax(prediction)) for prediction in predictions]\n",
    "#         print(batch_result)\n",
    "#         for prediction in predictions :\n",
    "            \n",
    "#         results.append(int(torch.argmax(predictions[0])))\n",
    "#             output = model(img)\n",
    "#         output = torch.tensor(torch.argmax(output, dim=1), dtype=torch.int32).cpu().numpy()\n",
    "        results.extend(batch_result)\n",
    "    return results\n",
    "\n",
    "# kfold_models_path = glob('./model/k_public_vill_50k_pretrain_Effi_1000_CNN2RNN/*.pt')\n",
    "kfold_models_path = glob('./model/k_public_vill_50k_pretrain_Effi_1000_CNN2RNN/*.pt') + glob('./model/k_fold_50k_pretrained_effiv2S/*.pt')\n",
    "models = []\n",
    "for idx, kfold_model_path in enumerate(kfold_models_path) :\n",
    "    if idx < 4 :\n",
    "        model = CNN2RNNModel(model_path, n_classes)    \n",
    "    elif idx > 3 :\n",
    "        model = _EffiV2S('',test=True)\n",
    "#     model = CNN2RNNModel(model_path, n_classes) \n",
    "\n",
    "    model.load_state_dict(torch.load(kfold_model_path, map_location=device))\n",
    "    model.to(device).eval()\n",
    "    models.append(model)\n",
    "\n",
    "    \n",
    "preds = predict(test_dataloader, models)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0992e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cp = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83a97fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cp = np.array([train_label_decoder[int(val)] for val in preds_cp])\n",
    "submission_csv = pd.read_csv('./data/sample_submission.csv')\n",
    "submission_csv['label'] = preds_cp\n",
    "submission_csv.to_csv('./data/k_public_vill_50k_pretrain_KFOLD_Effi_Effi_HardVoting_1000_CNN2RNN.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d992c1fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
