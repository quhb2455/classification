{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e88109d",
   "metadata": {},
   "source": [
    "## Input layer 바꾸기 -> patch embedding to linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7afe9fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: easydict in c:\\users\\quhb2\\anaconda3\\envs\\torch-1.9\\lib\\site-packages (1.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\quhb2\\anaconda3\\envs\\torch-1.9\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2185f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af9b1132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm \n",
    "\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6001e550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "          Identity-2             [-1, 196, 768]               0\n",
      "        PatchEmbed-3             [-1, 196, 768]               0\n",
      "           Dropout-4             [-1, 197, 768]               0\n",
      "         LayerNorm-5             [-1, 197, 768]           1,536\n",
      "            Linear-6            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-7         [-1, 12, 197, 197]               0\n",
      "            Linear-8             [-1, 197, 768]         590,592\n",
      "           Dropout-9             [-1, 197, 768]               0\n",
      "        Attention-10             [-1, 197, 768]               0\n",
      "         Identity-11             [-1, 197, 768]               0\n",
      "        LayerNorm-12             [-1, 197, 768]           1,536\n",
      "           Linear-13            [-1, 197, 3072]       2,362,368\n",
      "             GELU-14            [-1, 197, 3072]               0\n",
      "          Dropout-15            [-1, 197, 3072]               0\n",
      "           Linear-16             [-1, 197, 768]       2,360,064\n",
      "          Dropout-17             [-1, 197, 768]               0\n",
      "              Mlp-18             [-1, 197, 768]               0\n",
      "         Identity-19             [-1, 197, 768]               0\n",
      "            Block-20             [-1, 197, 768]               0\n",
      "        LayerNorm-21             [-1, 197, 768]           1,536\n",
      "           Linear-22            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-23         [-1, 12, 197, 197]               0\n",
      "           Linear-24             [-1, 197, 768]         590,592\n",
      "          Dropout-25             [-1, 197, 768]               0\n",
      "        Attention-26             [-1, 197, 768]               0\n",
      "         Identity-27             [-1, 197, 768]               0\n",
      "        LayerNorm-28             [-1, 197, 768]           1,536\n",
      "           Linear-29            [-1, 197, 3072]       2,362,368\n",
      "             GELU-30            [-1, 197, 3072]               0\n",
      "          Dropout-31            [-1, 197, 3072]               0\n",
      "           Linear-32             [-1, 197, 768]       2,360,064\n",
      "          Dropout-33             [-1, 197, 768]               0\n",
      "              Mlp-34             [-1, 197, 768]               0\n",
      "         Identity-35             [-1, 197, 768]               0\n",
      "            Block-36             [-1, 197, 768]               0\n",
      "        LayerNorm-37             [-1, 197, 768]           1,536\n",
      "           Linear-38            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-39         [-1, 12, 197, 197]               0\n",
      "           Linear-40             [-1, 197, 768]         590,592\n",
      "          Dropout-41             [-1, 197, 768]               0\n",
      "        Attention-42             [-1, 197, 768]               0\n",
      "         Identity-43             [-1, 197, 768]               0\n",
      "        LayerNorm-44             [-1, 197, 768]           1,536\n",
      "           Linear-45            [-1, 197, 3072]       2,362,368\n",
      "             GELU-46            [-1, 197, 3072]               0\n",
      "          Dropout-47            [-1, 197, 3072]               0\n",
      "           Linear-48             [-1, 197, 768]       2,360,064\n",
      "          Dropout-49             [-1, 197, 768]               0\n",
      "              Mlp-50             [-1, 197, 768]               0\n",
      "         Identity-51             [-1, 197, 768]               0\n",
      "            Block-52             [-1, 197, 768]               0\n",
      "        LayerNorm-53             [-1, 197, 768]           1,536\n",
      "           Linear-54            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-55         [-1, 12, 197, 197]               0\n",
      "           Linear-56             [-1, 197, 768]         590,592\n",
      "          Dropout-57             [-1, 197, 768]               0\n",
      "        Attention-58             [-1, 197, 768]               0\n",
      "         Identity-59             [-1, 197, 768]               0\n",
      "        LayerNorm-60             [-1, 197, 768]           1,536\n",
      "           Linear-61            [-1, 197, 3072]       2,362,368\n",
      "             GELU-62            [-1, 197, 3072]               0\n",
      "          Dropout-63            [-1, 197, 3072]               0\n",
      "           Linear-64             [-1, 197, 768]       2,360,064\n",
      "          Dropout-65             [-1, 197, 768]               0\n",
      "              Mlp-66             [-1, 197, 768]               0\n",
      "         Identity-67             [-1, 197, 768]               0\n",
      "            Block-68             [-1, 197, 768]               0\n",
      "        LayerNorm-69             [-1, 197, 768]           1,536\n",
      "           Linear-70            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-71         [-1, 12, 197, 197]               0\n",
      "           Linear-72             [-1, 197, 768]         590,592\n",
      "          Dropout-73             [-1, 197, 768]               0\n",
      "        Attention-74             [-1, 197, 768]               0\n",
      "         Identity-75             [-1, 197, 768]               0\n",
      "        LayerNorm-76             [-1, 197, 768]           1,536\n",
      "           Linear-77            [-1, 197, 3072]       2,362,368\n",
      "             GELU-78            [-1, 197, 3072]               0\n",
      "          Dropout-79            [-1, 197, 3072]               0\n",
      "           Linear-80             [-1, 197, 768]       2,360,064\n",
      "          Dropout-81             [-1, 197, 768]               0\n",
      "              Mlp-82             [-1, 197, 768]               0\n",
      "         Identity-83             [-1, 197, 768]               0\n",
      "            Block-84             [-1, 197, 768]               0\n",
      "        LayerNorm-85             [-1, 197, 768]           1,536\n",
      "           Linear-86            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-87         [-1, 12, 197, 197]               0\n",
      "           Linear-88             [-1, 197, 768]         590,592\n",
      "          Dropout-89             [-1, 197, 768]               0\n",
      "        Attention-90             [-1, 197, 768]               0\n",
      "         Identity-91             [-1, 197, 768]               0\n",
      "        LayerNorm-92             [-1, 197, 768]           1,536\n",
      "           Linear-93            [-1, 197, 3072]       2,362,368\n",
      "             GELU-94            [-1, 197, 3072]               0\n",
      "          Dropout-95            [-1, 197, 3072]               0\n",
      "           Linear-96             [-1, 197, 768]       2,360,064\n",
      "          Dropout-97             [-1, 197, 768]               0\n",
      "              Mlp-98             [-1, 197, 768]               0\n",
      "         Identity-99             [-1, 197, 768]               0\n",
      "           Block-100             [-1, 197, 768]               0\n",
      "       LayerNorm-101             [-1, 197, 768]           1,536\n",
      "          Linear-102            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-103         [-1, 12, 197, 197]               0\n",
      "          Linear-104             [-1, 197, 768]         590,592\n",
      "         Dropout-105             [-1, 197, 768]               0\n",
      "       Attention-106             [-1, 197, 768]               0\n",
      "        Identity-107             [-1, 197, 768]               0\n",
      "       LayerNorm-108             [-1, 197, 768]           1,536\n",
      "          Linear-109            [-1, 197, 3072]       2,362,368\n",
      "            GELU-110            [-1, 197, 3072]               0\n",
      "         Dropout-111            [-1, 197, 3072]               0\n",
      "          Linear-112             [-1, 197, 768]       2,360,064\n",
      "         Dropout-113             [-1, 197, 768]               0\n",
      "             Mlp-114             [-1, 197, 768]               0\n",
      "        Identity-115             [-1, 197, 768]               0\n",
      "           Block-116             [-1, 197, 768]               0\n",
      "       LayerNorm-117             [-1, 197, 768]           1,536\n",
      "          Linear-118            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-119         [-1, 12, 197, 197]               0\n",
      "          Linear-120             [-1, 197, 768]         590,592\n",
      "         Dropout-121             [-1, 197, 768]               0\n",
      "       Attention-122             [-1, 197, 768]               0\n",
      "        Identity-123             [-1, 197, 768]               0\n",
      "       LayerNorm-124             [-1, 197, 768]           1,536\n",
      "          Linear-125            [-1, 197, 3072]       2,362,368\n",
      "            GELU-126            [-1, 197, 3072]               0\n",
      "         Dropout-127            [-1, 197, 3072]               0\n",
      "          Linear-128             [-1, 197, 768]       2,360,064\n",
      "         Dropout-129             [-1, 197, 768]               0\n",
      "             Mlp-130             [-1, 197, 768]               0\n",
      "        Identity-131             [-1, 197, 768]               0\n",
      "           Block-132             [-1, 197, 768]               0\n",
      "       LayerNorm-133             [-1, 197, 768]           1,536\n",
      "          Linear-134            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-135         [-1, 12, 197, 197]               0\n",
      "          Linear-136             [-1, 197, 768]         590,592\n",
      "         Dropout-137             [-1, 197, 768]               0\n",
      "       Attention-138             [-1, 197, 768]               0\n",
      "        Identity-139             [-1, 197, 768]               0\n",
      "       LayerNorm-140             [-1, 197, 768]           1,536\n",
      "          Linear-141            [-1, 197, 3072]       2,362,368\n",
      "            GELU-142            [-1, 197, 3072]               0\n",
      "         Dropout-143            [-1, 197, 3072]               0\n",
      "          Linear-144             [-1, 197, 768]       2,360,064\n",
      "         Dropout-145             [-1, 197, 768]               0\n",
      "             Mlp-146             [-1, 197, 768]               0\n",
      "        Identity-147             [-1, 197, 768]               0\n",
      "           Block-148             [-1, 197, 768]               0\n",
      "       LayerNorm-149             [-1, 197, 768]           1,536\n",
      "          Linear-150            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-151         [-1, 12, 197, 197]               0\n",
      "          Linear-152             [-1, 197, 768]         590,592\n",
      "         Dropout-153             [-1, 197, 768]               0\n",
      "       Attention-154             [-1, 197, 768]               0\n",
      "        Identity-155             [-1, 197, 768]               0\n",
      "       LayerNorm-156             [-1, 197, 768]           1,536\n",
      "          Linear-157            [-1, 197, 3072]       2,362,368\n",
      "            GELU-158            [-1, 197, 3072]               0\n",
      "         Dropout-159            [-1, 197, 3072]               0\n",
      "          Linear-160             [-1, 197, 768]       2,360,064\n",
      "         Dropout-161             [-1, 197, 768]               0\n",
      "             Mlp-162             [-1, 197, 768]               0\n",
      "        Identity-163             [-1, 197, 768]               0\n",
      "           Block-164             [-1, 197, 768]               0\n",
      "       LayerNorm-165             [-1, 197, 768]           1,536\n",
      "          Linear-166            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-167         [-1, 12, 197, 197]               0\n",
      "          Linear-168             [-1, 197, 768]         590,592\n",
      "         Dropout-169             [-1, 197, 768]               0\n",
      "       Attention-170             [-1, 197, 768]               0\n",
      "        Identity-171             [-1, 197, 768]               0\n",
      "       LayerNorm-172             [-1, 197, 768]           1,536\n",
      "          Linear-173            [-1, 197, 3072]       2,362,368\n",
      "            GELU-174            [-1, 197, 3072]               0\n",
      "         Dropout-175            [-1, 197, 3072]               0\n",
      "          Linear-176             [-1, 197, 768]       2,360,064\n",
      "         Dropout-177             [-1, 197, 768]               0\n",
      "             Mlp-178             [-1, 197, 768]               0\n",
      "        Identity-179             [-1, 197, 768]               0\n",
      "           Block-180             [-1, 197, 768]               0\n",
      "       LayerNorm-181             [-1, 197, 768]           1,536\n",
      "          Linear-182            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-183         [-1, 12, 197, 197]               0\n",
      "          Linear-184             [-1, 197, 768]         590,592\n",
      "         Dropout-185             [-1, 197, 768]               0\n",
      "       Attention-186             [-1, 197, 768]               0\n",
      "        Identity-187             [-1, 197, 768]               0\n",
      "       LayerNorm-188             [-1, 197, 768]           1,536\n",
      "          Linear-189            [-1, 197, 3072]       2,362,368\n",
      "            GELU-190            [-1, 197, 3072]               0\n",
      "         Dropout-191            [-1, 197, 3072]               0\n",
      "          Linear-192             [-1, 197, 768]       2,360,064\n",
      "         Dropout-193             [-1, 197, 768]               0\n",
      "             Mlp-194             [-1, 197, 768]               0\n",
      "        Identity-195             [-1, 197, 768]               0\n",
      "           Block-196             [-1, 197, 768]               0\n",
      "       LayerNorm-197             [-1, 197, 768]           1,536\n",
      "        Identity-198                  [-1, 768]               0\n",
      "          Linear-199                 [-1, 1000]         769,000\n",
      "================================================================\n",
      "Total params: 86,415,592\n",
      "Trainable params: 86,415,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 408.54\n",
      "Params size (MB): 329.65\n",
      "Estimated Total Size (MB): 738.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn([3,224,224])\n",
    "# data.shape\n",
    "summary(model, (3,224,224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d34b9b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (pre_logits): Identity()\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "513b8e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (drop_path): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (1): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (drop_path): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (2): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (drop_path): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (3): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (drop_path): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (4): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (drop_path): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (5): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (drop_path): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (6): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (drop_path): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (7): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (drop_path): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (8): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (drop_path): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (9): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (drop_path): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (10): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (drop_path): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (11): Block(\n",
       "    (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (attn): Attention(\n",
       "      (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (drop_path): Identity()\n",
       "    (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "    (mlp): Mlp(\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (act): GELU()\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f408bcb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'blocks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-9dce9501fe6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m768\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\quhb2\\anaconda3\\envs\\torch-1.9\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1129\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m-> 1131\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m   1132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'blocks'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(nn.Linear(768, 1024), model.blocks)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9c2fc48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1             [-1, 768, 768]         197,376\n",
      "         LayerNorm-2             [-1, 768, 768]           1,536\n",
      "            Linear-3            [-1, 768, 2304]       1,771,776\n",
      "           Dropout-4         [-1, 12, 768, 768]               0\n",
      "            Linear-5             [-1, 768, 768]         590,592\n",
      "           Dropout-6             [-1, 768, 768]               0\n",
      "         Attention-7             [-1, 768, 768]               0\n",
      "          Identity-8             [-1, 768, 768]               0\n",
      "         LayerNorm-9             [-1, 768, 768]           1,536\n",
      "           Linear-10            [-1, 768, 3072]       2,362,368\n",
      "             GELU-11            [-1, 768, 3072]               0\n",
      "          Dropout-12            [-1, 768, 3072]               0\n",
      "           Linear-13             [-1, 768, 768]       2,360,064\n",
      "          Dropout-14             [-1, 768, 768]               0\n",
      "              Mlp-15             [-1, 768, 768]               0\n",
      "         Identity-16             [-1, 768, 768]               0\n",
      "            Block-17             [-1, 768, 768]               0\n",
      "        LayerNorm-18             [-1, 768, 768]           1,536\n",
      "           Linear-19            [-1, 768, 2304]       1,771,776\n",
      "          Dropout-20         [-1, 12, 768, 768]               0\n",
      "           Linear-21             [-1, 768, 768]         590,592\n",
      "          Dropout-22             [-1, 768, 768]               0\n",
      "        Attention-23             [-1, 768, 768]               0\n",
      "         Identity-24             [-1, 768, 768]               0\n",
      "        LayerNorm-25             [-1, 768, 768]           1,536\n",
      "           Linear-26            [-1, 768, 3072]       2,362,368\n",
      "             GELU-27            [-1, 768, 3072]               0\n",
      "          Dropout-28            [-1, 768, 3072]               0\n",
      "           Linear-29             [-1, 768, 768]       2,360,064\n",
      "          Dropout-30             [-1, 768, 768]               0\n",
      "              Mlp-31             [-1, 768, 768]               0\n",
      "         Identity-32             [-1, 768, 768]               0\n",
      "            Block-33             [-1, 768, 768]               0\n",
      "        LayerNorm-34             [-1, 768, 768]           1,536\n",
      "           Linear-35            [-1, 768, 2304]       1,771,776\n",
      "          Dropout-36         [-1, 12, 768, 768]               0\n",
      "           Linear-37             [-1, 768, 768]         590,592\n",
      "          Dropout-38             [-1, 768, 768]               0\n",
      "        Attention-39             [-1, 768, 768]               0\n",
      "         Identity-40             [-1, 768, 768]               0\n",
      "        LayerNorm-41             [-1, 768, 768]           1,536\n",
      "           Linear-42            [-1, 768, 3072]       2,362,368\n",
      "             GELU-43            [-1, 768, 3072]               0\n",
      "          Dropout-44            [-1, 768, 3072]               0\n",
      "           Linear-45             [-1, 768, 768]       2,360,064\n",
      "          Dropout-46             [-1, 768, 768]               0\n",
      "              Mlp-47             [-1, 768, 768]               0\n",
      "         Identity-48             [-1, 768, 768]               0\n",
      "            Block-49             [-1, 768, 768]               0\n",
      "        LayerNorm-50             [-1, 768, 768]           1,536\n",
      "           Linear-51            [-1, 768, 2304]       1,771,776\n",
      "          Dropout-52         [-1, 12, 768, 768]               0\n",
      "           Linear-53             [-1, 768, 768]         590,592\n",
      "          Dropout-54             [-1, 768, 768]               0\n",
      "        Attention-55             [-1, 768, 768]               0\n",
      "         Identity-56             [-1, 768, 768]               0\n",
      "        LayerNorm-57             [-1, 768, 768]           1,536\n",
      "           Linear-58            [-1, 768, 3072]       2,362,368\n",
      "             GELU-59            [-1, 768, 3072]               0\n",
      "          Dropout-60            [-1, 768, 3072]               0\n",
      "           Linear-61             [-1, 768, 768]       2,360,064\n",
      "          Dropout-62             [-1, 768, 768]               0\n",
      "              Mlp-63             [-1, 768, 768]               0\n",
      "         Identity-64             [-1, 768, 768]               0\n",
      "            Block-65             [-1, 768, 768]               0\n",
      "        LayerNorm-66             [-1, 768, 768]           1,536\n",
      "           Linear-67            [-1, 768, 2304]       1,771,776\n",
      "          Dropout-68         [-1, 12, 768, 768]               0\n",
      "           Linear-69             [-1, 768, 768]         590,592\n",
      "          Dropout-70             [-1, 768, 768]               0\n",
      "        Attention-71             [-1, 768, 768]               0\n",
      "         Identity-72             [-1, 768, 768]               0\n",
      "        LayerNorm-73             [-1, 768, 768]           1,536\n",
      "           Linear-74            [-1, 768, 3072]       2,362,368\n",
      "             GELU-75            [-1, 768, 3072]               0\n",
      "          Dropout-76            [-1, 768, 3072]               0\n",
      "           Linear-77             [-1, 768, 768]       2,360,064\n",
      "          Dropout-78             [-1, 768, 768]               0\n",
      "              Mlp-79             [-1, 768, 768]               0\n",
      "         Identity-80             [-1, 768, 768]               0\n",
      "            Block-81             [-1, 768, 768]               0\n",
      "        LayerNorm-82             [-1, 768, 768]           1,536\n",
      "           Linear-83            [-1, 768, 2304]       1,771,776\n",
      "          Dropout-84         [-1, 12, 768, 768]               0\n",
      "           Linear-85             [-1, 768, 768]         590,592\n",
      "          Dropout-86             [-1, 768, 768]               0\n",
      "        Attention-87             [-1, 768, 768]               0\n",
      "         Identity-88             [-1, 768, 768]               0\n",
      "        LayerNorm-89             [-1, 768, 768]           1,536\n",
      "           Linear-90            [-1, 768, 3072]       2,362,368\n",
      "             GELU-91            [-1, 768, 3072]               0\n",
      "          Dropout-92            [-1, 768, 3072]               0\n",
      "           Linear-93             [-1, 768, 768]       2,360,064\n",
      "          Dropout-94             [-1, 768, 768]               0\n",
      "              Mlp-95             [-1, 768, 768]               0\n",
      "         Identity-96             [-1, 768, 768]               0\n",
      "            Block-97             [-1, 768, 768]               0\n",
      "        LayerNorm-98             [-1, 768, 768]           1,536\n",
      "           Linear-99            [-1, 768, 2304]       1,771,776\n",
      "         Dropout-100         [-1, 12, 768, 768]               0\n",
      "          Linear-101             [-1, 768, 768]         590,592\n",
      "         Dropout-102             [-1, 768, 768]               0\n",
      "       Attention-103             [-1, 768, 768]               0\n",
      "        Identity-104             [-1, 768, 768]               0\n",
      "       LayerNorm-105             [-1, 768, 768]           1,536\n",
      "          Linear-106            [-1, 768, 3072]       2,362,368\n",
      "            GELU-107            [-1, 768, 3072]               0\n",
      "         Dropout-108            [-1, 768, 3072]               0\n",
      "          Linear-109             [-1, 768, 768]       2,360,064\n",
      "         Dropout-110             [-1, 768, 768]               0\n",
      "             Mlp-111             [-1, 768, 768]               0\n",
      "        Identity-112             [-1, 768, 768]               0\n",
      "           Block-113             [-1, 768, 768]               0\n",
      "       LayerNorm-114             [-1, 768, 768]           1,536\n",
      "          Linear-115            [-1, 768, 2304]       1,771,776\n",
      "         Dropout-116         [-1, 12, 768, 768]               0\n",
      "          Linear-117             [-1, 768, 768]         590,592\n",
      "         Dropout-118             [-1, 768, 768]               0\n",
      "       Attention-119             [-1, 768, 768]               0\n",
      "        Identity-120             [-1, 768, 768]               0\n",
      "       LayerNorm-121             [-1, 768, 768]           1,536\n",
      "          Linear-122            [-1, 768, 3072]       2,362,368\n",
      "            GELU-123            [-1, 768, 3072]               0\n",
      "         Dropout-124            [-1, 768, 3072]               0\n",
      "          Linear-125             [-1, 768, 768]       2,360,064\n",
      "         Dropout-126             [-1, 768, 768]               0\n",
      "             Mlp-127             [-1, 768, 768]               0\n",
      "        Identity-128             [-1, 768, 768]               0\n",
      "           Block-129             [-1, 768, 768]               0\n",
      "       LayerNorm-130             [-1, 768, 768]           1,536\n",
      "          Linear-131            [-1, 768, 2304]       1,771,776\n",
      "         Dropout-132         [-1, 12, 768, 768]               0\n",
      "          Linear-133             [-1, 768, 768]         590,592\n",
      "         Dropout-134             [-1, 768, 768]               0\n",
      "       Attention-135             [-1, 768, 768]               0\n",
      "        Identity-136             [-1, 768, 768]               0\n",
      "       LayerNorm-137             [-1, 768, 768]           1,536\n",
      "          Linear-138            [-1, 768, 3072]       2,362,368\n",
      "            GELU-139            [-1, 768, 3072]               0\n",
      "         Dropout-140            [-1, 768, 3072]               0\n",
      "          Linear-141             [-1, 768, 768]       2,360,064\n",
      "         Dropout-142             [-1, 768, 768]               0\n",
      "             Mlp-143             [-1, 768, 768]               0\n",
      "        Identity-144             [-1, 768, 768]               0\n",
      "           Block-145             [-1, 768, 768]               0\n",
      "       LayerNorm-146             [-1, 768, 768]           1,536\n",
      "          Linear-147            [-1, 768, 2304]       1,771,776\n",
      "         Dropout-148         [-1, 12, 768, 768]               0\n",
      "          Linear-149             [-1, 768, 768]         590,592\n",
      "         Dropout-150             [-1, 768, 768]               0\n",
      "       Attention-151             [-1, 768, 768]               0\n",
      "        Identity-152             [-1, 768, 768]               0\n",
      "       LayerNorm-153             [-1, 768, 768]           1,536\n",
      "          Linear-154            [-1, 768, 3072]       2,362,368\n",
      "            GELU-155            [-1, 768, 3072]               0\n",
      "         Dropout-156            [-1, 768, 3072]               0\n",
      "          Linear-157             [-1, 768, 768]       2,360,064\n",
      "         Dropout-158             [-1, 768, 768]               0\n",
      "             Mlp-159             [-1, 768, 768]               0\n",
      "        Identity-160             [-1, 768, 768]               0\n",
      "           Block-161             [-1, 768, 768]               0\n",
      "       LayerNorm-162             [-1, 768, 768]           1,536\n",
      "          Linear-163            [-1, 768, 2304]       1,771,776\n",
      "         Dropout-164         [-1, 12, 768, 768]               0\n",
      "          Linear-165             [-1, 768, 768]         590,592\n",
      "         Dropout-166             [-1, 768, 768]               0\n",
      "       Attention-167             [-1, 768, 768]               0\n",
      "        Identity-168             [-1, 768, 768]               0\n",
      "       LayerNorm-169             [-1, 768, 768]           1,536\n",
      "          Linear-170            [-1, 768, 3072]       2,362,368\n",
      "            GELU-171            [-1, 768, 3072]               0\n",
      "         Dropout-172            [-1, 768, 3072]               0\n",
      "          Linear-173             [-1, 768, 768]       2,360,064\n",
      "         Dropout-174             [-1, 768, 768]               0\n",
      "             Mlp-175             [-1, 768, 768]               0\n",
      "        Identity-176             [-1, 768, 768]               0\n",
      "           Block-177             [-1, 768, 768]               0\n",
      "       LayerNorm-178             [-1, 768, 768]           1,536\n",
      "          Linear-179            [-1, 768, 2304]       1,771,776\n",
      "         Dropout-180         [-1, 12, 768, 768]               0\n",
      "          Linear-181             [-1, 768, 768]         590,592\n",
      "         Dropout-182             [-1, 768, 768]               0\n",
      "       Attention-183             [-1, 768, 768]               0\n",
      "        Identity-184             [-1, 768, 768]               0\n",
      "       LayerNorm-185             [-1, 768, 768]           1,536\n",
      "          Linear-186            [-1, 768, 3072]       2,362,368\n",
      "            GELU-187            [-1, 768, 3072]               0\n",
      "         Dropout-188            [-1, 768, 3072]               0\n",
      "          Linear-189             [-1, 768, 768]       2,360,064\n",
      "         Dropout-190             [-1, 768, 768]               0\n",
      "             Mlp-191             [-1, 768, 768]               0\n",
      "        Identity-192             [-1, 768, 768]               0\n",
      "           Block-193             [-1, 768, 768]               0\n",
      "================================================================\n",
      "Total params: 85,251,840\n",
      "Trainable params: 85,251,840\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 2056.50\n",
      "Params size (MB): 325.21\n",
      "Estimated Total Size (MB): 2382.46\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (768, 256), device='cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
