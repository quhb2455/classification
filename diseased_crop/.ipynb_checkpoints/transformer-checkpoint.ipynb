{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03150c0",
   "metadata": {},
   "source": [
    "## Input layer 바꾸기 -> patch embedding to linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "920ffa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: easydict in c:\\users\\quhb2\\anaconda3\\envs\\torch-1.9\\lib\\site-packages (1.9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\quhb2\\anaconda3\\envs\\torch-1.9\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65f082df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "772fc011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm \n",
    "\n",
    "_model = timm.create_model('vit_base_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1234dc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
      "          Identity-2             [-1, 196, 768]               0\n",
      "        PatchEmbed-3             [-1, 196, 768]               0\n",
      "           Dropout-4             [-1, 197, 768]               0\n",
      "         LayerNorm-5             [-1, 197, 768]           1,536\n",
      "            Linear-6            [-1, 197, 2304]       1,771,776\n",
      "           Dropout-7         [-1, 12, 197, 197]               0\n",
      "            Linear-8             [-1, 197, 768]         590,592\n",
      "           Dropout-9             [-1, 197, 768]               0\n",
      "        Attention-10             [-1, 197, 768]               0\n",
      "         Identity-11             [-1, 197, 768]               0\n",
      "        LayerNorm-12             [-1, 197, 768]           1,536\n",
      "           Linear-13            [-1, 197, 3072]       2,362,368\n",
      "             GELU-14            [-1, 197, 3072]               0\n",
      "          Dropout-15            [-1, 197, 3072]               0\n",
      "           Linear-16             [-1, 197, 768]       2,360,064\n",
      "          Dropout-17             [-1, 197, 768]               0\n",
      "              Mlp-18             [-1, 197, 768]               0\n",
      "         Identity-19             [-1, 197, 768]               0\n",
      "            Block-20             [-1, 197, 768]               0\n",
      "        LayerNorm-21             [-1, 197, 768]           1,536\n",
      "           Linear-22            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-23         [-1, 12, 197, 197]               0\n",
      "           Linear-24             [-1, 197, 768]         590,592\n",
      "          Dropout-25             [-1, 197, 768]               0\n",
      "        Attention-26             [-1, 197, 768]               0\n",
      "         Identity-27             [-1, 197, 768]               0\n",
      "        LayerNorm-28             [-1, 197, 768]           1,536\n",
      "           Linear-29            [-1, 197, 3072]       2,362,368\n",
      "             GELU-30            [-1, 197, 3072]               0\n",
      "          Dropout-31            [-1, 197, 3072]               0\n",
      "           Linear-32             [-1, 197, 768]       2,360,064\n",
      "          Dropout-33             [-1, 197, 768]               0\n",
      "              Mlp-34             [-1, 197, 768]               0\n",
      "         Identity-35             [-1, 197, 768]               0\n",
      "            Block-36             [-1, 197, 768]               0\n",
      "        LayerNorm-37             [-1, 197, 768]           1,536\n",
      "           Linear-38            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-39         [-1, 12, 197, 197]               0\n",
      "           Linear-40             [-1, 197, 768]         590,592\n",
      "          Dropout-41             [-1, 197, 768]               0\n",
      "        Attention-42             [-1, 197, 768]               0\n",
      "         Identity-43             [-1, 197, 768]               0\n",
      "        LayerNorm-44             [-1, 197, 768]           1,536\n",
      "           Linear-45            [-1, 197, 3072]       2,362,368\n",
      "             GELU-46            [-1, 197, 3072]               0\n",
      "          Dropout-47            [-1, 197, 3072]               0\n",
      "           Linear-48             [-1, 197, 768]       2,360,064\n",
      "          Dropout-49             [-1, 197, 768]               0\n",
      "              Mlp-50             [-1, 197, 768]               0\n",
      "         Identity-51             [-1, 197, 768]               0\n",
      "            Block-52             [-1, 197, 768]               0\n",
      "        LayerNorm-53             [-1, 197, 768]           1,536\n",
      "           Linear-54            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-55         [-1, 12, 197, 197]               0\n",
      "           Linear-56             [-1, 197, 768]         590,592\n",
      "          Dropout-57             [-1, 197, 768]               0\n",
      "        Attention-58             [-1, 197, 768]               0\n",
      "         Identity-59             [-1, 197, 768]               0\n",
      "        LayerNorm-60             [-1, 197, 768]           1,536\n",
      "           Linear-61            [-1, 197, 3072]       2,362,368\n",
      "             GELU-62            [-1, 197, 3072]               0\n",
      "          Dropout-63            [-1, 197, 3072]               0\n",
      "           Linear-64             [-1, 197, 768]       2,360,064\n",
      "          Dropout-65             [-1, 197, 768]               0\n",
      "              Mlp-66             [-1, 197, 768]               0\n",
      "         Identity-67             [-1, 197, 768]               0\n",
      "            Block-68             [-1, 197, 768]               0\n",
      "        LayerNorm-69             [-1, 197, 768]           1,536\n",
      "           Linear-70            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-71         [-1, 12, 197, 197]               0\n",
      "           Linear-72             [-1, 197, 768]         590,592\n",
      "          Dropout-73             [-1, 197, 768]               0\n",
      "        Attention-74             [-1, 197, 768]               0\n",
      "         Identity-75             [-1, 197, 768]               0\n",
      "        LayerNorm-76             [-1, 197, 768]           1,536\n",
      "           Linear-77            [-1, 197, 3072]       2,362,368\n",
      "             GELU-78            [-1, 197, 3072]               0\n",
      "          Dropout-79            [-1, 197, 3072]               0\n",
      "           Linear-80             [-1, 197, 768]       2,360,064\n",
      "          Dropout-81             [-1, 197, 768]               0\n",
      "              Mlp-82             [-1, 197, 768]               0\n",
      "         Identity-83             [-1, 197, 768]               0\n",
      "            Block-84             [-1, 197, 768]               0\n",
      "        LayerNorm-85             [-1, 197, 768]           1,536\n",
      "           Linear-86            [-1, 197, 2304]       1,771,776\n",
      "          Dropout-87         [-1, 12, 197, 197]               0\n",
      "           Linear-88             [-1, 197, 768]         590,592\n",
      "          Dropout-89             [-1, 197, 768]               0\n",
      "        Attention-90             [-1, 197, 768]               0\n",
      "         Identity-91             [-1, 197, 768]               0\n",
      "        LayerNorm-92             [-1, 197, 768]           1,536\n",
      "           Linear-93            [-1, 197, 3072]       2,362,368\n",
      "             GELU-94            [-1, 197, 3072]               0\n",
      "          Dropout-95            [-1, 197, 3072]               0\n",
      "           Linear-96             [-1, 197, 768]       2,360,064\n",
      "          Dropout-97             [-1, 197, 768]               0\n",
      "              Mlp-98             [-1, 197, 768]               0\n",
      "         Identity-99             [-1, 197, 768]               0\n",
      "           Block-100             [-1, 197, 768]               0\n",
      "       LayerNorm-101             [-1, 197, 768]           1,536\n",
      "          Linear-102            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-103         [-1, 12, 197, 197]               0\n",
      "          Linear-104             [-1, 197, 768]         590,592\n",
      "         Dropout-105             [-1, 197, 768]               0\n",
      "       Attention-106             [-1, 197, 768]               0\n",
      "        Identity-107             [-1, 197, 768]               0\n",
      "       LayerNorm-108             [-1, 197, 768]           1,536\n",
      "          Linear-109            [-1, 197, 3072]       2,362,368\n",
      "            GELU-110            [-1, 197, 3072]               0\n",
      "         Dropout-111            [-1, 197, 3072]               0\n",
      "          Linear-112             [-1, 197, 768]       2,360,064\n",
      "         Dropout-113             [-1, 197, 768]               0\n",
      "             Mlp-114             [-1, 197, 768]               0\n",
      "        Identity-115             [-1, 197, 768]               0\n",
      "           Block-116             [-1, 197, 768]               0\n",
      "       LayerNorm-117             [-1, 197, 768]           1,536\n",
      "          Linear-118            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-119         [-1, 12, 197, 197]               0\n",
      "          Linear-120             [-1, 197, 768]         590,592\n",
      "         Dropout-121             [-1, 197, 768]               0\n",
      "       Attention-122             [-1, 197, 768]               0\n",
      "        Identity-123             [-1, 197, 768]               0\n",
      "       LayerNorm-124             [-1, 197, 768]           1,536\n",
      "          Linear-125            [-1, 197, 3072]       2,362,368\n",
      "            GELU-126            [-1, 197, 3072]               0\n",
      "         Dropout-127            [-1, 197, 3072]               0\n",
      "          Linear-128             [-1, 197, 768]       2,360,064\n",
      "         Dropout-129             [-1, 197, 768]               0\n",
      "             Mlp-130             [-1, 197, 768]               0\n",
      "        Identity-131             [-1, 197, 768]               0\n",
      "           Block-132             [-1, 197, 768]               0\n",
      "       LayerNorm-133             [-1, 197, 768]           1,536\n",
      "          Linear-134            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-135         [-1, 12, 197, 197]               0\n",
      "          Linear-136             [-1, 197, 768]         590,592\n",
      "         Dropout-137             [-1, 197, 768]               0\n",
      "       Attention-138             [-1, 197, 768]               0\n",
      "        Identity-139             [-1, 197, 768]               0\n",
      "       LayerNorm-140             [-1, 197, 768]           1,536\n",
      "          Linear-141            [-1, 197, 3072]       2,362,368\n",
      "            GELU-142            [-1, 197, 3072]               0\n",
      "         Dropout-143            [-1, 197, 3072]               0\n",
      "          Linear-144             [-1, 197, 768]       2,360,064\n",
      "         Dropout-145             [-1, 197, 768]               0\n",
      "             Mlp-146             [-1, 197, 768]               0\n",
      "        Identity-147             [-1, 197, 768]               0\n",
      "           Block-148             [-1, 197, 768]               0\n",
      "       LayerNorm-149             [-1, 197, 768]           1,536\n",
      "          Linear-150            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-151         [-1, 12, 197, 197]               0\n",
      "          Linear-152             [-1, 197, 768]         590,592\n",
      "         Dropout-153             [-1, 197, 768]               0\n",
      "       Attention-154             [-1, 197, 768]               0\n",
      "        Identity-155             [-1, 197, 768]               0\n",
      "       LayerNorm-156             [-1, 197, 768]           1,536\n",
      "          Linear-157            [-1, 197, 3072]       2,362,368\n",
      "            GELU-158            [-1, 197, 3072]               0\n",
      "         Dropout-159            [-1, 197, 3072]               0\n",
      "          Linear-160             [-1, 197, 768]       2,360,064\n",
      "         Dropout-161             [-1, 197, 768]               0\n",
      "             Mlp-162             [-1, 197, 768]               0\n",
      "        Identity-163             [-1, 197, 768]               0\n",
      "           Block-164             [-1, 197, 768]               0\n",
      "       LayerNorm-165             [-1, 197, 768]           1,536\n",
      "          Linear-166            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-167         [-1, 12, 197, 197]               0\n",
      "          Linear-168             [-1, 197, 768]         590,592\n",
      "         Dropout-169             [-1, 197, 768]               0\n",
      "       Attention-170             [-1, 197, 768]               0\n",
      "        Identity-171             [-1, 197, 768]               0\n",
      "       LayerNorm-172             [-1, 197, 768]           1,536\n",
      "          Linear-173            [-1, 197, 3072]       2,362,368\n",
      "            GELU-174            [-1, 197, 3072]               0\n",
      "         Dropout-175            [-1, 197, 3072]               0\n",
      "          Linear-176             [-1, 197, 768]       2,360,064\n",
      "         Dropout-177             [-1, 197, 768]               0\n",
      "             Mlp-178             [-1, 197, 768]               0\n",
      "        Identity-179             [-1, 197, 768]               0\n",
      "           Block-180             [-1, 197, 768]               0\n",
      "       LayerNorm-181             [-1, 197, 768]           1,536\n",
      "          Linear-182            [-1, 197, 2304]       1,771,776\n",
      "         Dropout-183         [-1, 12, 197, 197]               0\n",
      "          Linear-184             [-1, 197, 768]         590,592\n",
      "         Dropout-185             [-1, 197, 768]               0\n",
      "       Attention-186             [-1, 197, 768]               0\n",
      "        Identity-187             [-1, 197, 768]               0\n",
      "       LayerNorm-188             [-1, 197, 768]           1,536\n",
      "          Linear-189            [-1, 197, 3072]       2,362,368\n",
      "            GELU-190            [-1, 197, 3072]               0\n",
      "         Dropout-191            [-1, 197, 3072]               0\n",
      "          Linear-192             [-1, 197, 768]       2,360,064\n",
      "         Dropout-193             [-1, 197, 768]               0\n",
      "             Mlp-194             [-1, 197, 768]               0\n",
      "        Identity-195             [-1, 197, 768]               0\n",
      "           Block-196             [-1, 197, 768]               0\n",
      "       LayerNorm-197             [-1, 197, 768]           1,536\n",
      "        Identity-198                  [-1, 768]               0\n",
      "          Linear-199                 [-1, 1000]         769,000\n",
      "================================================================\n",
      "Total params: 86,415,592\n",
      "Trainable params: 86,415,592\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 408.54\n",
      "Params size (MB): 329.65\n",
      "Estimated Total Size (MB): 738.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn([3,224,224])\n",
    "# data.shape\n",
    "summary(_model, (3,224,224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "316ef763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class custom(nn.Module) :\n",
    "    def __init__(self, out) :\n",
    "        super(custom, self).__init__()\n",
    "        self.out = out\n",
    "        self.model = nn.Sequential(nn.Linear(777, 768), _model.blocks, _model.norm, _model.pre_logits, _model.head)\n",
    "        self.linear = nn.Linear(1000, out)\n",
    "        self.avgpool1d = nn.AdaptiveAvgPool1d((1))\n",
    "        \n",
    "        \n",
    "    def forward(self, x) :\n",
    "        x = self.model(x)\n",
    "        print(x.shape)\n",
    "        x = x.transpose(2,1)\n",
    "        print(x.shape)\n",
    "        x = self.avgpool1d(x)\n",
    "        print(x.shape)\n",
    "        x = x.squeeze(2)\n",
    "        print(x.shape)\n",
    "        x = self.linear(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "modelcus = custom(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bd5a99a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768, 1000])\n",
      "torch.Size([2, 1000, 768])\n",
      "torch.Size([2, 1000, 1])\n",
      "torch.Size([2, 1000])\n",
      "torch.Size([2, 25])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1             [-1, 768, 768]         597,504\n",
      "         LayerNorm-2             [-1, 768, 768]           1,536\n",
      "            Linear-3            [-1, 768, 2304]       1,771,776\n",
      "           Dropout-4         [-1, 12, 768, 768]               0\n",
      "            Linear-5             [-1, 768, 768]         590,592\n",
      "           Dropout-6             [-1, 768, 768]               0\n",
      "         Attention-7             [-1, 768, 768]               0\n",
      "          Identity-8             [-1, 768, 768]               0\n",
      "         LayerNorm-9             [-1, 768, 768]           1,536\n",
      "           Linear-10            [-1, 768, 3072]       2,362,368\n",
      "             GELU-11            [-1, 768, 3072]               0\n",
      "          Dropout-12            [-1, 768, 3072]               0\n",
      "           Linear-13             [-1, 768, 768]       2,360,064\n",
      "          Dropout-14             [-1, 768, 768]               0\n",
      "              Mlp-15             [-1, 768, 768]               0\n",
      "         Identity-16             [-1, 768, 768]               0\n",
      "            Block-17             [-1, 768, 768]               0\n",
      "        LayerNorm-18             [-1, 768, 768]           1,536\n",
      "           Linear-19            [-1, 768, 2304]       1,771,776\n",
      "          Dropout-20         [-1, 12, 768, 768]               0\n",
      "           Linear-21             [-1, 768, 768]         590,592\n",
      "          Dropout-22             [-1, 768, 768]               0\n",
      "        Attention-23             [-1, 768, 768]               0\n",
      "         Identity-24             [-1, 768, 768]               0\n",
      "        LayerNorm-25             [-1, 768, 768]           1,536\n",
      "           Linear-26            [-1, 768, 3072]       2,362,368\n",
      "             GELU-27            [-1, 768, 3072]               0\n",
      "          Dropout-28            [-1, 768, 3072]               0\n",
      "           Linear-29             [-1, 768, 768]       2,360,064\n",
      "          Dropout-30             [-1, 768, 768]               0\n",
      "              Mlp-31             [-1, 768, 768]               0\n",
      "         Identity-32             [-1, 768, 768]               0\n",
      "            Block-33             [-1, 768, 768]               0\n",
      "        LayerNorm-34             [-1, 768, 768]           1,536\n",
      "           Linear-35            [-1, 768, 2304]       1,771,776\n",
      "          Dropout-36         [-1, 12, 768, 768]               0\n",
      "           Linear-37             [-1, 768, 768]         590,592\n",
      "          Dropout-38             [-1, 768, 768]               0\n",
      "        Attention-39             [-1, 768, 768]               0\n",
      "         Identity-40             [-1, 768, 768]               0\n",
      "        LayerNorm-41             [-1, 768, 768]           1,536\n",
      "           Linear-42            [-1, 768, 3072]       2,362,368\n",
      "             GELU-43            [-1, 768, 3072]               0\n",
      "          Dropout-44            [-1, 768, 3072]               0\n",
      "           Linear-45             [-1, 768, 768]       2,360,064\n",
      "          Dropout-46             [-1, 768, 768]               0\n",
      "              Mlp-47             [-1, 768, 768]               0\n",
      "         Identity-48             [-1, 768, 768]               0\n",
      "            Block-49             [-1, 768, 768]               0\n",
      "        LayerNorm-50             [-1, 768, 768]           1,536\n",
      "           Linear-51            [-1, 768, 2304]       1,771,776\n",
      "          Dropout-52         [-1, 12, 768, 768]               0\n",
      "           Linear-53             [-1, 768, 768]         590,592\n",
      "          Dropout-54             [-1, 768, 768]               0\n",
      "        Attention-55             [-1, 768, 768]               0\n",
      "         Identity-56             [-1, 768, 768]               0\n",
      "        LayerNorm-57             [-1, 768, 768]           1,536\n",
      "           Linear-58            [-1, 768, 3072]       2,362,368\n",
      "             GELU-59            [-1, 768, 3072]               0\n",
      "          Dropout-60            [-1, 768, 3072]               0\n",
      "           Linear-61             [-1, 768, 768]       2,360,064\n",
      "          Dropout-62             [-1, 768, 768]               0\n",
      "              Mlp-63             [-1, 768, 768]               0\n",
      "         Identity-64             [-1, 768, 768]               0\n",
      "            Block-65             [-1, 768, 768]               0\n",
      "        LayerNorm-66             [-1, 768, 768]           1,536\n",
      "           Linear-67            [-1, 768, 2304]       1,771,776\n",
      "          Dropout-68         [-1, 12, 768, 768]               0\n",
      "           Linear-69             [-1, 768, 768]         590,592\n",
      "          Dropout-70             [-1, 768, 768]               0\n",
      "        Attention-71             [-1, 768, 768]               0\n",
      "         Identity-72             [-1, 768, 768]               0\n",
      "        LayerNorm-73             [-1, 768, 768]           1,536\n",
      "           Linear-74            [-1, 768, 3072]       2,362,368\n",
      "             GELU-75            [-1, 768, 3072]               0\n",
      "          Dropout-76            [-1, 768, 3072]               0\n",
      "           Linear-77             [-1, 768, 768]       2,360,064\n",
      "          Dropout-78             [-1, 768, 768]               0\n",
      "              Mlp-79             [-1, 768, 768]               0\n",
      "         Identity-80             [-1, 768, 768]               0\n",
      "            Block-81             [-1, 768, 768]               0\n",
      "        LayerNorm-82             [-1, 768, 768]           1,536\n",
      "           Linear-83            [-1, 768, 2304]       1,771,776\n",
      "          Dropout-84         [-1, 12, 768, 768]               0\n",
      "           Linear-85             [-1, 768, 768]         590,592\n",
      "          Dropout-86             [-1, 768, 768]               0\n",
      "        Attention-87             [-1, 768, 768]               0\n",
      "         Identity-88             [-1, 768, 768]               0\n",
      "        LayerNorm-89             [-1, 768, 768]           1,536\n",
      "           Linear-90            [-1, 768, 3072]       2,362,368\n",
      "             GELU-91            [-1, 768, 3072]               0\n",
      "          Dropout-92            [-1, 768, 3072]               0\n",
      "           Linear-93             [-1, 768, 768]       2,360,064\n",
      "          Dropout-94             [-1, 768, 768]               0\n",
      "              Mlp-95             [-1, 768, 768]               0\n",
      "         Identity-96             [-1, 768, 768]               0\n",
      "            Block-97             [-1, 768, 768]               0\n",
      "        LayerNorm-98             [-1, 768, 768]           1,536\n",
      "           Linear-99            [-1, 768, 2304]       1,771,776\n",
      "         Dropout-100         [-1, 12, 768, 768]               0\n",
      "          Linear-101             [-1, 768, 768]         590,592\n",
      "         Dropout-102             [-1, 768, 768]               0\n",
      "       Attention-103             [-1, 768, 768]               0\n",
      "        Identity-104             [-1, 768, 768]               0\n",
      "       LayerNorm-105             [-1, 768, 768]           1,536\n",
      "          Linear-106            [-1, 768, 3072]       2,362,368\n",
      "            GELU-107            [-1, 768, 3072]               0\n",
      "         Dropout-108            [-1, 768, 3072]               0\n",
      "          Linear-109             [-1, 768, 768]       2,360,064\n",
      "         Dropout-110             [-1, 768, 768]               0\n",
      "             Mlp-111             [-1, 768, 768]               0\n",
      "        Identity-112             [-1, 768, 768]               0\n",
      "           Block-113             [-1, 768, 768]               0\n",
      "       LayerNorm-114             [-1, 768, 768]           1,536\n",
      "          Linear-115            [-1, 768, 2304]       1,771,776\n",
      "         Dropout-116         [-1, 12, 768, 768]               0\n",
      "          Linear-117             [-1, 768, 768]         590,592\n",
      "         Dropout-118             [-1, 768, 768]               0\n",
      "       Attention-119             [-1, 768, 768]               0\n",
      "        Identity-120             [-1, 768, 768]               0\n",
      "       LayerNorm-121             [-1, 768, 768]           1,536\n",
      "          Linear-122            [-1, 768, 3072]       2,362,368\n",
      "            GELU-123            [-1, 768, 3072]               0\n",
      "         Dropout-124            [-1, 768, 3072]               0\n",
      "          Linear-125             [-1, 768, 768]       2,360,064\n",
      "         Dropout-126             [-1, 768, 768]               0\n",
      "             Mlp-127             [-1, 768, 768]               0\n",
      "        Identity-128             [-1, 768, 768]               0\n",
      "           Block-129             [-1, 768, 768]               0\n",
      "       LayerNorm-130             [-1, 768, 768]           1,536\n",
      "          Linear-131            [-1, 768, 2304]       1,771,776\n",
      "         Dropout-132         [-1, 12, 768, 768]               0\n",
      "          Linear-133             [-1, 768, 768]         590,592\n",
      "         Dropout-134             [-1, 768, 768]               0\n",
      "       Attention-135             [-1, 768, 768]               0\n",
      "        Identity-136             [-1, 768, 768]               0\n",
      "       LayerNorm-137             [-1, 768, 768]           1,536\n",
      "          Linear-138            [-1, 768, 3072]       2,362,368\n",
      "            GELU-139            [-1, 768, 3072]               0\n",
      "         Dropout-140            [-1, 768, 3072]               0\n",
      "          Linear-141             [-1, 768, 768]       2,360,064\n",
      "         Dropout-142             [-1, 768, 768]               0\n",
      "             Mlp-143             [-1, 768, 768]               0\n",
      "        Identity-144             [-1, 768, 768]               0\n",
      "           Block-145             [-1, 768, 768]               0\n",
      "       LayerNorm-146             [-1, 768, 768]           1,536\n",
      "          Linear-147            [-1, 768, 2304]       1,771,776\n",
      "         Dropout-148         [-1, 12, 768, 768]               0\n",
      "          Linear-149             [-1, 768, 768]         590,592\n",
      "         Dropout-150             [-1, 768, 768]               0\n",
      "       Attention-151             [-1, 768, 768]               0\n",
      "        Identity-152             [-1, 768, 768]               0\n",
      "       LayerNorm-153             [-1, 768, 768]           1,536\n",
      "          Linear-154            [-1, 768, 3072]       2,362,368\n",
      "            GELU-155            [-1, 768, 3072]               0\n",
      "         Dropout-156            [-1, 768, 3072]               0\n",
      "          Linear-157             [-1, 768, 768]       2,360,064\n",
      "         Dropout-158             [-1, 768, 768]               0\n",
      "             Mlp-159             [-1, 768, 768]               0\n",
      "        Identity-160             [-1, 768, 768]               0\n",
      "           Block-161             [-1, 768, 768]               0\n",
      "       LayerNorm-162             [-1, 768, 768]           1,536\n",
      "          Linear-163            [-1, 768, 2304]       1,771,776\n",
      "         Dropout-164         [-1, 12, 768, 768]               0\n",
      "          Linear-165             [-1, 768, 768]         590,592\n",
      "         Dropout-166             [-1, 768, 768]               0\n",
      "       Attention-167             [-1, 768, 768]               0\n",
      "        Identity-168             [-1, 768, 768]               0\n",
      "       LayerNorm-169             [-1, 768, 768]           1,536\n",
      "          Linear-170            [-1, 768, 3072]       2,362,368\n",
      "            GELU-171            [-1, 768, 3072]               0\n",
      "         Dropout-172            [-1, 768, 3072]               0\n",
      "          Linear-173             [-1, 768, 768]       2,360,064\n",
      "         Dropout-174             [-1, 768, 768]               0\n",
      "             Mlp-175             [-1, 768, 768]               0\n",
      "        Identity-176             [-1, 768, 768]               0\n",
      "           Block-177             [-1, 768, 768]               0\n",
      "       LayerNorm-178             [-1, 768, 768]           1,536\n",
      "          Linear-179            [-1, 768, 2304]       1,771,776\n",
      "         Dropout-180         [-1, 12, 768, 768]               0\n",
      "          Linear-181             [-1, 768, 768]         590,592\n",
      "         Dropout-182             [-1, 768, 768]               0\n",
      "       Attention-183             [-1, 768, 768]               0\n",
      "        Identity-184             [-1, 768, 768]               0\n",
      "       LayerNorm-185             [-1, 768, 768]           1,536\n",
      "          Linear-186            [-1, 768, 3072]       2,362,368\n",
      "            GELU-187            [-1, 768, 3072]               0\n",
      "         Dropout-188            [-1, 768, 3072]               0\n",
      "          Linear-189             [-1, 768, 768]       2,360,064\n",
      "         Dropout-190             [-1, 768, 768]               0\n",
      "             Mlp-191             [-1, 768, 768]               0\n",
      "        Identity-192             [-1, 768, 768]               0\n",
      "           Block-193             [-1, 768, 768]               0\n",
      "       LayerNorm-194             [-1, 768, 768]           1,536\n",
      "        Identity-195             [-1, 768, 768]               0\n",
      "          Linear-196            [-1, 768, 1000]         769,000\n",
      "AdaptiveAvgPool1d-197              [-1, 1000, 1]               0\n",
      "          Linear-198                   [-1, 25]          25,025\n",
      "================================================================\n",
      "Total params: 86,447,529\n",
      "Trainable params: 86,447,529\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.28\n",
      "Forward/backward pass size (MB): 2071.37\n",
      "Params size (MB): 329.77\n",
      "Estimated Total Size (MB): 2403.41\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(modelcus, (768, 777), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9ad74e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768, 1000])\n",
      "torch.Size([1, 1000, 768])\n",
      "torch.Size([1, 1000, 1])\n",
      "torch.Size([1, 1000])\n",
      "torch.Size([1, 25])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3285,  0.5938,  0.9646,  0.9012,  0.3262,  1.2143,  0.5092, -0.6902,\n",
       "         -0.4887,  0.2381,  1.1416, -0.4408, -0.3450,  0.9816, -0.2160,  0.2707,\n",
       "          0.0627, -0.7833,  1.3075,  0.5925,  1.2748,  0.6680, -0.3639,  0.6835,\n",
       "         -0.3724]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = torch.randn((1, 768, 777))\n",
    "pred = modelcus(data)\n",
    "display(pred.shape)\n",
    "display(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e0b97c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
