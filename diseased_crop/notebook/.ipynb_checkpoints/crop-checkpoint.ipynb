{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4636868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from easydict import EasyDict\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import json \n",
    "import timm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa2ce4",
   "metadata": {},
   "source": [
    "# Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd44a54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def label_preprocessing(path) :\n",
    "#     path = os.path.join(path, 'train.csv')\n",
    "    labels = pd.read_csv(path)\n",
    "\n",
    "    cnt = 0\n",
    "    label_encoder = {}\n",
    "    for i, label in enumerate(tqdm(sorted(labels['label']))) :\n",
    "        \n",
    "        if label not in label_encoder.values() :\n",
    "            label_encoder[cnt] = label\n",
    "            cnt += 1\n",
    "        \n",
    "    label_decoder = {val : key for key, val in label_encoder.items()}\n",
    "    \n",
    "    return label_encoder, label_decoder\n",
    "\n",
    "# enc, dec = label_preprocessing(\"../data/train.csv\")\n",
    "# display(enc)\n",
    "# display(dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45adac9d",
   "metadata": {},
   "source": [
    "# CSV feature - min, max value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195590dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_feature_dict(path, csv_features) :\n",
    "    \n",
    "    csv_files = sorted(glob(os.path.join(path, '*/*.csv')))\n",
    "\n",
    "    temp_csv = pd.read_csv(csv_files[0])[csv_features]\n",
    "    max_arr, min_arr = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "\n",
    "    # feature 별 최대값, 최솟값 계산\n",
    "    for csv in tqdm(csv_files[1:]):\n",
    "        temp_csv = pd.read_csv(csv)[csv_features]\n",
    "        temp_csv = temp_csv.replace('-',np.nan).dropna()\n",
    "        if len(temp_csv) == 0:\n",
    "            continue\n",
    "        temp_csv = temp_csv.astype(float)\n",
    "        temp_max, temp_min = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "        max_arr = np.max([max_arr,temp_max], axis=0)\n",
    "        min_arr = np.min([min_arr,temp_min], axis=0)\n",
    "\n",
    "    # feature 별 최대값, 최솟값 dictionary return\n",
    "    return {csv_features[i]:[min_arr[i], max_arr[i]] for i in range(len(csv_features))}\n",
    "\n",
    "# csv_feature_dict = csv_feature_dict('../data/train', csv_features)\n",
    "# csv_feature_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420ac20a",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a55be8e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_split(path, label_decoder, test_size=0.2) : \n",
    "    imgs = glob(os.path.join(path, '*/*.jpg'))\n",
    "    json_files = glob(os.path.join(path, '*/*.json'))\n",
    "    \n",
    "    label_list = []\n",
    "    for json_path in tqdm(json_files) :\n",
    "        json_file = json.load(open(json_path, 'r'))\n",
    "        \n",
    "        crop = json_file[\"annotations\"][\"crop\"]\n",
    "        disease = json_file[\"annotations\"][\"disease\"]\n",
    "        risk = json_file[\"annotations\"][\"risk\"]\n",
    "        \n",
    "        label = f'{crop}_{disease}_{risk}'\n",
    "        label_list.append(label_decoder[label])\n",
    "    \n",
    "    return train_test_split(imgs, test_size=test_size, shuffle=True, stratify=label_list)\n",
    "\n",
    "# a, b = data_split('../data/train', label_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a210c4",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f283bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(size=224):\n",
    "    train_transforms = A.Compose([\n",
    "                A.Resize(size ,size),\n",
    "                A.OneOf([\n",
    "                    A.Rotate(),\n",
    "                    A.HorizontalFlip(),\n",
    "                    A.VerticalFlip()\n",
    "                ], p=1)\n",
    "            ])\n",
    "\n",
    "    val_transforms = A.Compose([\n",
    "        A.Resize(size,size)\n",
    "    ])\n",
    "    \n",
    "    return train_transforms, val_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69b5c3",
   "metadata": {},
   "source": [
    "# Custom Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02b2705e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 files, \n",
    "                 transforms, \n",
    "                 label_decoder, \n",
    "                 opt,\n",
    "                 mode='train'):\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.files = files\n",
    "        self.label_decoder = label_decoder #label_encoder\n",
    "        self.csv_feature_dict = opt.csv_feature_dict\n",
    "        self.max_len = opt.max_len\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        file = self.files[i]\n",
    "#         file_name = file.split('\\\\')[-1].split('.')[0]\n",
    "        \n",
    "        # CSV\n",
    "        csv_data, seq_len = self.csv_preprocessing(file)\n",
    "        \n",
    "        # image\n",
    "        img = self.img_preprocessing(file)\n",
    "        \n",
    "        if self.mode == 'train':         \n",
    "            # Label\n",
    "            label = self.label_preprocessing(file)\n",
    "            \n",
    "            return {\n",
    "                'img' : torch.tensor(img, dtype=torch.float32),\n",
    "                'label' : torch.tensor(self.label_decoder[label], dtype=torch.long),\n",
    "                'csv_feature': torch.tensor(csv_data, dtype=torch.float32),\n",
    "                'seq_len' : seq_len\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            return {\n",
    "                'img' : torch.tensor(img, dtype=torch.float32),\n",
    "                'csv_feature': torch.tensor(csv_data, dtype=torch.float32),\n",
    "                'seq_len' : seq_len\n",
    "            }\n",
    "\n",
    "    def csv_preprocessing(self, file) :\n",
    "        # CSV\n",
    "        csv_path = file.replace(\"jpg\",\"csv\")\n",
    "        df = pd.read_csv(csv_path)[self.csv_feature_dict.keys()]\n",
    "        df = df.replace('-', 0)\n",
    "        \n",
    "        # MinMax scaling\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].astype(float) - self.csv_feature_dict[col][0]\n",
    "            df[col] = df[col] / (self.csv_feature_dict[col][1]-self.csv_feature_dict[col][0])\n",
    "\n",
    "        # pack_padded_sequence 하기 위한 len 추가\n",
    "        seq_len = len(df)\n",
    "\n",
    "        df_np = df.to_numpy()\n",
    "        df_len, df_features = df_np.shape\n",
    "        \n",
    "        csv_data = np.zeros([self.max_len, df_features])\n",
    "        csv_data[0:df_len, :] = df_np\n",
    "        \n",
    "        return csv_data, seq_len\n",
    "\n",
    "    def img_preprocessing(self, file) :\n",
    "        image_path = file\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = self.transforms(image=img)[\"image\"]\n",
    "        img = img.transpose(2,0,1)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def label_preprocessing(self, file) :\n",
    "        json_path = file.replace(\"jpg\",\"json\")\n",
    "        with open(json_path, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "\n",
    "        crop = json_file['annotations']['crop']\n",
    "        disease = json_file['annotations']['disease']\n",
    "        risk = json_file['annotations']['risk']\n",
    "        \n",
    "        return f'{crop}_{disease}_{risk}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce6704d",
   "metadata": {},
   "source": [
    "# Model - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1580252",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CNN_Encoder(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, pretrained_path=None):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        \n",
    "        if pretrained_path :\n",
    "            # no use pretrained model trained with Public dataset\n",
    "            self.model = self.create_pretrained_model(model_name, num_classes, pretrained_path)\n",
    "            \n",
    "        else :            \n",
    "            self.model = timm.create_model(model_name, num_classes=num_classes, pretrained=True)\n",
    "            \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        output = self.model(inputs)\n",
    "        return output\n",
    "\n",
    "    def create_pretrained_model(self, model_name, num_classes, pretrained_path):\n",
    "        pre_model = torch.load(pretrained_path, map_location=\"cpu\")\n",
    "        output_size = pre_model[list(pre_model.keys())[-1]].shape[0]\n",
    " \n",
    "        return nn.Sequential(\n",
    "                    timm.create_model(model_name, num_classes=output_size, pretrained=True),\n",
    "                    nn.Linear(output_size, num_classes)\n",
    "                )\n",
    "\n",
    "# model = CNN_Encoder(\"efficientnetv2_rw_s\", 1000)\n",
    "# model = CNN_Encoder(\"efficientnetv2_rw_s\", 1000, \"../model/k_fold_50k_pretrained_effiv2S/4_f9462_public_vill_50k_pretrain_efficientnetv2S.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5f419",
   "metadata": {},
   "source": [
    "# Model - RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0db46531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class MLSTMfcn(nn.Module):\n",
    "    def __init__(self, *, num_classes, max_seq_len, num_features,\n",
    "                 num_lstm_out=128, num_lstm_layers=1, \n",
    "                 conv1_nf=128, conv2_nf=256, conv3_nf=128,\n",
    "                 lstm_drop_p=0.8, fc_drop_p=0.3):\n",
    "        \n",
    "        super(MLSTMfcn, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_features = num_features\n",
    "\n",
    "        self.num_lstm_out = num_lstm_out\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "\n",
    "        self.conv1_nf = conv1_nf\n",
    "        self.conv2_nf = conv2_nf\n",
    "        self.conv3_nf = conv3_nf\n",
    "\n",
    "        self.lstm_drop_p = lstm_drop_p\n",
    "        self.fc_drop_p = fc_drop_p\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.num_features, \n",
    "                            hidden_size=self.num_lstm_out,\n",
    "                            num_layers=self.num_lstm_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(self.num_features, self.conv1_nf, 8)\n",
    "        self.conv2 = nn.Conv1d(self.conv1_nf, self.conv2_nf, 5)\n",
    "        self.conv3 = nn.Conv1d(self.conv2_nf, self.conv3_nf, 3)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(self.conv1_nf)\n",
    "        self.bn2 = nn.BatchNorm1d(self.conv2_nf)\n",
    "        self.bn3 = nn.BatchNorm1d(self.conv3_nf)\n",
    "\n",
    "        self.se1 = SELayer(self.conv1_nf)  # ex 128\n",
    "        self.se2 = SELayer(self.conv2_nf)  # ex 256\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstmDrop = nn.Dropout(self.lstm_drop_p)\n",
    "        self.convDrop = nn.Dropout(self.fc_drop_p)\n",
    "\n",
    "        self.fc = nn.Linear(self.conv3_nf+self.num_lstm_out, 128)\n",
    "\n",
    "        self.out_layer = nn.Linear(self.num_classes+128, self.num_classes)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, enc_out, x, seq_lens):\n",
    "        ''' input x should be in size [B,T,F], where \n",
    "            B = Batch size\n",
    "            T = Time samples\n",
    "            F = features\n",
    "        '''\n",
    "        x1 = nn.utils.rnn.pack_padded_sequence(x, seq_lens.cpu(), \n",
    "                                               batch_first=True, \n",
    "                                               enforce_sorted=False)\n",
    "        x1, (ht,ct) = self.lstm(x1)\n",
    "        x1, _ = nn.utils.rnn.pad_packed_sequence(x1, batch_first=True, \n",
    "                                                 padding_value=0.0)\n",
    "        x1 = x1[:,-1,:]\n",
    "        \n",
    "        x2 = x.transpose(2,1)\n",
    "        x2 = self.convDrop(self.relu(self.bn1(self.conv1(x2))))\n",
    "        x2 = self.se1(x2)\n",
    "        x2 = self.convDrop(self.relu(self.bn2(self.conv2(x2))))\n",
    "        x2 = self.se2(x2)\n",
    "        x2 = self.convDrop(self.relu(self.bn3(self.conv3(x2))))\n",
    "        x2 = torch.mean(x2,2)\n",
    "        \n",
    "        x_all = torch.cat((x1,x2),dim=1)\n",
    "        x_out = self.fc(x_all)\n",
    "        concat = torch.cat([enc_out, x_out], dim=1)  # enc_out + hidden \n",
    "        output = self.dropout(concat)\n",
    "        x_output = self.out_layer(output)\n",
    "        x_out = F.log_softmax(x_output, dim=1)\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "# model = MLSTMfcn(num_classes=38, max_seq_len=512, num_features=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0100c687",
   "metadata": {},
   "source": [
    "# Model - CNN + RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b59e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2RNN(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(CNN2RNN, self).__init__()\n",
    "        \n",
    "        self.cnn = CNN_Encoder(opt.model_name, opt.num_classes, opt.pretrained_path)\n",
    "        self.rnn = MLSTMfcn(num_classes=opt.num_classes, max_seq_len=opt.max_len, num_features=opt.num_features)\n",
    "\n",
    "\n",
    "    def forward(self, img, seq, seq_len):\n",
    "        cnn_output = self.cnn(img)\n",
    "        output = self.rnn(cnn_output, seq, seq_len)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# model = CNN2RNN(\"efficientnetv2_rw_s\", \n",
    "#                 1000, \n",
    "#                 512,\n",
    "#                 6,\n",
    "#                 \"../model/k_fold_50k_pretrained_effiv2S/4_f9462_public_vill_50k_pretrain_efficientnetv2S.pt\")\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13736063",
   "metadata": {},
   "source": [
    "# CutMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caedba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    " \n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e10041f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e49d8333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_function(real, pred):    \n",
    "    real = real.cpu()\n",
    "    pred = torch.argmax(pred, dim=1).cpu()\n",
    "    score = f1_score(real, pred, average='macro')\n",
    "    return score\n",
    "\n",
    "def run(train_loader, valid_loader, opt) :\n",
    "    print(\"optin : \",opt)\n",
    "    \n",
    "    model = CNN2RNN(opt).to(opt.device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr= opt.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_loss = 10\n",
    "    for epoch in range(opt.epochs) : \n",
    "\n",
    "        # training\n",
    "        tqdm_train = tqdm(train_loader)\n",
    "        train_loss, train_macro_f1 = 0, 0\n",
    "        for batch, batch_item in enumerate(tqdm_train) :\n",
    "            model.train()\n",
    "            \n",
    "            img = batch_item['img'].to(opt.device)\n",
    "            label = batch_item['label'].to(opt.device)\n",
    "            csv_feature = batch_item['csv_feature'].to(opt.device)\n",
    "            seq_lens = batch_item['seq_len'].to(opt.device)\n",
    "\n",
    "            lam = np.random.beta(1.0, 1.0)\n",
    "                        \n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # add - cutmix\n",
    "                rand_index = torch.randperm(img.size()[0])\n",
    "                target_a = label\n",
    "                target_b = label[rand_index]\n",
    "                bbx1, bby1, bbx2, bby2 = rand_bbox(img.size(), lam)\n",
    "                img[:, :, bbx1:bbx2, bby1:bby2] = img[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "                lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (img.size()[-1] * img.size()[-2]))\n",
    "\n",
    "                output = model(img, csv_feature, seq_lens)\n",
    "                loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1. - lam)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            score = accuracy_function(label, output)\n",
    "            \n",
    "            train_loss = (train_loss + loss.item()) / (batch + 1)\n",
    "            train_macro_f1 = (train_macro_f1 + score) / (batch + 1)\n",
    "            \n",
    "            tqdm_train.set_postfix({\"Epoch\" : epoch+1,\n",
    "                                    \"train_loss\" : train_loss,\n",
    "                                    \"train_f1\" : train_macro_f1})\n",
    "            \n",
    "#             print(f\"Traing Epoch : [{epoch}/{opt.epochs}] loss : {train_loss}  f1 : {train_macro_f1}\",end='\\r')\n",
    "            \n",
    "#         print(f\"Traing Epoch : [{epoch}/{opt.epochs}] loss : {train_loss}  f1 : {train_macro_f1}\")\n",
    "        \n",
    "        # validation\n",
    "        tqdm_valid = tqdm(valid_loader)\n",
    "        valid_loss, valid_macro_f1 = 0, 0\n",
    "        for batch, batch_item in enumerate(tqdm_valid) :\n",
    "            img = batch_item['img'].to(opt.device)\n",
    "            label = batch_item['label'].to(opt.device)\n",
    "            csv_feature = batch_item['csv_feature'].to(opt.device)\n",
    "            seq_lens = batch_item['seq_len'].to(opt.device)\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model(img, csv_feature, seq_lens)\n",
    "                loss = criterion(output, label)\n",
    "            score = accuracy_function(label, output)\n",
    "            \n",
    "            valid_loss = (valid_loss + loss.item()) / (batch + 1)\n",
    "            valid_macro_f1 = (valid_macro_f1 + score) / (batch + 1)\n",
    "            \n",
    "            tqdm_valid.set_postfix({\"valid_loss\" : valid_loss,\n",
    "                                    \"valid_f1\" : valid_macro_f1})\n",
    "            \n",
    "#             print(f\"Valid Epoch : [{epoch}/{opt.epochs}] loss : {valid_loss}  f1 : {valid_macro_f1}\",end='\\r')\n",
    "            \n",
    "#         print(f\"Valid Epoch : [{epoch}/{opt.epochs}] loss : {valid_loss}  f1 : {valid_macro_f1}\")\n",
    "\n",
    "        if valid_loss < best_loss :\n",
    "            best_f1 = valid_macro_f1\n",
    "            os.makedirs(opt.save_path, exist_ok=True)\n",
    "            torch.save(model.state_dict(), os.path.join(opt.save_path, f'{epoch}E_{round(valid_loss, 4)}_{opt.model_name}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cc477d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 5766/5766 [00:22<00:00, 253.84it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 5767/5767 [00:00<00:00, 2847386.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 5767/5767 [00:00<00:00, 15077.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# 변수 설명 csv 파일 참조\n",
    "crop = {'1':'딸기','2':'토마토','3':'파프리카','4':'오이','5':'고추','6':'시설포도'}\n",
    "disease = {'1':{'a1':'딸기잿빛곰팡이병','a2':'딸기흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '2':{'a5':'토마토흰가루병','a6':'토마토잿빛곰팡이병','b2':'열과','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '3':{'a9':'파프리카흰가루병','a10':'파프리카잘록병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '4':{'a3':'오이노균병','a4':'오이흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '5':{'a7':'고추탄저병','a8':'고추흰가루병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '6':{'a11':'시설포도탄저병','a12':'시설포도노균병','b4':'일소피해','b5':'축과병'}}\n",
    "risk = {'1':'초기','2':'중기','3':'말기'}\n",
    "\n",
    "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
    "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
    "\n",
    "\n",
    "opt = {\"dataset_path\" : \"../data/train\",\n",
    "       \"label_path\" : \"../data/train.csv\",\n",
    "       \"save_path\" : \"../pretrain\",\n",
    "       \"pretrained_path\" : None,\n",
    "       \"batch_size\" : 16,\n",
    "       \"model_name\" : 'deit_small_patch16_224',\n",
    "       \"resize\" : 224,\n",
    "       \"num_classes\" : 38,\n",
    "       \"learning_rate\" : 1e-4,\n",
    "       \"device\" : \"cuda\", \n",
    "       \"csv_features\" : csv_features,\n",
    "       \"max_len\" : 590,\n",
    "       \"num_features\" : len(csv_features),\n",
    "       \"epochs\" : 1}\n",
    "\n",
    "opt = EasyDict(opt)\n",
    "\n",
    "# csv_feature_dict 옵션 추가\n",
    "opt.csv_feature_dict = csv_feature_dict(opt.dataset_path, opt.csv_features)\n",
    "\n",
    "# label_enc, dec 및 trasnforms 설정\n",
    "label_encoder, label_decoder = label_preprocessing(opt.label_path)\n",
    "train_transforms, valid_transforms = transform(size=opt.resize)\n",
    "\n",
    "# data split with stratify\n",
    "train, valid = data_split(opt.dataset_path, label_decoder)\n",
    "\n",
    "train_dataset = CustomDataset(train, train_transforms, label_decoder, opt)\n",
    "valid_dataset = CustomDataset(valid, valid_transforms, label_decoder, opt)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=opt.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf06b2c0",
   "metadata": {},
   "source": [
    "# Train Without kfold training\n",
    "## loss 출력이랑 f1 score 출력 수정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "323d20ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optin :  {'dataset_path': '../data/train', 'label_path': '../data/train.csv', 'save_path': '../pretrain', 'pretrained_path': None, 'batch_size': 16, 'model_name': 'deit_small_patch16_224', 'resize': 224, 'num_classes': 38, 'learning_rate': 0.0001, 'device': 'cuda', 'csv_features': ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저'], 'max_len': 590, 'num_features': 9, 'epochs': 1, 'csv_feature_dict': {'내부 온도 1 평균': [3.4, 47.3], '내부 온도 1 최고': [3.4, 47.6], '내부 온도 1 최저': [3.3, 47.0], '내부 습도 1 평균': [23.7, 100.0], '내부 습도 1 최고': [25.9, 100.0], '내부 습도 1 최저': [0.0, 100.0], '내부 이슬점 평균': [0.1, 34.5], '내부 이슬점 최고': [0.2, 34.7], '내부 이슬점 최저': [0.0, 34.4]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                          | 0/289 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                                                           | 0/73 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                              | 0/73 [00:00<?, ?it/s, valid_loss=3.62, valid_f1=0]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|▋                                                     | 1/73 [00:00<00:34,  2.11it/s, valid_loss=3.62, valid_f1=0]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  1%|▋                                                     | 1/73 [00:00<00:34,  2.11it/s, valid_loss=3.63, valid_f1=0]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|█▍                                                    | 2/73 [00:00<00:22,  3.15it/s, valid_loss=3.63, valid_f1=0]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  3%|█▎                                               | 2/73 [00:00<00:22,  3.15it/s, valid_loss=2.42, valid_f1=0.0103]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|██                                               | 3/73 [00:00<00:18,  3.73it/s, valid_loss=2.42, valid_f1=0.0103]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  4%|█▉                                              | 3/73 [00:01<00:18,  3.73it/s, valid_loss=1.53, valid_f1=0.00256]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|██▋                                             | 4/73 [00:01<00:16,  4.18it/s, valid_loss=1.53, valid_f1=0.00256]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  5%|██▋                                             | 4/73 [00:01<00:16,  4.18it/s, valid_loss=1.02, valid_f1=0.00779]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|███▎                                            | 5/73 [00:01<00:15,  4.47it/s, valid_loss=1.02, valid_f1=0.00779]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|███▎                                            | 5/73 [00:01<00:15,  4.47it/s, valid_loss=0.779, valid_f1=0.0013]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▉                                            | 6/73 [00:01<00:14,  4.53it/s, valid_loss=0.779, valid_f1=0.0013]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  8%|███▊                                           | 6/73 [00:01<00:14,  4.53it/s, valid_loss=0.63, valid_f1=0.000185]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|████▌                                          | 7/73 [00:01<00:14,  4.62it/s, valid_loss=0.63, valid_f1=0.000185]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 10%|████▌                                          | 7/73 [00:01<00:14,  4.62it/s, valid_loss=0.528, valid_f1=2.32e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|█████▏                                         | 8/73 [00:01<00:14,  4.56it/s, valid_loss=0.528, valid_f1=2.32e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 11%|█████▏                                         | 8/73 [00:02<00:14,  4.56it/s, valid_loss=0.459, valid_f1=0.00247]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▊                                         | 9/73 [00:02<00:13,  4.62it/s, valid_loss=0.459, valid_f1=0.00247]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█████▋                                        | 9/73 [00:02<00:13,  4.62it/s, valid_loss=0.411, valid_f1=0.000247]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|██████▏                                      | 10/73 [00:02<00:14,  4.48it/s, valid_loss=0.411, valid_f1=0.000247]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 14%|██████▎                                       | 10/73 [00:02<00:14,  4.48it/s, valid_loss=0.371, valid_f1=2.25e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|██████▉                                       | 11/73 [00:02<00:14,  4.41it/s, valid_loss=0.371, valid_f1=2.25e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 15%|██████▉                                       | 11/73 [00:02<00:14,  4.41it/s, valid_loss=0.334, valid_f1=0.00309]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████▌                                      | 12/73 [00:02<00:13,  4.40it/s, valid_loss=0.334, valid_f1=0.00309]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 16%|███████▍                                     | 12/73 [00:03<00:13,  4.40it/s, valid_loss=0.309, valid_f1=0.000238]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|████████                                     | 13/73 [00:03<00:13,  4.43it/s, valid_loss=0.309, valid_f1=0.000238]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 18%|████████▎                                      | 13/73 [00:03<00:13,  4.43it/s, valid_loss=0.281, valid_f1=0.0024]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|█████████                                      | 14/73 [00:03<00:13,  4.43it/s, valid_loss=0.281, valid_f1=0.0024]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 19%|█████████                                      | 14/73 [00:03<00:13,  4.43it/s, valid_loss=0.26, valid_f1=0.00144]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|█████████▋                                     | 15/73 [00:03<00:13,  4.44it/s, valid_loss=0.26, valid_f1=0.00144]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 21%|█████████▏                                   | 15/73 [00:03<00:13,  4.44it/s, valid_loss=0.245, valid_f1=0.000891]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|█████████▊                                   | 16/73 [00:03<00:12,  4.40it/s, valid_loss=0.245, valid_f1=0.000891]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 22%|██████████                                    | 16/73 [00:03<00:12,  4.40it/s, valid_loss=0.225, valid_f1=0.00215]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|██████████▋                                   | 17/73 [00:03<00:12,  4.41it/s, valid_loss=0.225, valid_f1=0.00215]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 23%|██████████▋                                   | 17/73 [00:04<00:12,  4.41it/s, valid_loss=0.218, valid_f1=0.00012]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|███████████▎                                  | 18/73 [00:04<00:12,  4.42it/s, valid_loss=0.218, valid_f1=0.00012]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|███████████▌                                   | 18/73 [00:04<00:12,  4.42it/s, valid_loss=0.199, valid_f1=0.0032]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|████████████▏                                  | 19/73 [00:04<00:12,  4.47it/s, valid_loss=0.199, valid_f1=0.0032]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 26%|███████████▉                                  | 19/73 [00:04<00:12,  4.47it/s, valid_loss=0.194, valid_f1=0.00016]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|████████████▌                                 | 20/73 [00:04<00:11,  4.48it/s, valid_loss=0.194, valid_f1=0.00016]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|████████████▌                                 | 20/73 [00:04<00:11,  4.48it/s, valid_loss=0.181, valid_f1=0.00164]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|█████████████▏                                | 21/73 [00:04<00:11,  4.45it/s, valid_loss=0.181, valid_f1=0.00164]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 29%|█████████████▏                                | 21/73 [00:05<00:11,  4.45it/s, valid_loss=0.172, valid_f1=0.00134]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|█████████████▊                                | 22/73 [00:05<00:12,  4.22it/s, valid_loss=0.172, valid_f1=0.00134]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 30%|█████████████▊                                | 22/73 [00:05<00:12,  4.22it/s, valid_loss=0.165, valid_f1=5.81e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████▍                               | 23/73 [00:05<00:11,  4.20it/s, valid_loss=0.165, valid_f1=5.81e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 32%|██████████████▊                                | 23/73 [00:05<00:11,  4.20it/s, valid_loss=0.16, valid_f1=0.00139]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|███████████████▍                               | 24/73 [00:05<00:11,  4.13it/s, valid_loss=0.16, valid_f1=0.00139]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|██████████████▊                              | 24/73 [00:05<00:11,  4.13it/s, valid_loss=0.153, valid_f1=0.000889]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|███████████████▍                             | 25/73 [00:05<00:11,  4.02it/s, valid_loss=0.153, valid_f1=0.000889]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 34%|███████████████▍                             | 25/73 [00:06<00:11,  4.02it/s, valid_loss=0.146, valid_f1=0.000908]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|████████████████                             | 26/73 [00:06<00:11,  4.00it/s, valid_loss=0.146, valid_f1=0.000908]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 36%|████████████████▍                             | 26/73 [00:06<00:11,  4.00it/s, valid_loss=0.141, valid_f1=3.36e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|█████████████████                             | 27/73 [00:06<00:11,  3.93it/s, valid_loss=0.141, valid_f1=3.36e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 37%|█████████████████                             | 27/73 [00:06<00:11,  3.93it/s, valid_loss=0.132, valid_f1=0.00132]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|█████████████████▋                            | 28/73 [00:06<00:11,  3.92it/s, valid_loss=0.132, valid_f1=0.00132]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|█████████████████▋                            | 28/73 [00:06<00:11,  3.92it/s, valid_loss=0.13, valid_f1=0.000812]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|██████████████████▎                           | 29/73 [00:06<00:11,  3.94it/s, valid_loss=0.13, valid_f1=0.000812]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|██████████████████▎                           | 29/73 [00:07<00:11,  3.94it/s, valid_loss=0.126, valid_f1=2.71e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 41%|██████████████████▉                           | 30/73 [00:07<00:10,  4.04it/s, valid_loss=0.126, valid_f1=2.71e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████████████▎                           | 30/73 [00:07<00:10,  4.04it/s, valid_loss=0.122, valid_f1=0.0013]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|███████████████████▉                           | 31/73 [00:07<00:10,  3.95it/s, valid_loss=0.122, valid_f1=0.0013]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 42%|███████████████████▌                          | 31/73 [00:07<00:10,  3.95it/s, valid_loss=0.118, valid_f1=4.08e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|████████████████████▏                         | 32/73 [00:07<00:10,  3.96it/s, valid_loss=0.118, valid_f1=4.08e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 44%|████████████████████▏                         | 32/73 [00:07<00:10,  3.96it/s, valid_loss=0.112, valid_f1=0.00184]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████████████████████▊                         | 33/73 [00:07<00:09,  4.02it/s, valid_loss=0.112, valid_f1=0.00184]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 45%|████████████████████▊                         | 33/73 [00:08<00:09,  4.02it/s, valid_loss=0.112, valid_f1=5.41e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|█████████████████████▍                        | 34/73 [00:08<00:09,  4.00it/s, valid_loss=0.112, valid_f1=5.41e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|█████████████████████▍                        | 34/73 [00:08<00:09,  4.00it/s, valid_loss=0.107, valid_f1=1.54e-6]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|██████████████████████                        | 35/73 [00:08<00:09,  3.92it/s, valid_loss=0.107, valid_f1=1.54e-6]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 48%|█████████████████████▌                       | 35/73 [00:08<00:09,  3.92it/s, valid_loss=0.104, valid_f1=0.000617]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|██████████████████████▏                      | 36/73 [00:08<00:09,  3.92it/s, valid_loss=0.104, valid_f1=0.000617]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 49%|██████████████████████▋                       | 36/73 [00:08<00:09,  3.92it/s, valid_loss=0.102, valid_f1=1.67e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|███████████████████████▎                      | 37/73 [00:08<00:09,  3.93it/s, valid_loss=0.102, valid_f1=1.67e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 51%|██████████████████████▎                     | 37/73 [00:09<00:09,  3.93it/s, valid_loss=0.0968, valid_f1=0.000878]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▉                     | 38/73 [00:09<00:08,  3.98it/s, valid_loss=0.0968, valid_f1=0.000878]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 52%|██████████████████████▉                     | 38/73 [00:09<00:08,  3.98it/s, valid_loss=0.0956, valid_f1=0.000811]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|███████████████████████▌                    | 39/73 [00:09<00:08,  3.93it/s, valid_loss=0.0956, valid_f1=0.000811]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|███████████████████████▌                    | 39/73 [00:09<00:08,  3.93it/s, valid_loss=0.0936, valid_f1=0.000467]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|████████████████████████                    | 40/73 [00:09<00:08,  3.97it/s, valid_loss=0.0936, valid_f1=0.000467]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 55%|████████████████████████▋                    | 40/73 [00:09<00:08,  3.97it/s, valid_loss=0.0908, valid_f1=0.00112]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|█████████████████████████▎                   | 41/73 [00:09<00:08,  3.86it/s, valid_loss=0.0908, valid_f1=0.00112]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 56%|████████████████████████▋                   | 41/73 [00:10<00:08,  3.86it/s, valid_loss=0.0894, valid_f1=0.000707]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|█████████████████████████▎                  | 42/73 [00:10<00:07,  3.94it/s, valid_loss=0.0894, valid_f1=0.000707]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 58%|█████████████████████████▉                   | 42/73 [00:10<00:07,  3.94it/s, valid_loss=0.0858, valid_f1=0.00136]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|██████████████████████████▌                  | 43/73 [00:10<00:07,  3.95it/s, valid_loss=0.0858, valid_f1=0.00136]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 59%|█████████████████████████▉                  | 43/73 [00:10<00:07,  3.95it/s, valid_loss=0.0851, valid_f1=0.000621]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████████████████████████▌                 | 44/73 [00:10<00:07,  4.05it/s, valid_loss=0.0851, valid_f1=0.000621]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|███████████████████████████                  | 44/73 [00:10<00:07,  4.05it/s, valid_loss=0.082, valid_f1=0.000477]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|███████████████████████████▋                 | 45/73 [00:10<00:07,  3.93it/s, valid_loss=0.082, valid_f1=0.000477]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|███████████████████████████▋                 | 45/73 [00:11<00:07,  3.93it/s, valid_loss=0.0801, valid_f1=1.04e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|████████████████████████████▎                | 46/73 [00:11<00:06,  3.92it/s, valid_loss=0.0801, valid_f1=1.04e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 63%|███████████████████████████▋                | 46/73 [00:11<00:06,  3.92it/s, valid_loss=0.0793, valid_f1=0.000774]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|████████████████████████████▎               | 47/73 [00:11<00:06,  3.89it/s, valid_loss=0.0793, valid_f1=0.000774]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 64%|████████████████████████████▎               | 47/73 [00:11<00:06,  3.89it/s, valid_loss=0.0775, valid_f1=0.000774]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|████████████████████████████▉               | 48/73 [00:11<00:06,  3.78it/s, valid_loss=0.0775, valid_f1=0.000774]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 66%|████████████████████████████▉               | 48/73 [00:11<00:06,  3.78it/s, valid_loss=0.0764, valid_f1=0.000583]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|█████████████████████████████▌              | 49/73 [00:11<00:06,  3.92it/s, valid_loss=0.0764, valid_f1=0.000583]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|█████████████████████████████▌              | 49/73 [00:12<00:06,  3.92it/s, valid_loss=0.0741, valid_f1=0.000412]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████████████████████████████▏             | 50/73 [00:12<00:05,  3.93it/s, valid_loss=0.0741, valid_f1=0.000412]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 68%|██████████████████████████████▊              | 50/73 [00:12<00:05,  3.93it/s, valid_loss=0.0714, valid_f1=0.00088]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|███████████████████████████████▍             | 51/73 [00:12<00:05,  3.88it/s, valid_loss=0.0714, valid_f1=0.00088]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 70%|██████████████████████████████▋             | 51/73 [00:12<00:05,  3.88it/s, valid_loss=0.0696, valid_f1=0.000551]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████████████████████████████▎            | 52/73 [00:12<00:05,  3.89it/s, valid_loss=0.0696, valid_f1=0.000551]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 71%|███████████████████████████████▎            | 52/73 [00:13<00:05,  3.89it/s, valid_loss=0.0702, valid_f1=0.000296]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|███████████████████████████████▉            | 53/73 [00:13<00:05,  3.92it/s, valid_loss=0.0702, valid_f1=0.000296]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|████████████████████████████████▋            | 53/73 [00:13<00:05,  3.92it/s, valid_loss=0.0691, valid_f1=5.49e-6]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|█████████████████████████████████▎           | 54/73 [00:13<00:04,  3.91it/s, valid_loss=0.0691, valid_f1=5.49e-6]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 74%|██████████████████████████████████            | 54/73 [00:13<00:04,  3.91it/s, valid_loss=0.068, valid_f1=9.98e-8]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|██████████████████████████████████▋           | 55/73 [00:13<00:04,  3.98it/s, valid_loss=0.068, valid_f1=9.98e-8]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|█████████████████████████████████▉           | 55/73 [00:13<00:04,  3.98it/s, valid_loss=0.0674, valid_f1=1.78e-9]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|██████████████████████████████████▌          | 56/73 [00:13<00:04,  4.02it/s, valid_loss=0.0674, valid_f1=1.78e-9]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 77%|█████████████████████████████████▊          | 56/73 [00:14<00:04,  4.02it/s, valid_loss=0.0656, valid_f1=3.13e-11]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████████▎         | 57/73 [00:14<00:04,  3.95it/s, valid_loss=0.0656, valid_f1=3.13e-11]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 78%|██████████████████████████████████▎         | 57/73 [00:14<00:04,  3.95it/s, valid_loss=0.0633, valid_f1=5.39e-13]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|██████████████████████████████████▉         | 58/73 [00:14<00:03,  4.07it/s, valid_loss=0.0633, valid_f1=5.39e-13]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 79%|███████████████████████████████████▊         | 58/73 [00:14<00:03,  4.07it/s, valid_loss=0.062, valid_f1=0.000514]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|████████████████████████████████████▎        | 59/73 [00:14<00:03,  3.94it/s, valid_loss=0.062, valid_f1=0.000514]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 81%|███████████████████████████████████▌        | 59/73 [00:14<00:03,  3.94it/s, valid_loss=0.0618, valid_f1=0.000273]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████████████████████████████████▏       | 60/73 [00:14<00:03,  3.93it/s, valid_loss=0.0618, valid_f1=0.000273]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 82%|████████████████████████████████████▏       | 60/73 [00:15<00:03,  3.93it/s, valid_loss=0.0609, valid_f1=0.000551]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 84%|████████████████████████████████████▊       | 61/73 [00:15<00:03,  3.90it/s, valid_loss=0.0609, valid_f1=0.000551]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|███████████████████████████████████████▎       | 61/73 [00:15<00:03,  3.90it/s, valid_loss=0.0602, valid_f1=0.001]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|███████████████████████████████████████▉       | 62/73 [00:15<00:02,  3.96it/s, valid_loss=0.0602, valid_f1=0.001]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 85%|██████████████████████████████████████▏      | 62/73 [00:15<00:02,  3.96it/s, valid_loss=0.0592, valid_f1=1.59e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|██████████████████████████████████████▊      | 63/73 [00:15<00:02,  4.02it/s, valid_loss=0.0592, valid_f1=1.59e-5]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 86%|█████████████████████████████████████▉      | 63/73 [00:15<00:02,  4.02it/s, valid_loss=0.0577, valid_f1=0.000496]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|██████████████████████████████████████▌     | 64/73 [00:15<00:02,  3.98it/s, valid_loss=0.0577, valid_f1=0.000496]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|██████████████████████████████████████▌     | 64/73 [00:16<00:02,  3.98it/s, valid_loss=0.0558, valid_f1=0.000629]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|███████████████████████████████████████▏    | 65/73 [00:16<00:02,  3.93it/s, valid_loss=0.0558, valid_f1=0.000629]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 89%|████████████████████████████████████████     | 65/73 [00:16<00:02,  3.93it/s, valid_loss=0.0556, valid_f1=9.53e-6]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|████████████████████████████████████████▋    | 66/73 [00:16<00:01,  3.90it/s, valid_loss=0.0556, valid_f1=9.53e-6]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 90%|████████████████████████████████████████▋    | 66/73 [00:16<00:01,  3.90it/s, valid_loss=0.0544, valid_f1=1.42e-7]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|█████████████████████████████████████████▎   | 67/73 [00:16<00:01,  3.77it/s, valid_loss=0.0544, valid_f1=1.42e-7]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 92%|████████████████████████████████████████▍   | 67/73 [00:16<00:01,  3.77it/s, valid_loss=0.0539, valid_f1=0.000245]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|████████████████████████████████████████▉   | 68/73 [00:16<00:01,  3.79it/s, valid_loss=0.0539, valid_f1=0.000245]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|█████████████████████████████████████████▉   | 68/73 [00:17<00:01,  3.79it/s, valid_loss=0.053, valid_f1=0.000406]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|██████████████████████████████████████████▌  | 69/73 [00:17<00:01,  3.77it/s, valid_loss=0.053, valid_f1=0.000406]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 95%|██████████████████████████████████████████▌  | 69/73 [00:17<00:01,  3.77it/s, valid_loss=0.0517, valid_f1=0.00027]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|███████████████████████████████████████████▏ | 70/73 [00:17<00:00,  3.80it/s, valid_loss=0.0517, valid_f1=0.00027]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 96%|██████████████████████████████████████████▏ | 70/73 [00:17<00:00,  3.80it/s, valid_loss=0.0518, valid_f1=0.000473]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|██████████████████████████████████████████▊ | 71/73 [00:17<00:00,  3.88it/s, valid_loss=0.0518, valid_f1=0.000473]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 97%|████████████████████████████████████████████▋ | 71/73 [00:17<00:00,  3.88it/s, valid_loss=0.0518, valid_f1=0.0008]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 99%|█████████████████████████████████████████████▎| 72/73 [00:17<00:00,  3.90it/s, valid_loss=0.0518, valid_f1=0.0008]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████████████████████████████████████████| 73/73 [00:17<00:00,  4.07it/s, valid_loss=0.0491, valid_f1=1.1e-5]\u001b[A\u001b[A\u001b[A\n",
      "  0%|                                                                                          | 0/289 [00:18<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "run(train_loader, valid_loader, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd4f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac1cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee5772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c453e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
