{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f62904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import json \n",
    "import timm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9a9648",
   "metadata": {},
   "source": [
    "# Training Dataset 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b9eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 설명 csv 파일 참조\n",
    "crop = {'1':'딸기','2':'토마토','3':'파프리카','4':'오이','5':'고추','6':'시설포도'}\n",
    "disease = {'1':{'a1':'딸기잿빛곰팡이병','a2':'딸기흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '2':{'a5':'토마토흰가루병','a6':'토마토잿빛곰팡이병','b2':'열과','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '3':{'a9':'파프리카흰가루병','a10':'파프리카잘록병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '4':{'a3':'오이노균병','a4':'오이흰가루병','b1':'냉해피해','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '5':{'a7':'고추탄저병','a8':'고추흰가루병','b3':'칼슘결핍','b6':'다량원소결핍 (N)','b7':'다량원소결핍 (P)','b8':'다량원소결핍 (K)'},\n",
    "           '6':{'a11':'시설포도탄저병','a12':'시설포도노균병','b4':'일소피해','b5':'축과병'}}\n",
    "risk = {'1':'초기','2':'중기','3':'말기'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abad9847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1_00_0', '딸기_정상'),\n",
       " ('1_a1_1', '딸기_딸기잿빛곰팡이병_초기'),\n",
       " ('1_a1_2', '딸기_딸기잿빛곰팡이병_중기'),\n",
       " ('1_a1_3', '딸기_딸기잿빛곰팡이병_말기'),\n",
       " ('1_a2_1', '딸기_딸기흰가루병_초기'),\n",
       " ('1_a2_2', '딸기_딸기흰가루병_중기'),\n",
       " ('1_a2_3', '딸기_딸기흰가루병_말기'),\n",
       " ('1_b1_1', '딸기_냉해피해_초기'),\n",
       " ('1_b1_2', '딸기_냉해피해_중기'),\n",
       " ('1_b1_3', '딸기_냉해피해_말기')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_description = {}\n",
    "for key, value in disease.items():\n",
    "    label_description[f'{key}_00_0'] = f'{crop[key]}_정상'\n",
    "    for disease_code in value:\n",
    "        for risk_code in risk:\n",
    "            label = f'{key}_{disease_code}_{risk_code}'\n",
    "            label_description[label] = f'{crop[key]}_{disease[key][disease_code]}_{risk[risk_code]}'\n",
    "list(label_description.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aec6b6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 5767/5767 [00:00<00:00, 963725.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: '1_00_0',\n",
       " 1: '2_00_0',\n",
       " 2: '2_a5_2',\n",
       " 3: '3_00_0',\n",
       " 4: '3_a9_1',\n",
       " 5: '3_a9_2',\n",
       " 6: '3_a9_3',\n",
       " 7: '3_b3_1',\n",
       " 8: '3_b6_1',\n",
       " 9: '3_b7_1',\n",
       " 10: '3_b8_1',\n",
       " 11: '4_00_0',\n",
       " 12: '5_00_0',\n",
       " 13: '5_a7_2',\n",
       " 14: '5_b6_1',\n",
       " 15: '5_b7_1',\n",
       " 16: '5_b8_1',\n",
       " 17: '6_00_0',\n",
       " 18: '6_a11_1',\n",
       " 19: '6_a11_2',\n",
       " 20: '6_a12_1',\n",
       " 21: '6_a12_2',\n",
       " 22: '6_b4_1',\n",
       " 23: '6_b4_3',\n",
       " 24: '6_b5_1'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'1_00_0': 0,\n",
       " '2_00_0': 1,\n",
       " '2_a5_2': 2,\n",
       " '3_00_0': 3,\n",
       " '3_a9_1': 4,\n",
       " '3_a9_2': 5,\n",
       " '3_a9_3': 6,\n",
       " '3_b3_1': 7,\n",
       " '3_b6_1': 8,\n",
       " '3_b7_1': 9,\n",
       " '3_b8_1': 10,\n",
       " '4_00_0': 11,\n",
       " '5_00_0': 12,\n",
       " '5_a7_2': 13,\n",
       " '5_b6_1': 14,\n",
       " '5_b7_1': 15,\n",
       " '5_b8_1': 16,\n",
       " '6_00_0': 17,\n",
       " '6_a11_1': 18,\n",
       " '6_a11_2': 19,\n",
       " '6_a12_1': 20,\n",
       " '6_a12_2': 21,\n",
       " '6_b4_1': 22,\n",
       " '6_b4_3': 23,\n",
       " '6_b5_1': 24}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============= add\n",
    "labels = pd.read_csv('./data/train.csv')\n",
    "\n",
    "train_label_encoder = {}\n",
    "label_cnt = 0\n",
    "previous_label = '0_00_0'\n",
    "for i, label in enumerate(tqdm(sorted(labels['label']))) :\n",
    "    crop_val = label.split('_')[0] # crop\n",
    "    disease_val = label.split('_')[1] # disease\n",
    "    risk_val = label.split('_')[2] # risk\n",
    "    \n",
    "    tmp_label = f'{crop_val}_{disease_val}_{risk_val}'\n",
    "    if previous_label != tmp_label :\n",
    "        train_label_encoder[tmp_label] = label_cnt\n",
    "        previous_label = tmp_label\n",
    "        label_cnt += 1\n",
    "        \n",
    "train_label_decoder = {val : key for key, val in train_label_encoder.items()}\n",
    "display(train_label_decoder)\n",
    "display(train_label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b77ffb",
   "metadata": {},
   "source": [
    "# Custom Dataset 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98645b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, files, transform_224, transform_288, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.files = files\n",
    "        self.label_encoder = train_label_encoder #label_encoder\n",
    "        \n",
    "        self.transform_224 = transform_224\n",
    "        self.transform_288 = transform_288\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        file = self.files[i]\n",
    "        file_name = file.split('\\\\')[-1]\n",
    "        \n",
    "        \n",
    "        # image\n",
    "        image_path = f'{file}/{file_name}.jpg'\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "#         img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
    "#         img = img.astype(np.float32)/255\n",
    "\n",
    "        img_224 = self.transform_224(image=img)['image']\n",
    "        img_288 = self.transform_288(image=img)['image']\n",
    "        \n",
    "        img_224 = img_224.transpose(2, 0, 1)\n",
    "        img_288 = img_288.transpose(2, 0, 1)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            json_path = f'{file}/{file_name}.json'\n",
    "            with open(json_path, 'r') as f:\n",
    "                json_file = json.load(f)\n",
    "            \n",
    "            crop = json_file['annotations']['crop']\n",
    "            disease = json_file['annotations']['disease']\n",
    "            risk = json_file['annotations']['risk']\n",
    "            label = f'{crop}_{disease}_{risk}'\n",
    "            \n",
    "            return {\n",
    "#                 'img' : torch.tensor(img, dtype=torch.float32),\n",
    "                'effi' : torch.tensor(img_288, dtype=torch.float32) / 255.0,\n",
    "                'deit' : torch.tensor(img_224, dtype=torch.float32) / 255.0,\n",
    "                'label' : torch.tensor(self.label_encoder[label], dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'effi' : torch.tensor(img_288, dtype=torch.float32) / 255.0,\n",
    "                'deit' : torch.tensor(img_224, dtype=torch.float32) / 255.0\n",
    "#                 'img' : torch.tensor(img, dtype=torch.float32)\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2733fdb9",
   "metadata": {},
   "source": [
    "# Model 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ec2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class DeiT(nn.Module):\n",
    "    def __init__(self, model_name, n_classes):\n",
    "        super(DeiT, self).__init__()\n",
    "        self.model = timm.create_model(model_name, num_classes=n_classes, pretrained=True)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        output = self.model(inputs)\n",
    "        return output\n",
    "    \n",
    "class EffiV2S(nn.Module):\n",
    "    def __init__(self, model_name, n_classes):\n",
    "        super(EffiV2S, self).__init__()\n",
    "        self.model = timm.create_model(model_name, num_classes=n_classes, pretrained=True)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        output = self.model(inputs)\n",
    "        return output\n",
    "    \n",
    "class total_model(nn.Module):\n",
    "    def __init__(self, model_name_1, model_name_2, n_classes) :\n",
    "        super(total_model, self).__init__()\n",
    "        self.effi = EffiV2S(model_name_1, n_classes)\n",
    "        self.DeiT = DeiT(model_name_2, n_classes)\n",
    "        \n",
    "    def forward(self, input_224, input_288) : \n",
    "        output1 = self.effi(input_288)\n",
    "        output2 = self.DeiT(input_224)\n",
    "        \n",
    "        output = (output1 + output2) / 2\n",
    "        return output\n",
    "    \n",
    "\n",
    "class EffiDeiT(nn.Module):\n",
    "    def __init__(self, model_name_1, model_name_2, n_classes, test=False) :\n",
    "        super(EffiDeiT, self).__init__()\n",
    "        self.total_model = total_model(model_name_1, model_name_2, 38)\n",
    "        \n",
    "        if test == False :\n",
    "            self.total_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "            \n",
    "        self.fc = nn.Linear(38, n_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_224, input_288) :      \n",
    "        output = self.total_model(input_224, input_288)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57835e49",
   "metadata": {},
   "source": [
    "# Label 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f017bc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 5767/5767 [00:01<00:00, 3001.72it/s]\n"
     ]
    }
   ],
   "source": [
    "json_path = glob('./data/train/*/*.json')\n",
    "\n",
    "labels = []\n",
    "for path in tqdm(json_path) :\n",
    "    json_file = json.load(open(path, 'r'))\n",
    "    \n",
    "    crop = json_file['annotations']['crop']\n",
    "    disease = json_file['annotations']['disease']\n",
    "    risk = json_file['annotations']['risk']\n",
    "    \n",
    "    label = f'{crop}_{disease}_{risk}'\n",
    "    labels.append(train_label_encoder[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3bb6b6",
   "metadata": {},
   "source": [
    "# transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "937bfc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms_224 = A.Compose([\n",
    "                A.Resize(224 ,224),\n",
    "                A.OneOf([\n",
    "                    A.Rotate(),\n",
    "                    A.HorizontalFlip(),\n",
    "                    A.VerticalFlip()\n",
    "                ], p=1)\n",
    "            ])\n",
    "\n",
    "train_transforms_288 = A.Compose([\n",
    "                A.Resize(288 ,288),\n",
    "                A.OneOf([\n",
    "                    A.Rotate(),\n",
    "                    A.HorizontalFlip(),\n",
    "                    A.VerticalFlip()\n",
    "                ], p=1)\n",
    "            ])\n",
    "\n",
    "val_transforms_224 = A.Compose([\n",
    "    A.Resize(224,224)\n",
    "])\n",
    "val_transforms_288 = A.Compose([\n",
    "    A.Resize(288,288)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f028e51",
   "metadata": {},
   "source": [
    "# Cutmix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb58391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    " \n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f411aa7",
   "metadata": {},
   "source": [
    "# Training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "990da9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_function(real, pred):    \n",
    "    real = real.cpu()\n",
    "    pred = torch.argmax(pred, dim=1).cpu()\n",
    "    score = f1_score(real, pred, average='macro')\n",
    "    return score\n",
    "\n",
    "def train_step(batch_item, training):\n",
    "    img_effi = batch_item['effi'].to(device) # 288\n",
    "    img_deit = batch_item['deit'].to(device) # 224\n",
    "    label = batch_item['label'].to(device)\n",
    "\n",
    "    lam = np.random.beta(1.0, 1.0)\n",
    "    \n",
    "    if training is True:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # add - cutmix\n",
    "            rand_index = torch.randperm(img_deit.size()[0])\n",
    "            target_a = label\n",
    "            target_b = label[rand_index]\n",
    "            \n",
    "            # 224 size 기준으로 이미지 crop\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(img_deit.size(), lam)\n",
    "            \n",
    "            img_deit[:, :, bbx1:bbx2, bby1:bby2] = img_deit[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            img_effi[:, :, bbx1:bbx2, bby1:bby2] = img_effi[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            \n",
    "            # lam 값은 공유\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (img_deit.size()[-1] * img_deit.size()[-2]))\n",
    "            \n",
    "            output = model(img_deit, img_effi)\n",
    "            loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1. - lam)\n",
    "#             output = model(img, csv_feature)\n",
    "#             loss = criterion(output, label) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        score = accuracy_function(label, output)\n",
    "        return loss, score\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(img_deit, img_effi)\n",
    "            loss = criterion(output, label)\n",
    "        score = accuracy_function(label, output)\n",
    "        return loss, score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d24dc9",
   "metadata": {},
   "source": [
    "# HyperParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "126f56e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")#(\"cpu\")\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "epochs = 15\n",
    "\n",
    "model_name_1 = 'efficientnetv2_rw_s'\n",
    "model_name_2 = 'deit_small_patch16_224'\n",
    "n_classes = 25\n",
    "\n",
    "model_path = './model/public_vill_50k_pretrain_EffiDeit.pt'\n",
    "save_path = 'public_vill_50k_pretrain_EffiDeit.pt'\n",
    "\n",
    "early_stopping_cnt = 4\n",
    "fold_n = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ef02b",
   "metadata": {},
   "source": [
    "# Call model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fdc95ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EffiDeiT(model_name_1, model_name_2, n_classes)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0657fbca",
   "metadata": {},
   "source": [
    "# k fold Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63d64b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = glob('./data/train/*')\n",
    "label_list = labels\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=fold_n, random_state=13, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e96a81af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "===== k_fold : 1 / 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [02:25,  3.71it/s, Epoch=1, Loss=1.262334, Mean Loss=1.483260, Mean F-1=0.484079]\n",
      "181it [00:20,  8.97it/s, Epoch=1, Val Loss=0.534476, Mean Val Loss=0.371048, Mean Val F-1=0.779719]\n",
      "541it [02:21,  3.83it/s, Epoch=2, Loss=0.374705, Mean Loss=0.984043, Mean F-1=0.643270]\n",
      "181it [00:20,  8.82it/s, Epoch=2, Val Loss=0.369761, Mean Val Loss=0.215696, Mean Val F-1=0.856405]\n",
      "541it [02:25,  3.71it/s, Epoch=3, Loss=1.104398, Mean Loss=0.867648, Mean F-1=0.652309]\n",
      "181it [00:22,  8.14it/s, Epoch=3, Val Loss=0.659879, Mean Val Loss=0.194741, Mean Val F-1=0.875375]\n",
      "541it [02:36,  3.47it/s, Epoch=4, Loss=0.758780, Mean Loss=0.789214, Mean F-1=0.694373]\n",
      "181it [00:23,  7.75it/s, Epoch=4, Val Loss=0.295669, Mean Val Loss=0.127029, Mean Val F-1=0.921111]\n",
      "541it [02:51,  3.15it/s, Epoch=5, Loss=1.186151, Mean Loss=0.721706, Mean F-1=0.731193]\n",
      "181it [00:26,  6.75it/s, Epoch=5, Val Loss=0.490410, Mean Val Loss=0.129279, Mean Val F-1=0.909031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [02:56,  3.06it/s, Epoch=6, Loss=0.413052, Mean Loss=0.671426, Mean F-1=0.749833]\n",
      "181it [00:27,  6.55it/s, Epoch=6, Val Loss=0.275321, Mean Val Loss=0.112037, Mean Val F-1=0.924627]\n",
      "541it [03:01,  2.97it/s, Epoch=7, Loss=0.486192, Mean Loss=0.658108, Mean F-1=0.747335]\n",
      "181it [00:29,  6.17it/s, Epoch=7, Val Loss=0.264688, Mean Val Loss=0.111929, Mean Val F-1=0.927304]\n",
      "541it [03:07,  2.88it/s, Epoch=8, Loss=0.934950, Mean Loss=0.628171, Mean F-1=0.772970]\n",
      "181it [00:30,  5.94it/s, Epoch=8, Val Loss=0.073486, Mean Val Loss=0.121832, Mean Val F-1=0.915695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [03:13,  2.80it/s, Epoch=9, Loss=0.519669, Mean Loss=0.626554, Mean F-1=0.764980]\n",
      "181it [00:32,  5.60it/s, Epoch=9, Val Loss=0.200498, Mean Val Loss=0.100779, Mean Val F-1=0.928080]\n",
      "541it [03:21,  2.69it/s, Epoch=10, Loss=0.675605, Mean Loss=0.605398, Mean F-1=0.762400]\n",
      "181it [00:32,  5.61it/s, Epoch=10, Val Loss=0.103660, Mean Val Loss=0.106921, Mean Val F-1=0.927504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [03:29,  2.58it/s, Epoch=11, Loss=0.729499, Mean Loss=0.576031, Mean F-1=0.782970]\n",
      "181it [00:37,  4.83it/s, Epoch=11, Val Loss=0.006523, Mean Val Loss=0.100206, Mean Val F-1=0.936547]\n",
      "541it [03:34,  2.53it/s, Epoch=12, Loss=0.244528, Mean Loss=0.604836, Mean F-1=0.765667]\n",
      "181it [00:39,  4.64it/s, Epoch=12, Val Loss=0.198404, Mean Val Loss=0.111016, Mean Val F-1=0.931671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [03:44,  2.41it/s, Epoch=13, Loss=0.015268, Mean Loss=0.579740, Mean F-1=0.788381]\n",
      "181it [00:42,  4.23it/s, Epoch=13, Val Loss=0.019683, Mean Val Loss=0.175373, Mean Val F-1=0.919309]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [03:52,  2.32it/s, Epoch=14, Loss=0.324875, Mean Loss=0.573758, Mean F-1=0.777259]\n",
      "181it [00:44,  4.10it/s, Epoch=14, Val Loss=0.051260, Mean Val Loss=0.119307, Mean Val F-1=0.933193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [3 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [03:54,  2.31it/s, Epoch=15, Loss=0.374604, Mean Loss=0.564092, Mean F-1=0.782154]\n",
      "181it [00:47,  3.85it/s, Epoch=15, Val Loss=0.056598, Mean Val Loss=0.126276, Mean Val F-1=0.935431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [4 / 4\n",
      "== Early Stop ==\n",
      "\n",
      "\n",
      "\n",
      "===== k_fold : 2 / 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [02:53,  3.11it/s, Epoch=1, Loss=1.233305, Mean Loss=1.470769, Mean F-1=0.487548]\n",
      "181it [00:25,  6.99it/s, Epoch=1, Val Loss=0.048926, Mean Val Loss=0.358544, Mean Val F-1=0.785379]\n",
      "541it [02:50,  3.18it/s, Epoch=2, Loss=0.350313, Mean Loss=0.990728, Mean F-1=0.635618]\n",
      "181it [00:25,  7.06it/s, Epoch=2, Val Loss=0.039179, Mean Val Loss=0.249132, Mean Val F-1=0.838644]\n",
      "541it [02:52,  3.13it/s, Epoch=3, Loss=1.293963, Mean Loss=0.819541, Mean F-1=0.693467]\n",
      "181it [00:26,  6.72it/s, Epoch=3, Val Loss=0.005963, Mean Val Loss=0.180652, Mean Val F-1=0.875679]\n",
      "541it [02:56,  3.06it/s, Epoch=4, Loss=0.478967, Mean Loss=0.774001, Mean F-1=0.707493]\n",
      "181it [00:24,  7.47it/s, Epoch=4, Val Loss=0.009182, Mean Val Loss=0.161686, Mean Val F-1=0.880047]\n",
      "541it [02:41,  3.35it/s, Epoch=5, Loss=0.820817, Mean Loss=0.704911, Mean F-1=0.733950]\n",
      "181it [00:24,  7.49it/s, Epoch=5, Val Loss=0.002666, Mean Val Loss=0.123866, Mean Val F-1=0.900634]\n",
      "541it [02:41,  3.35it/s, Epoch=6, Loss=0.514521, Mean Loss=0.684835, Mean F-1=0.737979]\n",
      "181it [00:25,  7.07it/s, Epoch=6, Val Loss=0.003891, Mean Val Loss=0.153062, Mean Val F-1=0.893240]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [02:46,  3.25it/s, Epoch=7, Loss=0.490658, Mean Loss=0.670987, Mean F-1=0.735606]\n",
      "181it [00:26,  6.74it/s, Epoch=7, Val Loss=0.009444, Mean Val Loss=0.128065, Mean Val F-1=0.895962]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [02:51,  3.15it/s, Epoch=8, Loss=0.644825, Mean Loss=0.645003, Mean F-1=0.771374]\n",
      "181it [00:28,  6.37it/s, Epoch=8, Val Loss=0.001613, Mean Val Loss=0.145521, Mean Val F-1=0.902049]\n",
      "541it [02:56,  3.06it/s, Epoch=9, Loss=0.700701, Mean Loss=0.597609, Mean F-1=0.783557]\n",
      "181it [00:29,  6.05it/s, Epoch=9, Val Loss=0.031579, Mean Val Loss=0.124879, Mean Val F-1=0.901232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [03:01,  2.97it/s, Epoch=10, Loss=0.450081, Mean Loss=0.614080, Mean F-1=0.759407]\n",
      "181it [00:34,  5.26it/s, Epoch=10, Val Loss=0.008846, Mean Val Loss=0.138348, Mean Val F-1=0.914851]\n",
      "541it [03:07,  2.89it/s, Epoch=11, Loss=1.243132, Mean Loss=0.600131, Mean F-1=0.780875]\n",
      "181it [00:36,  4.90it/s, Epoch=11, Val Loss=0.006395, Mean Val Loss=0.108517, Mean Val F-1=0.926798]\n",
      "541it [03:13,  2.79it/s, Epoch=12, Loss=0.504172, Mean Loss=0.594912, Mean F-1=0.771280]\n",
      "181it [00:38,  4.70it/s, Epoch=12, Val Loss=0.002203, Mean Val Loss=0.149165, Mean Val F-1=0.933560]\n",
      "541it [03:19,  2.72it/s, Epoch=13, Loss=0.583256, Mean Loss=0.588367, Mean F-1=0.775559]\n",
      "181it [00:42,  4.24it/s, Epoch=13, Val Loss=0.004583, Mean Val Loss=0.151618, Mean Val F-1=0.909546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [03:25,  2.63it/s, Epoch=14, Loss=0.462105, Mean Loss=0.562251, Mean F-1=0.789381]\n",
      "181it [00:42,  4.25it/s, Epoch=14, Val Loss=0.002372, Mean Val Loss=0.123883, Mean Val F-1=0.923813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [03:29,  2.58it/s, Epoch=15, Loss=0.660171, Mean Loss=0.559112, Mean F-1=0.793107]\n",
      "181it [00:44,  4.11it/s, Epoch=15, Val Loss=0.022026, Mean Val Loss=0.178286, Mean Val F-1=0.904915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [3 / 4\n",
      "\n",
      "\n",
      "\n",
      "===== k_fold : 3 / 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [02:29,  3.62it/s, Epoch=1, Loss=0.652018, Mean Loss=1.433800, Mean F-1=0.506464]\n",
      "181it [00:22,  8.17it/s, Epoch=1, Val Loss=0.012445, Mean Val Loss=0.393662, Mean Val F-1=0.766572]\n",
      "541it [02:27,  3.67it/s, Epoch=2, Loss=0.216567, Mean Loss=0.975432, Mean F-1=0.643431]\n",
      "181it [00:21,  8.30it/s, Epoch=2, Val Loss=0.009252, Mean Val Loss=0.305133, Mean Val F-1=0.829625]\n",
      "541it [02:27,  3.66it/s, Epoch=3, Loss=0.645693, Mean Loss=0.830357, Mean F-1=0.675422]\n",
      "181it [00:21,  8.34it/s, Epoch=3, Val Loss=0.014635, Mean Val Loss=1.883229, Mean Val F-1=0.836898] \n",
      "541it [02:29,  3.62it/s, Epoch=4, Loss=0.874368, Mean Loss=0.743409, Mean F-1=0.728040]\n",
      "181it [00:22,  7.93it/s, Epoch=4, Val Loss=0.017069, Mean Val Loss=0.186955, Mean Val F-1=0.870965]\n",
      "541it [02:35,  3.49it/s, Epoch=5, Loss=0.616026, Mean Loss=0.723690, Mean F-1=0.729123]\n",
      "181it [00:24,  7.42it/s, Epoch=5, Val Loss=0.006131, Mean Val Loss=5.327231, Mean Val F-1=0.860966]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [02:41,  3.34it/s, Epoch=6, Loss=0.699418, Mean Loss=0.683976, Mean F-1=0.743930]\n",
      "181it [00:25,  7.01it/s, Epoch=6, Val Loss=0.016438, Mean Val Loss=0.400601, Mean Val F-1=0.890412] \n",
      "541it [02:50,  3.18it/s, Epoch=7, Loss=0.819117, Mean Loss=0.641112, Mean F-1=0.771144]\n",
      "181it [00:27,  6.51it/s, Epoch=7, Val Loss=0.001976, Mean Val Loss=24.416039, Mean Val F-1=0.884098]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [02:56,  3.07it/s, Epoch=8, Loss=0.492369, Mean Loss=0.614641, Mean F-1=0.746468]\n",
      "181it [00:29,  6.17it/s, Epoch=8, Val Loss=0.003149, Mean Val Loss=1.735569, Mean Val F-1=0.898817]  \n",
      "541it [03:03,  2.95it/s, Epoch=9, Loss=0.347127, Mean Loss=0.617854, Mean F-1=0.779960]\n",
      "181it [00:36,  4.92it/s, Epoch=9, Val Loss=0.001965, Mean Val Loss=0.414169, Mean Val F-1=0.886996] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [03:08,  2.87it/s, Epoch=10, Loss=0.324988, Mean Loss=0.593906, Mean F-1=0.774721]\n",
      "181it [00:38,  4.75it/s, Epoch=10, Val Loss=0.002819, Mean Val Loss=0.196033, Mean Val F-1=0.902410]\n",
      "541it [03:14,  2.79it/s, Epoch=11, Loss=0.647460, Mean Loss=0.594664, Mean F-1=0.776433]\n",
      "181it [00:38,  4.69it/s, Epoch=11, Val Loss=0.005537, Mean Val Loss=0.333523, Mean Val F-1=0.905754] \n",
      "541it [03:19,  2.71it/s, Epoch=12, Loss=0.279971, Mean Loss=0.571283, Mean F-1=0.797479]\n",
      "181it [00:40,  4.47it/s, Epoch=12, Val Loss=0.003217, Mean Val Loss=1.960877, Mean Val F-1=0.890987] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [03:25,  2.64it/s, Epoch=13, Loss=0.800833, Mean Loss=0.574390, Mean F-1=0.773570]\n",
      "181it [00:41,  4.32it/s, Epoch=13, Val Loss=0.001056, Mean Val Loss=0.357819, Mean Val F-1=0.918859]\n",
      "541it [03:31,  2.55it/s, Epoch=14, Loss=0.125304, Mean Loss=0.565356, Mean F-1=0.773215]\n",
      "181it [00:44,  4.08it/s, Epoch=14, Val Loss=0.003620, Mean Val Loss=3.182777, Mean Val F-1=0.901883]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [03:42,  2.44it/s, Epoch=15, Loss=0.079399, Mean Loss=0.550638, Mean F-1=0.794100]\n",
      "181it [00:46,  3.91it/s, Epoch=15, Val Loss=0.001704, Mean Val Loss=3.398019, Mean Val F-1=0.899875]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 4\n",
      "\n",
      "\n",
      "\n",
      "===== k_fold : 4 / 4 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [02:54,  3.10it/s, Epoch=1, Loss=1.395277, Mean Loss=1.507653, Mean F-1=0.486589]\n",
      "181it [00:27,  6.50it/s, Epoch=1, Val Loss=0.814030, Mean Val Loss=1.343399, Mean Val F-1=0.776442] \n",
      "541it [02:33,  3.52it/s, Epoch=2, Loss=0.597250, Mean Loss=0.973679, Mean F-1=0.627546]\n",
      "181it [00:23,  7.81it/s, Epoch=2, Val Loss=0.904697, Mean Val Loss=0.335133, Mean Val F-1=0.808348]\n",
      "541it [02:32,  3.54it/s, Epoch=3, Loss=0.935366, Mean Loss=0.839930, Mean F-1=0.684141]\n",
      "181it [00:23,  7.84it/s, Epoch=3, Val Loss=0.323997, Mean Val Loss=0.168485, Mean Val F-1=0.886911]\n",
      "541it [02:32,  3.54it/s, Epoch=4, Loss=0.871123, Mean Loss=0.771637, Mean F-1=0.693241]\n",
      "181it [00:23,  7.65it/s, Epoch=4, Val Loss=0.103376, Mean Val Loss=0.154185, Mean Val F-1=0.907905]\n",
      "541it [02:38,  3.41it/s, Epoch=5, Loss=0.795734, Mean Loss=0.698122, Mean F-1=0.734639]\n",
      "181it [00:25,  7.13it/s, Epoch=5, Val Loss=0.055574, Mean Val Loss=0.105056, Mean Val F-1=0.930435]\n",
      "541it [02:45,  3.27it/s, Epoch=6, Loss=0.508657, Mean Loss=0.680439, Mean F-1=0.747957]\n",
      "181it [00:27,  6.58it/s, Epoch=6, Val Loss=0.037153, Mean Val Loss=0.121313, Mean Val F-1=0.919968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [02:54,  3.11it/s, Epoch=7, Loss=0.662048, Mean Loss=0.654567, Mean F-1=0.732482]\n",
      "181it [00:29,  6.23it/s, Epoch=7, Val Loss=0.000184, Mean Val Loss=0.107817, Mean Val F-1=0.929118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [03:01,  2.98it/s, Epoch=8, Loss=0.255549, Mean Loss=0.625723, Mean F-1=0.765249]\n",
      "181it [00:32,  5.51it/s, Epoch=8, Val Loss=0.004176, Mean Val Loss=0.090530, Mean Val F-1=0.945685]\n",
      "541it [03:08,  2.87it/s, Epoch=9, Loss=1.033882, Mean Loss=0.627510, Mean F-1=0.743888]\n",
      "181it [00:37,  4.89it/s, Epoch=9, Val Loss=0.002754, Mean Val Loss=0.114787, Mean Val F-1=0.922545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [03:14,  2.77it/s, Epoch=10, Loss=0.868999, Mean Loss=0.624367, Mean F-1=0.772972]\n",
      "181it [00:38,  4.65it/s, Epoch=10, Val Loss=0.013982, Mean Val Loss=0.105603, Mean Val F-1=0.927604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [2 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [03:21,  2.68it/s, Epoch=11, Loss=0.484558, Mean Loss=0.589643, Mean F-1=0.782846]\n",
      "181it [00:41,  4.31it/s, Epoch=11, Val Loss=0.018749, Mean Val Loss=0.226079, Mean Val F-1=0.916993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [3 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "541it [03:28,  2.60it/s, Epoch=12, Loss=0.341924, Mean Loss=0.577737, Mean F-1=0.771209]\n",
      "181it [00:44,  4.10it/s, Epoch=12, Val Loss=0.004625, Mean Val Loss=0.521098, Mean Val F-1=0.925555]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopping Step : [4 / 4\n",
      "== Early Stop ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k, (fold_train, fold_val) in enumerate(kfold.split(data_list, label_list), 1) :\n",
    "    model = EffiDeiT(model_name_1, model_name_2, n_classes)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_data_list = []\n",
    "    val_data_list = []\n",
    "    \n",
    "    for k_train in fold_train :\n",
    "        train_data_list.append(data_list[k_train])\n",
    "    \n",
    "    for k_val in fold_val :\n",
    "        val_data_list.append(data_list[k_val])\n",
    "    \n",
    "    print(f\"\\n\\n\\n===== k_fold : {k} / {fold_n} =====\")\n",
    "    train_dataset = CustomDataset(train_data_list, train_transforms_224, train_transforms_288)\n",
    "    val_dataset = CustomDataset(val_data_list, val_transforms_224, val_transforms_288)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    loss_plot, val_loss_plot = [], []\n",
    "    metric_plot, val_metric_plot = [], []\n",
    "    \n",
    "    early_stopping = 0\n",
    "    for epoch in range(epochs):\n",
    "        total_loss, total_val_loss = 0, 0\n",
    "        total_acc, total_val_acc = 0, 0\n",
    "\n",
    "        tqdm_dataset = tqdm(enumerate(train_dataloader))\n",
    "        training = True\n",
    "        for batch, batch_item in tqdm_dataset:\n",
    "            batch_loss, batch_acc = train_step(batch_item, training)\n",
    "            total_loss += batch_loss\n",
    "            total_acc += batch_acc\n",
    "\n",
    "            tqdm_dataset.set_postfix({\n",
    "                'Epoch': epoch + 1,\n",
    "                'Loss': '{:06f}'.format(batch_loss.item()),\n",
    "                'Mean Loss' : '{:06f}'.format(total_loss/(batch+1)),\n",
    "                'Mean F-1' : '{:06f}'.format(total_acc/(batch+1))\n",
    "            })\n",
    "        loss_plot.append(total_loss/(batch+1))\n",
    "        metric_plot.append(total_acc/(batch+1))\n",
    "\n",
    "        tqdm_dataset = tqdm(enumerate(val_dataloader))\n",
    "        training = False\n",
    "        for batch, batch_item in tqdm_dataset:\n",
    "            batch_loss, batch_acc = train_step(batch_item, training)\n",
    "            total_val_loss += batch_loss\n",
    "            total_val_acc += batch_acc\n",
    "\n",
    "            tqdm_dataset.set_postfix({\n",
    "                'Epoch': epoch + 1,\n",
    "                'Val Loss': '{:06f}'.format(batch_loss.item()),\n",
    "                'Mean Val Loss' : '{:06f}'.format(total_val_loss/(batch+1)),\n",
    "                'Mean Val F-1' : '{:06f}'.format(total_val_acc/(batch+1))\n",
    "            })\n",
    "        val_loss_plot.append(total_val_loss/(batch+1))\n",
    "        val_metric_plot.append(total_val_acc/(batch+1))\n",
    "        \n",
    "        # scheduler\n",
    "#         scheduler.step()\n",
    "        \n",
    "        if np.max(val_metric_plot) == val_metric_plot[-1]:\n",
    "            torch.save(model.state_dict(), f'{k}_{save_path}')\n",
    "            early_stopping = 0\n",
    "    \n",
    "        elif np.max(val_metric_plot) > val_metric_plot[-1]: \n",
    "            early_stopping += 1\n",
    "            print(f\"Early Stopping Step : [{early_stopping} / {early_stopping_cnt}]\")\n",
    "\n",
    "        if early_stopping == early_stopping_cnt :\n",
    "            print(\"== Early Stop ==\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2e252c",
   "metadata": {},
   "source": [
    "# Test Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36871e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_transforms_224 = A.Compose([\n",
    "                A.Resize(224 ,224),\n",
    "                A.OneOf([\n",
    "                    A.Rotate(),\n",
    "                    A.HorizontalFlip(),\n",
    "                    A.VerticalFlip()\n",
    "                ], p=1)\n",
    "            ])\n",
    "\n",
    "tta_transforms_288 = A.Compose([\n",
    "                A.Resize(288 ,288),\n",
    "                A.OneOf([\n",
    "                    A.Rotate(),\n",
    "                    A.HorizontalFlip(),\n",
    "                    A.VerticalFlip()\n",
    "                ], p=1)\n",
    "            ])\n",
    "\n",
    "\n",
    "test = sorted(glob('data/test/*'))\n",
    "test_dataset = CustomDataset(test, tta_transforms_224, tta_transforms_288, mode='test')\n",
    "\n",
    "\n",
    "# test_dataset = CustomDataset(test, val_transforms, mode = 'test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1e1c9d",
   "metadata": {},
   "source": [
    "# Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8d0ca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def softvoting(models, img_deit, img_effi, n_classes=25) :\n",
    "\n",
    "    predicts = torch.zeros(img_deit.size(0), n_classes)\n",
    "    with torch.no_grad() :\n",
    "        for model in models :\n",
    "            output = model(img_deit, img_effi)\n",
    "            output = F.softmax(output.cpu(), dim=1)\n",
    "            \n",
    "            predicts += output\n",
    "\n",
    "    # 둘다 값은 똑같이 나옴.\n",
    "    # pred_avg = predicts / len(models)\n",
    "    # answer = pred_avg.argmax(dim=-1)\n",
    "    # _, answer2 = torch.max(pred_avg, 1)\n",
    "\n",
    "    return predicts.detach().cpu() / len(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae1bfcc",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53314cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6489it [29:53,  3.62it/s]\n"
     ]
    }
   ],
   "source": [
    "def predict(dataset, models) :\n",
    "    tqdm_dataset = tqdm(enumerate(dataset))\n",
    "    results = []\n",
    "    for batch, batch_item in tqdm_dataset :\n",
    "        img_effi = batch_item['effi'].to(device) # 288\n",
    "        img_deit = batch_item['deit'].to(device) # 224\n",
    "\n",
    "    \n",
    "#         print(img.shape)\n",
    "        predictions = softvoting(models, img_deit, img_effi)\n",
    "        batch_result = [int(torch.argmax(prediction)) for prediction in predictions]\n",
    "#         print(batch_result)\n",
    "#         for prediction in predictions :\n",
    "            \n",
    "#         results.append(int(torch.argmax(predictions[0])))\n",
    "#             output = model(img)\n",
    "#         output = torch.tensor(torch.argmax(output, dim=1), dtype=torch.int32).cpu().numpy()\n",
    "        results.extend(batch_result)\n",
    "    return results\n",
    "\n",
    "\n",
    "model_name_1 = 'efficientnetv2_rw_s'\n",
    "model_name_2 = 'deit_small_patch16_224'\n",
    "n_classes = 25\n",
    "\n",
    "kfold_models_path = glob('./model/k_public_vill_50k_pretrain_EffiDeit/*.pt')\n",
    "models = []\n",
    "for kfold_model_path in kfold_models_path :\n",
    "    model = EffiDeiT(model_name_1, model_name_2, n_classes=25)\n",
    "    model.load_state_dict(torch.load(kfold_model_path, map_location=device))\n",
    "    model.to(device).eval()\n",
    "    models.append(model)\n",
    "    \n",
    "preds = predict(test_dataloader, models)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0af804a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cp = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "799edbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cp = np.array([train_label_decoder[int(val)] for val in preds_cp])\n",
    "submission_csv = pd.read_csv('./data/sample_submission.csv')\n",
    "submission_csv['label'] = preds_cp\n",
    "submission_csv.to_csv('./data/k_tta_public_vill_50k_pretrain_EffiDeit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cafcca",
   "metadata": {},
   "source": [
    "# 단일 모델 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fda9921e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EffiDeiT(\n",
       "  (total_model): total_model(\n",
       "    (effi): EffiV2S(\n",
       "      (model): EfficientNet(\n",
       "        (conv_stem): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (blocks): Sequential(\n",
       "          (0): Sequential(\n",
       "            (0): EdgeResidual(\n",
       "              (conv_exp): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (se): Identity()\n",
       "              (conv_pwl): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): EdgeResidual(\n",
       "              (conv_exp): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (se): Identity()\n",
       "              (conv_pwl): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): EdgeResidual(\n",
       "              (conv_exp): Conv2d(24, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (se): Identity()\n",
       "              (conv_pwl): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): EdgeResidual(\n",
       "              (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (se): Identity()\n",
       "              (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (2): EdgeResidual(\n",
       "              (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (se): Identity()\n",
       "              (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (3): EdgeResidual(\n",
       "              (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (se): Identity()\n",
       "              (conv_pwl): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): EdgeResidual(\n",
       "              (conv_exp): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (se): Identity()\n",
       "              (conv_pwl): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): EdgeResidual(\n",
       "              (conv_exp): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (se): Identity()\n",
       "              (conv_pwl): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (2): EdgeResidual(\n",
       "              (conv_exp): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (se): Identity()\n",
       "              (conv_pwl): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (3): EdgeResidual(\n",
       "              (conv_exp): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (se): Identity()\n",
       "              (conv_pwl): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (3): Sequential(\n",
       "            (0): InvertedResidual(\n",
       "              (conv_pw): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): InvertedResidual(\n",
       "              (conv_pw): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (2): InvertedResidual(\n",
       "              (conv_pw): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (3): InvertedResidual(\n",
       "              (conv_pw): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (4): InvertedResidual(\n",
       "              (conv_pw): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (5): InvertedResidual(\n",
       "              (conv_pw): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (4): Sequential(\n",
       "            (0): InvertedResidual(\n",
       "              (conv_pw): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "              (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): InvertedResidual(\n",
       "              (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "              (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (2): InvertedResidual(\n",
       "              (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "              (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (3): InvertedResidual(\n",
       "              (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "              (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (4): InvertedResidual(\n",
       "              (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "              (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (5): InvertedResidual(\n",
       "              (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "              (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (6): InvertedResidual(\n",
       "              (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "              (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (7): InvertedResidual(\n",
       "              (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "              (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (8): InvertedResidual(\n",
       "              (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "              (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): Sequential(\n",
       "            (0): InvertedResidual(\n",
       "              (conv_pw): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(960, 960, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=960, bias=False)\n",
       "              (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (2): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (3): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (4): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (5): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (6): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (7): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (8): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (9): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (10): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (11): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (12): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (13): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (14): InvertedResidual(\n",
       "              (conv_pw): Conv2d(272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act1): SiLU(inplace=True)\n",
       "              (conv_dw): Conv2d(1632, 1632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1632, bias=False)\n",
       "              (bn2): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (act2): SiLU(inplace=True)\n",
       "              (se): SqueezeExcite(\n",
       "                (conv_reduce): Conv2d(1632, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (act1): SiLU(inplace=True)\n",
       "                (conv_expand): Conv2d(68, 1632, kernel_size=(1, 1), stride=(1, 1))\n",
       "                (gate): Sigmoid()\n",
       "              )\n",
       "              (conv_pwl): Conv2d(1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn3): BatchNorm2d(272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (conv_head): Conv2d(272, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(1792, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "        (classifier): Linear(in_features=1792, out_features=38, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (DeiT): DeiT(\n",
       "      (model): VisionTransformer(\n",
       "        (patch_embed): PatchEmbed(\n",
       "          (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "          (norm): Identity()\n",
       "        )\n",
       "        (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "        (blocks): Sequential(\n",
       "          (0): Block(\n",
       "            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): Block(\n",
       "            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): Block(\n",
       "            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): Block(\n",
       "            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Block(\n",
       "            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): Block(\n",
       "            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): Block(\n",
       "            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): Block(\n",
       "            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): Block(\n",
       "            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): Block(\n",
       "            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): Block(\n",
       "            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): Block(\n",
       "            (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): Mlp(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU()\n",
       "              (drop1): Dropout(p=0.0, inplace=False)\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop2): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (pre_logits): Identity()\n",
       "        (head): Linear(in_features=384, out_features=38, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=38, out_features=25, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(dataset, model) :\n",
    "    tqdm_dataset = tqdm(enumerate(dataset))\n",
    "    results = []\n",
    "    for batch, batch_item in tqdm_dataset :\n",
    "        img_effi = batch_item['effi'].to(device) # 288\n",
    "        img_deit = batch_item['deit'].to(device) # 224\n",
    "\n",
    "    \n",
    "#         print(img.shape)\n",
    "        predictions = model(img_deit, img_effi)\n",
    "        predictions = F.softmax(predictions.cpu(), dim=1)\n",
    "        batch_result = [int(torch.argmax(prediction.detach().cpu())) for prediction in predictions]\n",
    "#         print(batch_result)\n",
    "#         for prediction in predictions :\n",
    "            \n",
    "#         results.append(int(torch.argmax(predictions[0])))\n",
    "#             output = model(img)\n",
    "#         output = torch.tensor(torch.argmax(output, dim=1), dtype=torch.int32).cpu().numpy()\n",
    "        results.extend(batch_result)\n",
    "    return results\n",
    "\n",
    "\n",
    "model_name_1 = 'efficientnetv2_rw_s'\n",
    "model_name_2 = 'deit_small_patch16_224'\n",
    "n_classes = 25\n",
    "\n",
    "kfold_model_path = '4_9456_public_vill_50k_pretrain_EffiDeit.pt'\n",
    "\n",
    "\n",
    "model = EffiDeiT(model_name_1, model_name_2, n_classes=25, test=True)\n",
    "model.load_state_dict(torch.load(kfold_model_path, map_location=device))\n",
    "model.to(device).eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "760b5535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:06,  6.89s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 3.64 GiB already allocated; 0 bytes free; 3.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-2627d15d6fc9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-7d0301329b37>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(dataset, model)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#         print(img.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_deit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_effi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mbatch_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sub\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-7a98435a328c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_224, input_288)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_288\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_288\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sub\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-7a98435a328c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_224, input_288)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_288\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0moutput1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meffi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_288\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0moutput2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDeiT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sub\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-7a98435a328c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sub\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sub\\lib\\site-packages\\timm\\models\\efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_rate\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sub\\lib\\site-packages\\timm\\models\\efficientnet.py\u001b[0m in \u001b[0;36mforward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_head\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sub\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sub\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sub\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sub\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sub\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sub\\lib\\site-packages\\timm\\models\\efficientnet_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;31m# Depth-wise convolution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_dw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sub\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sub\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\sub\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 443\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 3.64 GiB already allocated; 0 bytes free; 3.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "    \n",
    "preds = predict(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cp = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ea94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_cp = np.array([train_label_decoder[int(val)] for val in preds_cp])\n",
    "submission_csv = pd.read_csv('./data/sample_submission.csv')\n",
    "submission_csv['label'] = preds_cp\n",
    "submission_csv.to_csv('./data/k_public_vill_50k_pretrain_EffiDeit.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470e2029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed402d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41dd84bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[ 3.6658, -0.5562, -0.1303,  0.5749, -0.8724, -0.6668, -2.5437,  0.1855,\n",
    "         -0.2706,  0.0678, -2.0500,  1.9760, -1.0370,  0.3441,  0.0985, -0.5866,\n",
    "          1.5043,  9.5056, -2.4885, -1.4415,  0.1996, -1.3245,  1.3968, -1.2040,\n",
    "         -1.8589],\n",
    "        [ 1.8219, -1.0577,  0.3212,  1.3284, -2.0215, -1.9177, -1.8310, -1.0591,\n",
    "         -1.0233, -0.4418, -2.0763,  1.7701,  1.8592, -0.8363,  9.2857,  0.1729,\n",
    "          4.6880,  0.9189, -0.9552, -3.4629, -2.3523, -1.3255, -2.2687, -0.3306,\n",
    "         -2.9632],\n",
    "        [ 2.5711, -0.8122, -0.7851,  3.7447,  0.2264, -0.7621, -2.6715,  0.6736,\n",
    "          0.6371,  0.6878, -0.2916,  7.9526,  0.5164,  0.2882,  0.2075, -1.2009,\n",
    "          0.3114,  1.2553, -1.6136, -3.8794, -2.2638, -3.2898, -1.2467, -2.5894,\n",
    "         -2.0315],\n",
    "        [ 2.8435,  0.1963,  1.5067, 12.1120, -1.7412, -3.2380, -3.3557, -0.2230,\n",
    "          0.1163,  0.9585, -0.8354,  4.2385,  0.9182,  1.3466,  0.9568,  1.3733,\n",
    "         -0.6904,  3.4432, -2.1798, -4.2389, -4.0742, -2.4559, -3.0247, -3.1489,\n",
    "         -2.2941],\n",
    "        [ 0.8880,  0.2074, -0.1185,  3.3785, -0.1042,  0.1022, -0.2805,  3.8969,\n",
    "          4.9954, -0.7490,  8.5330,  1.3133, -1.4416,  0.1890, -1.2645,  0.6467,\n",
    "         -1.5140,  1.0825, -2.3855, -3.1290, -1.3034, -1.7277, -2.6166, -1.6868,\n",
    "         -2.7182],\n",
    "        [ 1.9738,  0.9231,  0.7457,  1.3640,  0.5198,  0.2546, -1.9709,  1.8885,\n",
    "          1.1139, -1.8140, -0.1462,  1.7628, -1.9919, -1.5093,  1.0957, -0.4808,\n",
    "         -0.0146,  9.5250, -2.0937, -2.2077, -0.2840, -1.3846, -1.4953, -2.0964,\n",
    "         -2.7014],\n",
    "        [10.4486, -0.9522,  0.7026,  1.3279, -0.1882, -1.3398, -1.5953,  0.0131,\n",
    "         -0.2339, -0.4893,  0.9711,  1.7524, -0.6166,  0.2391, -0.1995, -0.3715,\n",
    "         -0.9501,  0.8616, -0.9770, -1.1656, -0.6833, -1.9415,  0.0552, -1.1305,\n",
    "         -0.1138],\n",
    "        [ 1.2428, -0.4092, -0.2502,  3.4074, -0.9933, -2.5447, -1.9779, -1.5831,\n",
    "         -1.2632,  0.3616, -1.1316,  2.7023,  1.4124, -0.5245,  1.4021,  9.2077,\n",
    "         -1.4010,  0.3429, -0.4383, -3.2723, -2.3361, -0.8439, -4.3588, -3.1138,\n",
    "         -0.4072]], device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "974c7151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 14, 11, 3, 10, 17, 0, 15]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_result = [int(torch.argmax(ab)) for ab in a]\n",
    "batch_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
