{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "import torchvision\n",
    "\n",
    "# Utils\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# For Image Models\n",
    "import timm\n",
    "\n",
    "# Albumentations for augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "\n",
    "# For colored terminal text\n",
    "from colorama import Fore, Back, Style\n",
    "b_ = Fore.BLUE\n",
    "sr_ = Style.RESET_ALL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For descriptive error messages\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"seed\": 2455,\n",
    "    \"img_size\": 512,\n",
    "    \"model_name\": \"efficientnetv2_rw_s\", #\"tf_efficientnetv2_s_in21ft1k\",\n",
    "    \"num_classes\": 5,\n",
    "    \"valid_batch_size\": 32,\n",
    "    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '/kaggle/input/UBC-OCEAN'\n",
    "TEST_DIR = '/kaggle/input/UBC-OCEAN/test_thumbnails'\n",
    "ALT_TEST_DIR = '/kaggle/input/UBC-OCEAN/test_images'\n",
    "# TEST_DIR = '/kaggle/input/UBC-OCEAN/train_thumbnails'\n",
    "# ALT_TEST_DIR = '/kaggle/input/UBC-OCEAN/train_images'\n",
    "LABEL_ENCODER_BIN = ['CC', 'EC', 'HGSC', 'LGSC', 'MC']\n",
    "BEST_WEIGHT = \"/kaggle/input/effiv2/4E-val0.5517857142857143-efficientnetv2_rw_s.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_file_path(image_id):\n",
    "    if os.path.exists(f\"{TEST_DIR}/{image_id}_thumbnail.png\"):\n",
    "        return f\"{TEST_DIR}/{image_id}_thumbnail.png\"\n",
    "    else:\n",
    "        return f\"{ALT_TEST_DIR}/{image_id}.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{ROOT_DIR}/test.csv\")\n",
    "# df = pd.read_csv(f\"{ROOT_DIR}/train.csv\")\n",
    "df['file_path'] = df['image_id'].apply(get_test_file_path)\n",
    "df['label'] = 0 # dummy\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(f\"{ROOT_DIR}/sample_submission.csv\")\n",
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = joblib.load( LABEL_ENCODER_BIN )\n",
    "encoder = LABEL_ENCODER_BIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UBCDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None):\n",
    "        self.df = df\n",
    "        self.file_names = df['file_path'].values\n",
    "        self.labels = df['label'].values\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_names[index]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "            \n",
    "        return {\n",
    "            'image': img,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackgroundRemove(ImageOnlyTransform):\n",
    "    def __init__(self,\n",
    "                 threshold=215, \n",
    "                 always_apply: bool = False, p: float = 0.5):\n",
    "        super(BackgroundRemove, self).__init__(always_apply, p)\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def apply(self, img, **params):\n",
    "        return self.remove_background(img)\n",
    "    \n",
    "    def remove_background(self, img):\n",
    "        img[np.where((img > [self.threshold, self.threshold, self.threshold]).all(axis = 2))] = [0,0,0]\n",
    "        return img\n",
    "    \n",
    "    def get_transform_init_args_names(self):\n",
    "        return (\"threshold\",)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GeM(nn.Module):\n",
    "#     def __init__(self, p=3, eps=1e-6):\n",
    "#         super(GeM, self).__init__()\n",
    "#         self.p = nn.Parameter(torch.ones(1)*p)\n",
    "#         self.eps = eps\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.gem(x, p=self.p, eps=self.eps)\n",
    "        \n",
    "#     def gem(self, x, p=3, eps=1e-6):\n",
    "#         return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "        \n",
    "#     def __repr__(self):\n",
    "#         return self.__class__.__name__ + \\\n",
    "#                 '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "#                 ', ' + 'eps=' + str(self.eps) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class UBCModel(nn.Module):\n",
    "#     def __init__(self, model_name, num_classes, pretrained=False, checkpoint_path=None):\n",
    "#         super(UBCModel, self).__init__()\n",
    "#         self.model = timm.create_model('tf_efficientnetv2_s_in21ft1k', pretrained=pretrained)\n",
    "\n",
    "#         in_features = self.model.classifier.in_features\n",
    "#         self.model.classifier = nn.Identity()\n",
    "#         self.model.global_pool = nn.Identity()\n",
    "#         self.pooling = GeM()\n",
    "#         self.linear = nn.Linear(in_features, num_classes)\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "#     def forward(self, images):\n",
    "#         features = self.model(images)\n",
    "#         pooled_features = self.pooling(features).flatten(1)\n",
    "#         output = self.linear(pooled_features)\n",
    "#         return output\n",
    "\n",
    "class BaseModel(nn.Module) :\n",
    "    def __init__(self, model_name, num_classes) -> None:\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = timm.create_model(model_name=model_name, pretrained=False)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            self.backbone,\n",
    "            nn.Linear(1000, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        return self.model(x)\n",
    "    \n",
    "model = BaseModel(CONFIG['model_name'], CONFIG['num_classes'])\n",
    "model.load_state_dict(torch.load( BEST_WEIGHT )['model_state_dict'])\n",
    "model.to(CONFIG['device']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"valid\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        BackgroundRemove(always_apply=True), ## 0.25 점수 제출하고 추가\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.)\n",
    "}\n",
    "\n",
    "\n",
    "test_dataset = UBCDataset(df, transforms=data_transforms[\"valid\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['valid_batch_size'], \n",
    "                          num_workers=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "with torch.no_grad():\n",
    "    bar = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    for step, data in bar:        \n",
    "        images = data['image'].to(CONFIG[\"device\"], dtype=torch.float)        \n",
    "        batch_size = images.size(0)\n",
    "#         outputs = model(images)\n",
    "        predicted = model(images).argmax(1).detach().cpu().numpy().tolist()\n",
    "        preds.append(predicted)\n",
    "preds = np.concatenate(preds).flatten()\n",
    "# pred_labels = encoder.inverse_transform( preds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = [encoder[p] for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub[\"label\"] = pred_labels\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-1.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
