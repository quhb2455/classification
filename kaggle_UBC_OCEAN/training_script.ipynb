{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENCV_IO_MAX_IMAGE_PIXELS\"] = pow(2,40).__str__()\n",
    "# import pyvips\n",
    "\n",
    "import json\n",
    "import time\n",
    "from typing import Any\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "import timm \n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yes | sudo dpkg -i /kaggle/input/libvips-pyvips-installation-and-getting-started/libvips/*.deb\n",
    "# !pip install /kaggle/input/libvips-pyvips-installation-and-getting-started/pyvips/pyvips-2.2.1-py2.py3-none-any.whl --no-index --find-links /kaggle/input/libvips-pyvips-installation-and-getting-started/pyvips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_weight(data_path):\n",
    "    num_data_samples = []\n",
    "    for p in sorted(glob(os.path.join(data_path, \"*\"))) :\n",
    "        num_data_samples.append(len(os.listdir(p)))\n",
    "    return [1 - (x / sum(num_data_samples)) for x in num_data_samples]\n",
    "\n",
    "def score(true_labels, model_preds, threshold=None) :\n",
    "    model_preds = model_preds.argmax(1).detach().cpu().numpy().tolist()\n",
    "    true_labels = true_labels.detach().cpu().numpy().tolist()\n",
    "    return f1_score(true_labels, model_preds, average='weighted')\n",
    "\n",
    "def save_config(config, save_path, save_name=\"\") :\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    cfg_save_time = datetime.now().strftime('%Y_%m_%d-%H_%M_%S')\n",
    "    with open(os.path.join(save_path, f\"{save_name}_{cfg_save_time}.json\"), 'w') as f:\n",
    "        json.dump(config, f, indent=\"\\t\")\n",
    "\n",
    "def save_img(path, img, extension=\".png\") :\n",
    "    result, encoded_img = cv2.imencode(extension, img)\n",
    "    if result:\n",
    "        with open(path, mode='w+b') as f:\n",
    "            encoded_img.tofile(f)\n",
    "\n",
    "def load_img(path) :\n",
    "    img_array = np.fromfile(path, np.uint8)\n",
    "    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "    return img\n",
    "\n",
    "def vips_read_image(image_path, longest_edge):\n",
    "    \n",
    "    \"\"\"\n",
    "    Read image using libvips\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image_path: str\n",
    "        Path of the image\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    image: numpy.ndarray of shape (height, width, 3)\n",
    "        Image array\n",
    "    \"\"\"\n",
    "    \n",
    "    image_thumbnail = pyvips.Image.thumbnail(image_path, longest_edge)\n",
    "\n",
    "    return np.ndarray(\n",
    "        buffer=image_thumbnail.write_to_memory(),\n",
    "        dtype=np.uint8,\n",
    "        shape=[image_thumbnail.height, image_thumbnail.width, image_thumbnail.bands]\n",
    "    )\n",
    "\n",
    "def read_json(path):\n",
    "    with open(path, \"r\") as j :\n",
    "        m = json.load(j)\n",
    "    return m\n",
    "\n",
    "def label_enc(label_name) : \n",
    "    return {n:idx for idx, n in enumerate(sorted(label_name))}\n",
    "\n",
    "def label_dec(label_name) : \n",
    "    return {idx:n for idx, n in enumerate(sorted(label_name))}\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int32(W * cut_rat)\n",
    "    cut_h = np.int32(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def cutmix(imgs, labels):\n",
    "    lam = np.random.beta(1.0, 1.0)\n",
    "    rand_index = torch.randperm(imgs.size()[0]).cuda()\n",
    "    target_a = labels\n",
    "    target_b = labels[rand_index]\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(imgs.size(), lam)\n",
    "    imgs[:, :, bbx1:bbx2, bby1:bby2] = imgs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (imgs.size()[-1] * imgs.size()[-2]))\n",
    "\n",
    "    return imgs, lam, target_a, target_b\n",
    "\n",
    "def mixup(imgs, labels):\n",
    "    lam = np.random.beta(1.0, 1.0)\n",
    "    rand_index = torch.randperm(imgs.size()[0]).cuda()\n",
    "    mixed_imgs = lam * imgs + (1 - lam) * imgs[rand_index, :]\n",
    "    target_a, target_b = labels, labels[rand_index]\n",
    "\n",
    "    return mixed_imgs, lam, target_a, target_b\n",
    "\n",
    "def logging(path):\n",
    "    logger = SummaryWriter(path)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer() :\n",
    "    def __init__(self) -> None:\n",
    "        # self.train_loader = None\n",
    "        # self.valid_loader = None\n",
    "        # self.criterion = None\n",
    "        # self.optimizer = None\n",
    "        # self.model = None\n",
    "        \n",
    "        self.best_score = 0\n",
    "        self.early_stop_cnt = 0\n",
    "        \n",
    "    def run(self, **cfg) :\n",
    "        self.log = logging(os.path.join(cfg[\"log_path\"], cfg[\"model_name\"], time.strftime('%Y%m%d_%H_%M_%S', time.localtime())))\n",
    "        \n",
    "        start_epoch = self.train_weight_load(cfg[\"weight_path\"]) if cfg[\"reuse\"] else 0\n",
    "            \n",
    "        for e in range(start_epoch, cfg[\"epochs\"]) :\n",
    "            self.train_on_epoch(e, **cfg)\n",
    "            valid_acc, valid_loss = self.valid_on_epoch(e, **cfg)\n",
    "            \n",
    "            self.logging({\"Valid ACC\" : valid_acc, \"Valid Loss\" : valid_loss}, e)\n",
    "            self.save_checkpoint(e, valid_acc, **cfg)\n",
    "            \n",
    "            if self.early_stop_cnt == cfg[\"early_stop_patient\"] :\n",
    "                print(\"=== EARLY STOP ===\")\n",
    "                break\n",
    "    \n",
    "    def train_weight_load(self, weight_path) :\n",
    "        checkpoint = torch.load(weight_path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])        \n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        return checkpoint['epoch'] + 1\n",
    "        \n",
    "    def train_on_epoch(self, epoch, **cfg):\n",
    "        self.model.train()\n",
    "        train_acc, train_loss = [], []\n",
    "        tqdm_train = tqdm(self.train_loader)\n",
    "        for step, (img, label) in enumerate(tqdm_train) :\n",
    "            batch_res = self.train_on_batch(img, label, **cfg)\n",
    "            \n",
    "            train_acc.append(batch_res[\"acc\"])\n",
    "            train_loss.append(batch_res[\"loss\"])\n",
    "            \n",
    "            log = {\n",
    "                \"Epoch\" : epoch,\n",
    "                \"Training Acc\" : np.mean(train_acc),\n",
    "                \"Training Loss\" : np.mean(train_loss),\n",
    "            }\n",
    "            tqdm_train.set_postfix(log)\n",
    "            self.logging(log, step)\n",
    "            \n",
    "        # self.scheduler.step()\n",
    "        \n",
    "    def train_on_batch(self, img, label, **cfg) :\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        img = img.to(cfg[\"device\"])\n",
    "        label = label.to(cfg[\"device\"])\n",
    "        \n",
    "        output = self.model(img)\n",
    "        loss = self.criterion(output, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        acc = score(label, output)\n",
    "        \n",
    "        batch_metric = {\n",
    "            \"acc\" : acc,\n",
    "            \"loss\" : loss.item()\n",
    "        }\n",
    "        return batch_metric \n",
    "    \n",
    "    def valid_on_epoch(self, epoch, **cfg):\n",
    "        self.model.eval()\n",
    "        valid_acc, valid_loss = [], []\n",
    "        tqdm_valid = tqdm(self.valid_loader)\n",
    "        for step, (img, label) in enumerate(tqdm_valid) :\n",
    "            batch_res = self.valid_on_batch(img, label, **cfg)\n",
    "            \n",
    "            valid_acc.append(batch_res[\"acc\"])\n",
    "            valid_loss.append(batch_res[\"loss\"])\n",
    "            log = {\n",
    "                \"Epoch\" : epoch,\n",
    "                \"Validation Acc\" : np.mean(valid_acc),\n",
    "                \"Validation Loss\" : np.mean(valid_loss),\n",
    "            }\n",
    "            tqdm_valid.set_postfix(log)\n",
    "            \n",
    "        return np.mean(valid_acc), np.mean(valid_loss)\n",
    "    \n",
    "    def valid_on_batch(self, img, label, **cfg):\n",
    "        img = img.to(cfg[\"device\"])\n",
    "        label = label.to(cfg[\"device\"])\n",
    "        \n",
    "        output = self.model(img)\n",
    "        loss = self.criterion(output, label)\n",
    "        \n",
    "        acc = score(label, output)\n",
    "        \n",
    "        batch_metric = {\n",
    "            \"acc\" : acc,\n",
    "            \"loss\" : loss.item()\n",
    "        }\n",
    "        \n",
    "        return batch_metric\n",
    "    \n",
    "    def save_checkpoint(self, epoch, val_acc, **cfg) :\n",
    "        if self.best_score < val_acc:\n",
    "            self.best_score = val_acc\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"optimizer_state_dict\": self.optimizer.state_dict()\n",
    "            }, os.path.join(cfg[\"save_path\"], str(epoch) + 'E-val' + str(self.best_score) + '-' + cfg[\"model_name\"] + '.pth'))\n",
    "            self.early_stop_cnt = 0 \n",
    "        else : \n",
    "            self.early_stop_cnt += 1\n",
    "            \n",
    "    def logging(self, log_dict, step) :\n",
    "        for k, v in log_dict.items():\n",
    "            if k == \"Epoch\" : continue\n",
    "            self.log.add_scalar(k, v, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor() :\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "        \n",
    "    def prediction(self, **cfg) :\n",
    "        self.pred_weight_load(cfg[\"weight_path\"])\n",
    "        self.model.eval()        \n",
    "        model_preds = []\n",
    "        with torch.no_grad() :\n",
    "            for img in tqdm(self.test_loader) :\n",
    "                model_preds += self.predict_on_batch(img, **cfg)\n",
    "        \n",
    "        self.save_to_csv(model_preds, **cfg)\n",
    "        \n",
    "    \n",
    "    def predict_on_batch(self, img, **cfg) :\n",
    "        img = img.to(cfg[\"device\"])\n",
    "        return self.model(img).argmax(1).detach().cpu().numpy().tolist()\n",
    "        \n",
    "    def save_to_csv(self, results, **cfg) :\n",
    "        _label_dec = label_dec(cfg[\"label_name\"])\n",
    "        \n",
    "        csv_file = pd.read_csv(cfg[\"csv_path\"])\n",
    "        img_name_list = [df['image_id'] for df in csv_file.iloc]\n",
    "#         img_name_list = [os.path.basename(p).split(\".\")[0] for p in glob(os.path.join(cfg[\"data_path\"], \"*\"))]\n",
    "        res_label_list = [_label_dec[i] for i in results]        \n",
    "        \n",
    "        df = pd.DataFrame({\"image_id\" : img_name_list, \"label\":res_label_list})\n",
    "        df.to_csv(os.path.join(cfg[\"output_path\"], \"submission.csv\"), index=False)\n",
    "    \n",
    "    def pred_weight_load(self, weight_path) :\n",
    "        checkpoint = torch.load(weight_path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module) :\n",
    "    def __init__(self, **cfg) -> None:\n",
    "        super(BaseModel, self).__init__()\n",
    "#         self.model = timm.create_model(model_name=cfg[\"model_name\"], \n",
    "#                                        num_classes=cfg[\"num_classes\"], \n",
    "#                                        pretrained=False,\n",
    "#                                        checkpoint_path=\"/kaggle/input/effiv2s/efficientnet_v2s_ra2_288-a6477665.pth\")\n",
    "        self.model = timm.create_model(model_name=cfg[\"model_name\"], \n",
    "                                       num_classes=cfg[\"num_classes\"], \n",
    "                                       pretrained=True)\n",
    "\n",
    "#         self.model = nn.Sequential(\n",
    "#             self.backbone,\n",
    "#             nn.Linear(1000, cfg[\"num_classes\"])\n",
    "#         )\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, imgs, labels, transform=None, binary_mode=False):\n",
    "        self.imgs = imgs\n",
    "        self.labels = labels\n",
    "        self.label_enc = label_enc(sorted(set(labels))) if labels != None else None\n",
    "        self.transform = transform\n",
    "        self.binary_mode = binary_mode\n",
    "        \n",
    "        self.tma_img_dicts = {}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.imgs[index]\n",
    "        \n",
    "        image = load_img(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#         image = vips_read_image(img_path, 512)\n",
    "        \n",
    "        if self.transform :\n",
    "            image = self.transform(image=image)['image']\n",
    "\n",
    "        if self.labels is not None:\n",
    "            if self.binary_mode :\n",
    "                label = torch.tensor(self.binary_encoder(self.label_enc[self.labels[index]]), dtype=torch.long)\n",
    "            else :\n",
    "                label = self.label_enc[self.labels[index]]\n",
    "            \n",
    "            return image, label\n",
    "\n",
    "        else:\n",
    "            return image\n",
    "    \n",
    "    def binary_encoder(self, label):\n",
    "        total_label_len = len(self.label_enc.keys())\n",
    "        return [1 if i == label else 0 for i in range(total_label_len) ]\n",
    "    \n",
    "class DatasetCreater() :\n",
    "    def __init__(self) :\n",
    "        self.base_filename = \".png\"\n",
    "    \n",
    "    def create_dataset(self, transform, **cfg) :\n",
    "        img_path, label_list = self.get_data(**cfg)        \n",
    "        \n",
    "        if cfg[\"mode\"] == \"train\" :\n",
    "            save_config(transform[0].to_dict(), cfg[\"save_path\"], save_name=\"train_transform\")\n",
    "            save_config(transform[1].to_dict(), cfg[\"save_path\"], save_name=\"valid_transform\")\n",
    "            \n",
    "            return [CustomDataset(img_path[0], label_list[0], transform=transform[0], binary_mode=cfg[\"binary_mode\"]), \n",
    "                    CustomDataset(img_path[1], label_list[1], transform=transform[1], binary_mode=cfg[\"binary_mode\"])]\n",
    "            \n",
    "        elif cfg[\"mode\"] == 'infer' :\n",
    "#             save_config(transform.to_dict(), cfg[\"output_path\"], save_name=\"infer_transform\")\n",
    "            return CustomDataset(img_path, label_list, transform=transform)\n",
    "    \n",
    "    \n",
    "    def create_dataloader(self, transform, **cfg) :\n",
    "        ds = self.create_dataset(transform, **cfg)\n",
    "        \n",
    "        if isinstance(ds, list) :\n",
    "            return (DataLoader(ds[0], batch_size=cfg[\"batch_size\"], shuffle=cfg[\"shuffle\"], num_workers=cfg[\"num_worker\"]), \n",
    "                    DataLoader(ds[1], batch_size=cfg[\"batch_size\"], shuffle=cfg[\"shuffle\"], num_workers=cfg[\"num_worker\"]))\n",
    "        else :\n",
    "            return DataLoader(ds, batch_size=cfg[\"batch_size\"], shuffle=cfg[\"shuffle\"], num_workers=cfg[\"num_worker\"], pin_memory=True)\n",
    "        \n",
    "        \n",
    "    def get_data(self, **cfg) :\n",
    "        data_path = cfg[\"data_path\"]\n",
    "        csv_file = pd.read_csv(cfg[\"csv_path\"])\n",
    "        mode = cfg[\"mode\"]\n",
    "        \n",
    "        img_path_list = []\n",
    "        label_list = []\n",
    "        if mode == \"infer\" :\n",
    "            for df in csv_file.iloc :\n",
    "                if os.path.exists(os.path.join(data_path, \"test_images\",str(df['image_id'])+\".png\")):\n",
    "                    img_path_list.append(os.path.join(data_path, \"test_images\", str(df['image_id'])+\".png\"))\n",
    "\n",
    "                else:\n",
    "                    img_path_list.append(os.path.join(data_path, \"test_thumbnails\",str(df['image_id'])+\"_thumbnail.png\"))            \n",
    "            \n",
    "            return img_path_list, None\n",
    "        \n",
    "        else :\n",
    "            for df in csv_file.iloc :\n",
    "#                 if df['is_tma'] :\n",
    "#                     img_path_list.append(os.path.join(data_path, \"train_images\", str(df['image_id'])+\".png\"))\n",
    "#                 else :\n",
    "#                     img_path_list.append(os.path.join(data_path, \"train_thumbnails\", str(df['image_id'])+\"_thumbnail.png\"))\n",
    "                img_path_list.append(os.path.join(data_path, str(df['image_id'])+\".png\"))\n",
    "                label_list.append(df['label'])\n",
    "                \n",
    "            train_img, valid_img, train_label, valid_label = train_test_split(img_path_list, \n",
    "                                                                              label_list, \n",
    "                                                                              test_size=0.1, \n",
    "                                                                              stratify=label_list, \n",
    "                                                                              random_state=2455)\n",
    "            \n",
    "            return [train_img, valid_img], [train_label, valid_label]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BackgroundRemove(ImageOnlyTransform):\n",
    "#     def __init__(self,\n",
    "#                  threshold=215, \n",
    "#                  always_apply: bool = False, p: float = 0.5):\n",
    "#         super(BackgroundRemove, self).__init__(always_apply, p)\n",
    "#         self.threshold = threshold\n",
    "    \n",
    "#     def apply(self, img, **params):\n",
    "#         return self.remove_background(img)\n",
    "    \n",
    "#     def remove_background(self, img):\n",
    "#         img[np.where((img > [self.threshold, self.threshold, self.threshold]).all(axis = 2))] = [0,0,0]\n",
    "#         return img\n",
    "    \n",
    "#     def get_transform_init_args_names(self):\n",
    "#         return (\"threshold\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class BaseMain(Trainer, Predictor, DatasetCreater) :\n",
    "    def __init__(self, **cfg) -> None:\n",
    "        super().__init__()\n",
    "        DatasetCreater.__init__(self)\n",
    "        \n",
    "        self.model = BaseModel(**cfg).to(cfg[\"device\"])\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=cfg[\"learning_rate\"])\n",
    "#         self.criterion = nn.CrossEntropyLoss(weight=torch.tensor(cfg['label_weight'])).to(\"cuda\")        \n",
    "        self.criterion = nn.CrossEntropyLoss().to(\"cuda\")        \n",
    "        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=60, eta_min=5e-4)\n",
    "        \n",
    "        if cfg[\"mode\"] == 'train' :\n",
    "            self.train_loader, self.valid_loader = self.create_dataloader([self.get_transform('train', **cfg), \n",
    "                                                                           self.get_transform('valid', **cfg)], **cfg)\n",
    "        elif cfg[\"mode\"] == 'infer' :\n",
    "            self.test_loader = self.create_dataloader(self.get_transform('infer', **cfg), **cfg)\n",
    "            \n",
    "    def train(self, **cfg) :\n",
    "        self.run(**cfg)\n",
    "        \n",
    "    def train_on_batch(self, img, label, **cfg) :\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        img = img.to(cfg[\"device\"])\n",
    "        label = label.to(cfg[\"device\"])\n",
    "    \n",
    "        output = self.model(img)\n",
    "        loss = self.criterion(output, label)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        acc = score(label, output)\n",
    "\n",
    "        \n",
    "        batch_metric = {\n",
    "            \"acc\" : acc,\n",
    "            \"loss\" : loss.item()\n",
    "        }\n",
    "        \n",
    "        return batch_metric \n",
    "\n",
    "    def valid_on_batch(self, img, label, **cfg):\n",
    "        img = img.to(cfg[\"device\"])\n",
    "        label = label.to(cfg[\"device\"])\n",
    "        \n",
    "        if cfg[\"binary_mode\"] :\n",
    "            mixup_label = torch.argmax(label, dim=1)\n",
    "        \n",
    "            output = self.model(img)\n",
    "            loss = self.criterion(output, label.type(torch.float32))\n",
    "            \n",
    "            acc = score(mixup_label, output)\n",
    "        else :        \n",
    "            output = self.model(img)\n",
    "            loss = self.criterion(output, label)\n",
    "            \n",
    "            acc = score(label, output)\n",
    "        batch_metric = {\n",
    "            \"acc\" : acc,\n",
    "            \"loss\" : loss.item()\n",
    "        }\n",
    "        \n",
    "        return batch_metric\n",
    "       \n",
    "    def infer(self, **cfg) :\n",
    "        self.prediction(**cfg)\n",
    "    \n",
    "    # def predict_on_batch(self, img, **cfg) :\n",
    "        \n",
    "    #     img = img.to(cfg[\"device\"])\n",
    "    #     output = self.model(img)\n",
    "        \n",
    "    #     # output = output * cfg[\"label_weight\"]\n",
    "    #     output = output.detach().cpu() * np.array([[cfg[\"label_weight\"]] * output.shape[0]])[0]\n",
    "\n",
    "    #     # binary_ = torch.softmax(output, dim=1)\n",
    "    #     # binary_[binary_  > 0.9] = 1 \n",
    "    #     # binary_[binary_  <= 0.9] = 0 \n",
    "    #     # return output.argmax(1).detach().cpu().numpy().tolist()\n",
    "        \n",
    "    #     return output.argmax(1).numpy().tolist()\n",
    "    \n",
    "    def get_transform(self, _mode, **cfg) :\n",
    "        resize = cfg[\"resize\"]\n",
    "        if _mode == 'train' :\n",
    "            return A.Compose([\n",
    "                A.Resize(resize, resize),\n",
    "                A.OneOf([\n",
    "                    A.HorizontalFlip(p=1),\n",
    "                    A.VerticalFlip(p=1),\n",
    "                    A.RandomRotate90(p=1)], p=1),\n",
    "                \n",
    "                A.RandomBrightnessContrast(\n",
    "                    brightness_limit=(-0.1, 0.1),\n",
    "                    contrast_limit=(-0.1, 0.1), p=0.3\n",
    "                ),\n",
    "                A.RandomGridShuffle((3, 3), p=0.4),\n",
    "                A.Normalize(),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        elif _mode == 'valid' :\n",
    "            return A.Compose([\n",
    "                A.Resize(resize, resize),\n",
    "                A.Normalize(),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        elif _mode == 'infer' : \n",
    "            return A.Compose([\n",
    "                A.Resize(resize, resize),\n",
    "                A.Normalize(),\n",
    "                ToTensorV2()\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196/196 [00:51<00:00,  3.80it/s, Epoch=0, Training Acc=0.435, Training Loss=1.46]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.22it/s, Epoch=0, Validation Acc=0.466, Validation Loss=1.37]\n",
      "100%|██████████| 196/196 [00:44<00:00,  4.43it/s, Epoch=1, Training Acc=0.631, Training Loss=0.988]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.44it/s, Epoch=1, Validation Acc=0.585, Validation Loss=1.1]  \n",
      "100%|██████████| 196/196 [00:44<00:00,  4.36it/s, Epoch=2, Training Acc=0.707, Training Loss=0.84] \n",
      "100%|██████████| 22/22 [00:01<00:00, 12.07it/s, Epoch=2, Validation Acc=0.682, Validation Loss=0.906]\n",
      "100%|██████████| 196/196 [00:44<00:00,  4.37it/s, Epoch=3, Training Acc=0.808, Training Loss=0.601]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.13it/s, Epoch=3, Validation Acc=0.694, Validation Loss=0.815]\n",
      "100%|██████████| 196/196 [00:45<00:00,  4.35it/s, Epoch=4, Training Acc=0.825, Training Loss=0.566]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.32it/s, Epoch=4, Validation Acc=0.75, Validation Loss=0.814] \n",
      "100%|██████████| 196/196 [00:45<00:00,  4.35it/s, Epoch=5, Training Acc=0.852, Training Loss=0.469]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.15it/s, Epoch=5, Validation Acc=0.778, Validation Loss=0.624]\n",
      "100%|██████████| 196/196 [00:44<00:00,  4.36it/s, Epoch=6, Training Acc=0.864, Training Loss=0.432]\n",
      "100%|██████████| 22/22 [00:01<00:00, 11.69it/s, Epoch=6, Validation Acc=0.767, Validation Loss=0.654]\n",
      "100%|██████████| 196/196 [00:44<00:00,  4.36it/s, Epoch=7, Training Acc=0.899, Training Loss=0.368]\n",
      "100%|██████████| 22/22 [00:01<00:00, 11.68it/s, Epoch=7, Validation Acc=0.762, Validation Loss=0.63] \n",
      "100%|██████████| 196/196 [00:44<00:00,  4.38it/s, Epoch=8, Training Acc=0.919, Training Loss=0.288]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.06it/s, Epoch=8, Validation Acc=0.792, Validation Loss=0.582]\n",
      "100%|██████████| 196/196 [00:44<00:00,  4.40it/s, Epoch=9, Training Acc=0.92, Training Loss=0.266] \n",
      "100%|██████████| 22/22 [00:01<00:00, 12.18it/s, Epoch=9, Validation Acc=0.8, Validation Loss=0.659]  \n",
      "100%|██████████| 196/196 [00:44<00:00,  4.38it/s, Epoch=10, Training Acc=0.919, Training Loss=0.304]\n",
      "100%|██████████| 22/22 [00:01<00:00, 11.98it/s, Epoch=10, Validation Acc=0.734, Validation Loss=0.648]\n",
      "100%|██████████| 196/196 [00:44<00:00,  4.37it/s, Epoch=11, Training Acc=0.944, Training Loss=0.221]\n",
      "100%|██████████| 22/22 [00:01<00:00, 11.88it/s, Epoch=11, Validation Acc=0.848, Validation Loss=0.693]\n",
      "100%|██████████| 196/196 [00:44<00:00,  4.37it/s, Epoch=12, Training Acc=0.935, Training Loss=0.22] \n",
      "100%|██████████| 22/22 [00:01<00:00, 11.93it/s, Epoch=12, Validation Acc=0.811, Validation Loss=0.631]\n",
      "100%|██████████| 196/196 [00:44<00:00,  4.38it/s, Epoch=13, Training Acc=0.956, Training Loss=0.176]\n",
      "100%|██████████| 22/22 [00:01<00:00, 11.71it/s, Epoch=13, Validation Acc=0.811, Validation Loss=0.596]\n",
      "100%|██████████| 196/196 [00:44<00:00,  4.39it/s, Epoch=14, Training Acc=0.937, Training Loss=0.22] \n",
      "100%|██████████| 22/22 [00:01<00:00, 12.30it/s, Epoch=14, Validation Acc=0.773, Validation Loss=0.7]  \n",
      "100%|██████████| 196/196 [00:44<00:00,  4.39it/s, Epoch=15, Training Acc=0.97, Training Loss=0.129] \n",
      "100%|██████████| 22/22 [00:01<00:00, 11.79it/s, Epoch=15, Validation Acc=0.758, Validation Loss=0.66] \n",
      "100%|██████████| 196/196 [00:44<00:00,  4.36it/s, Epoch=16, Training Acc=0.963, Training Loss=0.143]\n",
      "100%|██████████| 22/22 [00:01<00:00, 11.93it/s, Epoch=16, Validation Acc=0.781, Validation Loss=0.703]\n",
      "100%|██████████| 196/196 [00:44<00:00,  4.39it/s, Epoch=17, Training Acc=0.963, Training Loss=0.153]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.08it/s, Epoch=17, Validation Acc=0.744, Validation Loss=0.753]\n",
      "100%|██████████| 196/196 [00:45<00:00,  4.34it/s, Epoch=18, Training Acc=0.956, Training Loss=0.164]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.09it/s, Epoch=18, Validation Acc=0.829, Validation Loss=0.607]\n",
      "100%|██████████| 196/196 [00:46<00:00,  4.24it/s, Epoch=19, Training Acc=0.949, Training Loss=0.148]\n",
      "100%|██████████| 22/22 [00:01<00:00, 12.04it/s, Epoch=19, Validation Acc=0.855, Validation Loss=0.554]\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "\n",
    "cfg = {\n",
    "        \"mode\" : \"train\", #train, #infer\n",
    "        \n",
    "        \"model_name\" : \"efficientnetv2_rw_s.ra2_in1k\", #\"tf_efficientnetv2_m.in21k\", #\"swinv2_base_window12to16_192to256_22kft1k\",\n",
    "        #\"tf_efficientnetv2_s.in21k\",#\"eva_large_patch14_196.in22k_ft_in1k\",#\"beit_base_patch16_224.in22k_ft_in22k\",\n",
    "        \"num_classes\" : 5,\n",
    "        \n",
    "        \"learning_rate\" : 1e-4,\n",
    "        \"focal_alpha\" : 2,\n",
    "        \"focal_gamma\" : 2,\n",
    "        \"resize\" : 512,\n",
    "        \n",
    "        \"data_path\" : \"./data/crop_resize_rmbg\", \n",
    "        \"csv_path\" : \"./data/crop_resize_rmbg.csv\",\n",
    "        \"epochs\" : 20,\n",
    "        \"batch_size\" : 4,\n",
    "        \"num_worker\" : 0,\n",
    "        \"early_stop_patient\" : 10,\n",
    "        \n",
    "        \"binary_mode\" : False,\n",
    "        \"reuse\" : False, #True, #False\n",
    "        \"weight_path\" : None, #\"./ckpt/tf_efficientnetv2_s.in21k/rmbg_lossweight_effiv2s_512/11E-val0.5294871794871795-tf_efficientnetv2_s.in21k.pth\",\n",
    "        \n",
    "        \"save_path\" : \"./ckpt/tf_efficientnetv2_s.in21k/cr_rs_rb_effiv2s_512\",\n",
    "        \"output_path\" : \"./output/tf_efficientnetv2_s.in21k/cr_rs_rb_effiv2s_512\",\n",
    "        \"log_path\" : \"./logging\",\n",
    "        \"device\" : \"cuda\",\n",
    "        \"label_name\" :[\"HGSC\", \"LGSC\", \"EC\", \"CC\", \"MC\"],\n",
    "        \n",
    "#         \"label_weight\" : [0.4646, 0.3709, 0.2072, 0.9787, 1.0]\n",
    "}        \n",
    "\n",
    "set_seed(2455)\n",
    "\n",
    "if cfg[\"mode\"] == \"train\" :\n",
    "    cfg[\"shuffle\"] = True\n",
    "elif cfg[\"mode\"] == \"infer\" :\n",
    "    cfg[\"shuffle\"] = False\n",
    "\n",
    "save_config(cfg, cfg[\"save_path\"], save_name=cfg[\"mode\"]+\"_config\")\n",
    "\n",
    "base_main = BaseMain(**cfg)\n",
    "\n",
    "if cfg[\"mode\"] == \"train\" :\n",
    "    base_main.train(**cfg)\n",
    "elif cfg[\"mode\"] == \"infer\" :\n",
    "    base_main.infer(**cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-1.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
