{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32523444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py # .h5 파일을 읽기 위한 패키지\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from cfg.voxelnet_cfg import config as cfg \n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67cb3016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "# conv2d + bn + relu\n",
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k,s,p, activation=True, batch_norm=True):\n",
    "        super(Conv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=k,stride=s,padding=p)\n",
    "        \n",
    "        if batch_norm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        else:\n",
    "            self.bn = None\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x=self.bn(x)\n",
    "        if self.activation:\n",
    "            return F.relu(x,inplace=True)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "# conv3d + bn + relu\n",
    "class Conv3d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, k, s, p, batch_norm=True):\n",
    "        super(Conv3d, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=k, stride=s, padding=p)\n",
    "        if batch_norm:\n",
    "            self.bn = nn.BatchNorm3d(out_channels)\n",
    "        else:\n",
    "            self.bn = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "\n",
    "        return F.relu(x, inplace=True)\n",
    "\n",
    "# Fully Connected Network\n",
    "class FCN(nn.Module):\n",
    "\n",
    "    def __init__(self,cin,cout):\n",
    "        super(FCN, self).__init__()\n",
    "        self.cout = cout\n",
    "        self.linear = nn.Linear(cin, cout)\n",
    "        self.bn = nn.BatchNorm1d(cout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # KK is the stacked k across batch\n",
    "#         print(\"1. FCN x.shape : \",x.shape)\n",
    "        kk, t, _ = x.shape\n",
    "#         N, D, H, W = x.shape\n",
    "\n",
    "        x = self.linear(x.view(kk * t, -1))\n",
    "#         x = self.linear(x.view(N*D*H, -1))\n",
    "        \n",
    "\n",
    "#         print(\"2. FCN x.shape : \", x.shape)\n",
    "        x = F.relu(self.bn(x))\n",
    "        \n",
    "        return x.view(kk,t, -1)\n",
    "#         return x.view(N, -1, H, x.shape[1])\n",
    "\n",
    "# Voxel Feature Encoding layer\n",
    "class VFE(nn.Module):\n",
    "\n",
    "    def __init__(self,cin,cout):\n",
    "        super(VFE, self).__init__()\n",
    "        assert cout % 2 == 0\n",
    "        self.units = cout // 2\n",
    "        self.fcn = FCN(cin,self.units)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # point-wise feauture\n",
    "        pwf = self.fcn(x)\n",
    "        \n",
    "        #locally aggregated feature\n",
    "#         print(\"1. VEF pwf.shape : \", pwf.shape)\n",
    "#         print(\"1. VEF torch max pwf shape : \",  torch.max(pwf,2)[0].shape)\n",
    "#         print(\"1. VEF torch max pwf unsq(1) shape : \",  torch.max(pwf,2)[0].unsqueeze(2).shape)\n",
    "#         print(\"1. VEF torch max pwf unsq(1) repeat(1,35,1) shape : \",  torch.max(pwf,2)[0].unsqueeze(2).repeat(1,1,cfg.T,1).shape)\n",
    "        laf = torch.max(pwf,1)[0].unsqueeze(1).repeat(1,cfg.T,1)\n",
    "#         laf = torch.max(pwf,2)[0].unsqueeze(2).repeat(1,1,cfg.T,1)\n",
    "#         print(\"2. VEF laf.shape : \", laf.shape)\n",
    "        \n",
    "        # point-wise concat feature\n",
    "        pwcf = torch.cat((pwf,laf),dim=2)\n",
    "#         pwcf = torch.cat((pwf,laf),dim=3)\n",
    "#         print(\"3. VEF pwcf.shape : \", pwcf.shape)\n",
    "\n",
    "        # apply mask\n",
    "#         print(\"4. VEF mask shape : \", mask.shape)\n",
    "#         print(\"4. VEF mask unsq(2) shape : \", mask.unsqueeze(2).shape)\n",
    "#         print(\"4. VEF mask unsq(2) repeat(1,1, ??) shape : \", mask.unsqueeze(2).repeat(1, 1, self.units * 2).shape)\n",
    "        mask = mask.unsqueeze(2).repeat(1, 1, self.units * 2)\n",
    "#         mask = mask.unsqueeze(3).repeat(1, 1, 1, self.units * 2)\n",
    "#         print(\"4. VEF mask.shape : \", mask.shape)\n",
    "        pwcf = pwcf * mask.float()\n",
    "\n",
    "        return pwcf\n",
    "\n",
    "# Stacked Voxel Feature Encoding\n",
    "class SVFE(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SVFE, self).__init__()\n",
    "#         self.vfe_1 = VFE(6,32)\n",
    "        self.vfe_1 = VFE(7,32)\n",
    "        self.vfe_2 = VFE(32,128)\n",
    "        self.fcn = FCN(128,128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mask = torch.ne(torch.max(x,2)[0], 0)\n",
    "#         print(\"SVFE Mask value : \", mask)\n",
    "        x = self.vfe_1(x, mask)\n",
    "        x = self.vfe_2(x, mask)\n",
    "        x = self.fcn(x)\n",
    "#         print(\"SVFE x.shape: \",x.shape)\n",
    "        # element-wise max pooling\n",
    "        x = torch.max(x,1)[0]\n",
    "        return x\n",
    "\n",
    "# Convolutional Middle Layer\n",
    "class CML(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CML, self).__init__()\n",
    "        self.conv3d_1 = Conv3d(128, 64, 3, s=(2, 1, 1), p=(1, 1, 1))\n",
    "        self.conv3d_2 = Conv3d(64, 64, 3, s=(1, 1, 1), p=(0, 1, 1))\n",
    "        self.conv3d_3 = Conv3d(64, 64, 3, s=(2, 1, 1), p=(1, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv3d_1(x)\n",
    "        x = self.conv3d_2(x)\n",
    "        x = self.conv3d_3(x)\n",
    "        return x\n",
    "\n",
    "class RPN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RPN, self).__init__()\n",
    "        self.in_c = 448 # 192\n",
    "        self.out_c = 576 # 256\n",
    "        self.score_c = 1728 # 768\n",
    "        self.cls_c = 2560 # 640\n",
    "        \n",
    "        self.block_1 = [Conv2d(self.in_c, self.in_c, 3, 2, 1)]\n",
    "        self.block_1 += [Conv2d(self.in_c, self.in_c, 3, 1, 1) for _ in range(3)]\n",
    "        self.block_1 = nn.Sequential(*self.block_1)\n",
    "\n",
    "        self.block_2 = [Conv2d(self.in_c, self.in_c, 3, 2, 1)]\n",
    "        self.block_2 += [Conv2d(self.in_c, self.in_c, 3, 1, 1) for _ in range(5)]\n",
    "        self.block_2 = nn.Sequential(*self.block_2)\n",
    "\n",
    "        self.block_3 = [Conv2d(self.in_c, self.out_c, 3, 2, 1)]\n",
    "        self.block_3 += [nn.Conv2d(self.out_c, self.out_c, 3, 1, 1) for _ in range(5)]\n",
    "        self.block_3 = nn.Sequential(*self.block_3)\n",
    "\n",
    "        self.deconv_1 = nn.Sequential(nn.ConvTranspose2d(self.out_c, self.out_c, 4, 4, 0),nn.BatchNorm2d(self.out_c))\n",
    "        self.deconv_2 = nn.Sequential(nn.ConvTranspose2d(self.in_c, self.out_c, 2, 2, 0),nn.BatchNorm2d(self.out_c))\n",
    "        self.deconv_3 = nn.Sequential(nn.ConvTranspose2d(self.in_c, self.out_c, 1, 1, 0),nn.BatchNorm2d(self.out_c))\n",
    "\n",
    "        self.score_head = Conv2d(self.score_c, 10, 1, 1, 0, activation=False, batch_norm=False)\n",
    "#         self.reg_head = Conv2d(768, 7 * cfg.anchors_per_position, 1, 1, 0, activation=False, batch_norm=False)\n",
    "        self.cls = Classifier(cin=self.cls_c)\n",
    "    def forward(self,x, batch_size):\n",
    "        x = self.block_1(x)\n",
    "        x_skip_1 = x\n",
    "        x = self.block_2(x)\n",
    "        x_skip_2 = x\n",
    "        x = self.block_3(x)\n",
    "#         print(\"RPN block_3 x.shape : \",x.shape)\n",
    "        x_0 = self.deconv_1(x)\n",
    "#         print(\"RPN deconv_1 x_0.shape : \",x_0.shape)\n",
    "        x_1 = self.deconv_2(x_skip_2)\n",
    "#         print(\"RPN deconv_2 x_1.shape : \",x_1.shape)\n",
    "        x_2 = self.deconv_3(x_skip_1)\n",
    "#         print(\"RPN deconv_3 x_2.shape : \",x_2.shape)\n",
    "        x = torch.cat((x_0,x_1,x_2),1)\n",
    "        x = self.score_head(x)\n",
    "        x = self.cls(x.view(batch_size, -1))\n",
    "#         print(\"RPN cls x.shape : \",x.shape)\n",
    "        return x #self.score_head(x),self.reg_head(x)\n",
    "    \n",
    "    \n",
    "# classifier\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, cin, num_classes=10, dropout_rate=0.5):\n",
    "        super(Classifier, self).__init__()  \n",
    "        \n",
    "        self.linear_1 = nn.Sequential(\n",
    "            nn.Linear(cin, cin//2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate, inplace=False)\n",
    "        )\n",
    "        \n",
    "        self.linear_2 = nn.Sequential(\n",
    "            nn.Linear(cin//2, cin//4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate, inplace=False)\n",
    "        )\n",
    "        \n",
    "        self.linear_3 = nn.Linear(cin//4, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # KK is the stacked k across batch\n",
    "#         print(\"classifier x.shape : \", x.shape)\n",
    "#         print(\"classifier x.view.shape : \", x.view(2,-1).shape)\n",
    "        b, f, *_ = x.shape\n",
    "        x = self.linear_1(x.view(b, -1))\n",
    "        x = self.linear_2(x)\n",
    "        x = self.linear_3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class VoxelNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VoxelNet, self).__init__()\n",
    "        self.svfe = SVFE()\n",
    "        self.cml = CML()\n",
    "        self.rpn = RPN()\n",
    "        self.cls = Classifier(cin=64)\n",
    "        \n",
    "    def voxel_indexing(self, sparse_features, coords, batch_size):\n",
    "        dim = sparse_features.shape[-1]\n",
    "#         print(\"sparse_features.shape : \", sparse_features.shape)\n",
    "        \n",
    "        coords= coords.type(torch.long)\n",
    "#         print(\"coords.shape : \", coords.shape)\n",
    "#         print(\"coords values [:,0]: \",coords[:,0])\n",
    "        dense_feature = torch.zeros(dim, batch_size, cfg.D, cfg.H, cfg.W).to(cfg.device)\n",
    "#         print(\"dense_feature.shape : \", dense_feature.shape)\n",
    "        dense_feature[:, coords[:,0], coords[:,1], coords[:,2], coords[:,3]]= sparse_features.transpose(0,1)\n",
    "#         dense_feature[:, coords[:,0], coords[:,1], coords[:,2]]= sparse_features\n",
    "        return dense_feature.transpose(0, 1)\n",
    "\n",
    "    def forward(self, voxel_features, voxel_coords, batch_size):\n",
    "\n",
    "        # feature learning network\n",
    "        vwfs = self.svfe(voxel_features)\n",
    "        vwfs = self.voxel_indexing(vwfs,voxel_coords, batch_size)\n",
    "\n",
    "        # convolutional middle network\n",
    "        cml_out = self.cml(vwfs)\n",
    "        cml_out = cml_out.view(batch_size, -1,cfg.H, cfg.W)\n",
    "#         print(\"cml_out.shape : \", cml_out.shape)\n",
    "\n",
    "        # region proposal network\n",
    "        score = self.rpn(cml_out, batch_size)\n",
    "        # merge the depth and feature dim into one, output probability score map and regression map\n",
    "        # psm,rm = self.rpn(cml_out.view(cfg.N,-1,cfg.H, cfg.W))\n",
    "        \n",
    "        # classifier\n",
    "#         print(\"score shape : \", score)\n",
    "\n",
    "        return score #psm, rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c7a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_collate(batch):\n",
    "    voxel_features = []\n",
    "    voxel_coords = []\n",
    "    train = True if len(batch[0]) == 3 else False\n",
    "    \n",
    "    for i, sample in enumerate(batch):\n",
    "        voxel_features.append(sample[0])\n",
    "        voxel_coords.append(np.pad(sample[1], ((0,0), (1,0)), mode='constant', constant_values=i))\n",
    "\n",
    "    if train :\n",
    "        return np.concatenate(voxel_features), np.concatenate(voxel_coords), np.array(batch)[:, 2].astype(np.long)\n",
    "    else :\n",
    "        return np.concatenate(voxel_features), np.concatenate(voxel_coords), len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e73538de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxelDataset(Dataset) :\n",
    "    def __init__(self, id_list, label_list, point_list) :\n",
    "        self.id_list = id_list\n",
    "        self.label_list = label_list\n",
    "        self.point_list = point_list\n",
    "        \n",
    "        self.x_alpha = 0\n",
    "        \n",
    "    def __getitem__(self, index) :\n",
    "        image_id = self.id_list[index]\n",
    "        points= self.point_list[str(image_id)][:]\n",
    "\n",
    "        if self.label_list is not None:\n",
    "            points = self.rand_sampling(points, low_rate=0.8, high_rate=0.6)            \n",
    "            points= self.trans_axis_range(points, axis=[0,1,2])\n",
    "            \n",
    "            x_degree = np.random.choice([self.rand_degree(*CFG['x_degree'][0]), self.rand_degree(*CFG['x_degree'][1])])#self.rand_degree(*CFG['x_degree'])\n",
    "            y_degree = np.random.choice([self.rand_degree(*CFG['y_degree'][0]), self.rand_degree(*CFG['y_degree'][1])])#self.rand_degree(*CFG['y_degree'])\n",
    "            z_degree = np.random.choice([self.rand_degree(*CFG['z_degree'][0]), self.rand_degree(*CFG['z_degree'][1])])#self.rand_degree(*CFG['z_degree'])\n",
    "            \n",
    "            if CFG['rand_rotation'] :\n",
    "                points = self.rand_rotation(x_degree, y_degree, z_degree, points, p=0.80)\n",
    "                \n",
    "            else :    \n",
    "                points = self.rotation(x_degree, y_degree, z_degree, points)\n",
    "                \n",
    "            points = self.jittering(points, CFG['jitter'])\n",
    "            points = self.scaling(points, CFG['scale'])\n",
    "            voxel_features, voxel_coords= self.voxelization(points)\n",
    "            label = self.label_list[index]\n",
    "            return voxel_features, voxel_coords, label\n",
    "        \n",
    "        else:\n",
    "            # TTA\n",
    "#             points = self.point_normalize(points, test=True)\n",
    "            points = self.rand_sampling(points, low_rate=0.8, high_rate=0.6)\n",
    "            points= self.trans_axis_range(points, axis=[0,1,2])\n",
    "    \n",
    "#             x_degree = rand_degree(-np.pi/6, -np.pi/4)#self.rand_degree(-np.pi/4, np.pi/4)\n",
    "#             y_degree = rand_degree(-np.pi/2.5, -np.pi/2)#self.rand_degree(-np.pi/4, np.pi/4)\n",
    "#             z_degree = np.random.choice([np.pi/5, -np.pi/5])#self.rand_degree(-np.pi/6, np.pi/6)\n",
    "            \n",
    "#             points = self.rand_rotation(x_degree, y_degree, z_degree, points, p=0.80)\n",
    "            \n",
    "            points = self.jittering(points, (-0.02, 0.02))\n",
    "            points = self.scaling(points, (0.98, 1.02))\n",
    "            voxel_features, voxel_coords= self.voxelization(points)\n",
    "            return voxel_features, voxel_coords\n",
    "    \n",
    "    def rand_rotation(self, x_degree, y_degree, z_degree, point, p=0.5) :\n",
    "        _p = np.random.uniform(0, 1)\n",
    "        if _p < p :\n",
    "            return self.rotation(x_degree, y_degree, z_degree, point)\n",
    "        \n",
    "        else :\n",
    "            return point\n",
    "        \n",
    "    def rand_degree(self, *rotation_range) :\n",
    "        assert len([rotation_range]) != 2, 'expected 2 parameters, but given more or less'\n",
    "        return np.random.uniform(*rotation_range)        \n",
    "    \n",
    "    def rotation(self, a, b, c, dots):\n",
    "        mx = np.array([[1, 0, 0], [0, np.cos(a), -np.sin(a)], [0, np.sin(a), np.cos(a)]])\n",
    "        my = np.array([[np.cos(b), 0, np.sin(b)], [0, 1, 0], [-np.sin(b), 0, np.cos(b)]])\n",
    "        mz = np.array([[np.cos(c), -np.sin(c), 0], [np.sin(c), np.cos(c), 0], [0, 0, 1]])\n",
    "        m = np.dot(np.dot(mx,my),mz)\n",
    "        dots = np.dot(dots, m.T)\n",
    "        return dots\n",
    "    \n",
    "    def jittering(self, point, scale_range) :\n",
    "        point += np.random.uniform(*scale_range, size=point.shape) \n",
    "        return point\n",
    "    \n",
    "    def scaling(self, point, scale_range) :\n",
    "        point *= np.random.uniform(*scale_range)\n",
    "        return point\n",
    "    \n",
    "    def rand_sampling(self, point, low_rate=0.8, high_rate=0.6) :\n",
    "        point_num = point.shape[0]\n",
    "        \n",
    "        if point_num >= 20000 :\n",
    "            sampling_rate=high_rate\n",
    "        else :\n",
    "            sampling_rate=low_rate\n",
    "            \n",
    "        sampling_idx = np.sort(np.random.choice(point_num, int(point_num * sampling_rate) , replace=False))\n",
    "        point = point[sampling_idx, :]\n",
    "        return point\n",
    "    \n",
    "    def trans_axis_range(self, point, axis=[0]) :\n",
    "        # Transform train point range to test point range\n",
    "        point[:, axis] = point[:, axis] / (np.max(np.abs(cfg.train_range[axis])) + 0.1) * (np.min(np.abs(cfg.test_range[axis])) - 0.1)\n",
    "        return point\n",
    "    \n",
    "    def point_normalize(self, point, test=False) :\n",
    "        \n",
    "#         xyzmin = np.min(point, axis=0)\n",
    "#         xyzmax = np.max(point, axis=0)\n",
    "        if not test :\n",
    "            axis=[0, 1, 2]\n",
    "            point[:, axis] = (point[:, axis] + np.abs(cfg.train_range[axis, 0])) / (np.sum(np.abs(cfg.train_range[axis]), 1))\n",
    "            \n",
    "        else :\n",
    "            axis=[1, 2]\n",
    "#             point[:, axis] = (point[:, axis] + np.abs(cfg.test_range[axis, 0])) / (np.sum(np.abs(cfg.test_range[axis]), 1))\n",
    "            # norm test point to train point\n",
    "            point[:, 0] = np.clip(point[:, 0], -0.15, 0.15)\n",
    "            point[:, axis] = point[:, axis] /  np.max(np.abs(cfg.test_range[axis])) * np.max(np.abs(cfg.train_range[axis]))\n",
    "        \n",
    "        return point#, xyzmin, xyzmax\n",
    "    \n",
    "    def grid_cutout(self, points, low=5, high=10) :\n",
    "        num_drop = np.random.randint(low, high)\n",
    "        xyzmax = np.max(points, 0)\n",
    "        xyzmin = np.min(points, 0)\n",
    "\n",
    "        r, s = np.linspace(xyzmin[:], xyzmax[:], num=64, endpoint=False, retstep=True)\n",
    "\n",
    "        x_pick = np.sort(np.random.choice(r[:, 0], num_drop))\n",
    "        y_pick = np.sort(np.random.choice(r[:, 1], num_drop))\n",
    "        z_pick = np.sort(np.random.choice(r[:, 2], num_drop))\n",
    "\n",
    "        for idx, pick in enumerate([x_pick, y_pick, z_pick]) :\n",
    "            cut_list = []\n",
    "            for i in range(num_drop):\n",
    "                r = np.where(((pick[i]< points) & (points < pick[i] + s[idx])))[0]\n",
    "                if r.shape[0] != 0 :\n",
    "                    cut_list.append(r)\n",
    "            \n",
    "            if len(cut_list) != 0 :\n",
    "                cut_list = np.unique(np.concatenate(cut_list))\n",
    "\n",
    "                diff_point = np.setdiff1d(np.arange(points.shape[0]), cut_list)\n",
    "                points = points[diff_point, :] \n",
    "        return points\n",
    "\n",
    "    def voxelization(self, point):# ,xyzmin, xyzmax) :\n",
    "        point_reflectance = np.zeros((point.shape[0],1))\n",
    "        point = np.concatenate((point, point_reflectance), 1)\n",
    "        \n",
    "        voxel_coords = ((point[:, :3] - np.array([cfg.test_range[0][0], cfg.test_range[1][0], cfg.test_range[2][0]])) / \n",
    "                       (cfg.vw, cfg.vh, cfg.vd)).astype(np.int32)\n",
    "\n",
    "        # convert to (D,H,W)\n",
    "        voxel_coords = voxel_coords[:, [2,1,0]]\n",
    "        voxel_coords, inv_ind, voxel_counts = np.unique(voxel_coords, \n",
    "                                                        axis=0, \n",
    "                                                        return_inverse=True, \n",
    "                                                        return_counts=True)\n",
    "        voxel_features = []\n",
    "        for i in range(len(voxel_coords)) :\n",
    "            voxel = np.zeros((cfg.T, 7), dtype=np.float32)\n",
    "            pts = point[inv_ind == i]\n",
    "            \n",
    "            # Random sampling\n",
    "            if voxel_counts[i] > cfg.T :\n",
    "                random_sampling = np.random.randint(0, pts.shape[0], size=cfg.T)\n",
    "                pts = pts[random_sampling, : ]\n",
    "                voxel_counts[i] = cfg.T\n",
    "            \n",
    "            voxel[:pts.shape[0], :] = np.concatenate((pts, pts[:, :3] - np.mean(pts[:, :3], 0)), axis=1)\n",
    "            voxel_features.append(voxel)\n",
    "        return np.array(voxel_features), voxel_coords\n",
    "    \n",
    "    def __len__(self) :\n",
    "        return len(self.id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b1c94",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4003782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "                \n",
    "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "924b6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module) :\n",
    "    def __init__(self, alpha=2, gamma=2, reduction='none') :\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.ce = nn.CrossEntropyLoss(reduction=self.reduction)\n",
    "    \n",
    "    def forward(self, inputs, targets) :\n",
    "        ce_loss = self.ce(inputs, targets)\n",
    "\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        F_loss = self.alpha * ((1-pt)**self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction :\n",
    "            return torch.mean(F_loss)\n",
    "        else :\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f985339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_score(true_labels, model_preds) :\n",
    "    model_preds = model_preds.argmax(1).detach().cpu().numpy().tolist()\n",
    "    true_labels = true_labels.detach().cpu().numpy().tolist()\n",
    "    return accuracy_score(true_labels, model_preds), f1_score(true_labels, model_preds, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95ccf513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_load(model, optimizer, ckpt) :\n",
    "    checkpoint = torch.load(ckpt)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    \n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "#                                                            T_max=CFG['cosine_lr_T_max'], \n",
    "#                                                            eta_min=CFG['cosine_lr_eta_min'])\n",
    "#     scheduler = CosineAnnealingWarmUpRestarts(optimizer, \n",
    "#                                               T_0=7, \n",
    "#                                               T_mult=1, \n",
    "#                                               eta_max=0.001,  \n",
    "#                                               T_up=3, \n",
    "#                                               gamma=0.5)\n",
    "\n",
    "#     optimizer.param_groups[0]['initial_lr'] = CFG['LEARNING_RATE']\n",
    "#     optimizer.param_groups[0]['lr'] = CFG['LEARNING_RATE']\n",
    "#     epoch = CFG['EPOCHS']\n",
    "#     scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: 0.95 ** epoch)\n",
    "    \n",
    "    scheduler = None\n",
    "    return model, optimizer, scheduler, checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2f9f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logging(logger, data, step) :\n",
    "    for i, (k,v) in enumerate(data.items()) :\n",
    "        logger.add_scalar(k, v, step)\n",
    "    #return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a00c5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    \n",
    "    if CFG['reuse'] :\n",
    "        model, optimizer, scheduler, E = weight_load(model, optimizer, CFG['checkpoint'])\n",
    "    else :\n",
    "        E = 0\n",
    "    \n",
    "    # tensorboard\n",
    "    log_writter = SummaryWriter(CFG['LOG'])\n",
    "    \n",
    "    model.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion = FocalLoss(alpha=2, gamma=2)\n",
    "    best_score = 0\n",
    "    for epoch in range(E, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        training_bar = tqdm(iter(train_loader))\n",
    "        batch = 1\n",
    "        \n",
    "        for vf, vc, label in training_bar:\n",
    "                \n",
    "            vf = torch.tensor(vf).to(device)\n",
    "            vc = torch.tensor(vc).to(device)\n",
    "            label = torch.tensor(label, dtype=torch.long).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(vf, vc, batch_size=label.shape[0])\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            \n",
    "            acc, f1 = cal_score(label, output)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "            training_bar.set_postfix({\n",
    "                'Training Loss' : np.mean(train_loss), \n",
    "                'Training ACC' : np.mean(train_acc)})\n",
    "            \n",
    "            \n",
    "            data = {'Training Loss' : loss.item(),\n",
    "                    'Training Accuracy' : acc,\n",
    "                    'Training F1' : f1,\n",
    "                    'Learning Rate' : optimizer.param_groups[0]['lr']}\n",
    "                                    \n",
    "            \n",
    "            logging(log_writter, data, epoch * len(train_loader) + batch)\n",
    "            batch += 1\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        val_loss, val_acc, val_f1 = validation(model, criterion, val_loader, device, log_writter)\n",
    "        \n",
    "        data = {'Validation Loss' :  val_loss,\n",
    "                'Validation Accuracy' : val_acc,\n",
    "                'Validation F1' : val_f1}                               \n",
    "        logging(log_writter, data, epoch)\n",
    "        \n",
    "        print(f'Epoch : [{epoch}] Val Loss : [{val_loss}] Val ACC : [{val_acc}] Val F1 : [{val_f1}] ')\n",
    "        \n",
    "        if best_score < val_acc:\n",
    "            best_score = val_acc\n",
    "            torch.save({\n",
    "                    \"epoch\" : epoch,\n",
    "                    \"model_state_dict\" : model.state_dict(),\n",
    "                    \"optimizer_state_dict\" : optimizer.state_dict()\n",
    "                }, './ckpt/'+str(epoch)+'E-val'+str(best_score)+'-'+CFG['output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42e8fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device, log_writter):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    model_preds = []\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for vf, vc, label in tqdm(iter(val_loader)):\n",
    "            vf = torch.tensor(vf).to(device)\n",
    "            vc = torch.tensor(vc).to(device)\n",
    "            label = torch.tensor(label, dtype=torch.long).to(device)\n",
    "\n",
    "            model_pred = model(vf, vc, batch_size=label.shape[0])\n",
    "            loss = criterion(model_pred, label)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            model_preds += model_pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "            true_labels += label.detach().cpu().numpy().tolist()            \n",
    "            \n",
    "    return np.mean(val_loss), accuracy_score(true_labels, model_preds), f1_score(true_labels, model_preds, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19c53a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':10,\n",
    "    'LEARNING_RATE':1e-4,\n",
    "    'BATCH_SIZE':32,\n",
    "    \n",
    "#     'cosine_lr_eta_min' : 1e-4,\n",
    "#     'cosine_lr_T_max' : 10,\n",
    "#     'weight_decay' : 0.01,\n",
    "    'rand_rotation' : True,\n",
    "    'x_degree' : [[-np.pi/6, -np.pi/4], [np.pi/6, np.pi/4]],\n",
    "    'y_degree' : [[-np.pi/3, -np.pi/2], [np.pi/3, np.pi/2]],\n",
    "    'z_degree' : [[-np.pi/6, -np.pi/4], [np.pi/6, np.pi/4]],\n",
    "    'jitter' : (-0.02, 0.02),\n",
    "    'scale' : (0.98, 1.02),\n",
    "    \n",
    "    'LOG' : \"./tensorboard/xyz_divided_rotation/43x32y43z\",   \n",
    "    'output' : 'focal-43x32y43z-voxelnet.pth',\n",
    "    \n",
    "    'checkpoint' : './ckpt/40E-val0.9645-focal2-y90rotate-voxelnet.pth',\n",
    "    'reuse' : False\n",
    "}\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d61e020",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_save_name = datetime.now().strftime('%Y_%m_%d-%H_%M_%S') + '_' +CFG['output'].split('.')[0]\n",
    "with open(f\"./ckpt/{cfg_save_name}.json\", 'w') as f :\n",
    "    json.dump(CFG, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a15cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv('./data/train.csv')\n",
    "all_points = h5py.File('./data/train.h5', 'r')\n",
    "\n",
    "train_df = all_df.iloc[:int(len(all_df)*0.8)]\n",
    "val_df = all_df.iloc[int(len(all_df)*0.8):]\n",
    "\n",
    "train_dataset = VoxelDataset(train_df['ID'].values, train_df['label'].values, all_points)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], collate_fn=detection_collate, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = VoxelDataset(val_df['ID'].values, val_df['label'].values, all_points)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], collate_fn=detection_collate, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cc765ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7876e6f2176a48e7bdbe4175b7af2a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8fbcb442894304a0077de800a18de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [0] Val Loss : [0.5135340788446295] Val ACC : [0.8514] Val F1 : [0.8514] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fd261c4c284dceb2e015ac743741f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70391d1c9ee442ea6796b7336680a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [1] Val Loss : [0.2522198849033338] Val ACC : [0.921] Val F1 : [0.9209999999999999] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e326074995d14203bf20d19346c77809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e92a45941044169c7e593f50365ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [2] Val Loss : [0.24027022073110835] Val ACC : [0.9188] Val F1 : [0.9188000000000001] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf06f08d7ef417c872e85f1ccdf2574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e56eba05cb4aa0a53eef391a468790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [3] Val Loss : [0.16831098080273516] Val ACC : [0.9417] Val F1 : [0.9417] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30eb3a360c24183815b1a368f84407e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042083482d4143cba07963ad0278e734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [4] Val Loss : [0.16403519894299914] Val ACC : [0.946] Val F1 : [0.946] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4e19d960da459eb59fc2f86b37e752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c2a5e8278f42fe8e44d217c3185e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [5] Val Loss : [0.1399655194801335] Val ACC : [0.9522] Val F1 : [0.9522] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0bf12002b8240f7ae09f096a913f7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69662d084d5044ba8eb38ab2665a230a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [6] Val Loss : [0.12313865166538344] Val ACC : [0.9572] Val F1 : [0.9572] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfdf25b9e6634ee6b564670adbd0a2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9bebfc2b6a24e11a78f460e132bc569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [7] Val Loss : [0.11531957054544931] Val ACC : [0.9618] Val F1 : [0.9618] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aba02d667b64e5abe380a245762ffd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235fc606e25f417eb359f59e47e4152c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [8] Val Loss : [0.10254396937791996] Val ACC : [0.9665] Val F1 : [0.9665] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee791de46e048c0bf44145f49f469cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6359d470fc764b7bac974b7cb27d53b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [9] Val Loss : [0.15258736369094422] Val ACC : [0.9475] Val F1 : [0.9475] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb94ffd729f94f38899d017d992d6b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6e4b6f9fc24cd9ad3eb720c6e816a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [10] Val Loss : [0.10639761577393063] Val ACC : [0.9645] Val F1 : [0.9645] \n"
     ]
    }
   ],
   "source": [
    "model = VoxelNet().to(device)\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), \n",
    "                             lr = CFG[\"LEARNING_RATE\"])\n",
    "#                              weight_decay=CFG['weight_decay'])\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "#                                                       T_max=CFG['cosine_lr_T_max'], \n",
    "#                                                       eta_min=CFG['cosine_lr_eta_min'])\n",
    "\n",
    "# epoch = CFG['EPOCHS']\n",
    "# optimizer.param_groups[0]['initial_lr'] = CFG['LEARNING_RATE']\n",
    "# optimizer.param_groups[0]['lr'] = CFG['LEARNING_RATE']\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: 0.95 ** epoch)\n",
    "\n",
    "# scheduler = CosineAnnealingWarmUpRestarts(optimizer, \n",
    "#                                           T_0=7, \n",
    "#                                           T_mult=1, \n",
    "#                                           eta_max=0.001,  \n",
    "#                                           T_up=3, \n",
    "#                                           gamma=0.5)\n",
    "scheduler = None\n",
    "\n",
    "train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0707111",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f614852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader, device, dim_changer=None):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    model_preds = []\n",
    "    with torch.no_grad():\n",
    "        for vf, vc, batch_size in tqdm(iter(test_loader)):\n",
    "            vf = torch.tensor(vf).to(device)\n",
    "            vc = torch.tensor(vc).to(device)\n",
    "\n",
    "            batch_pred = model(vf, vc, batch_size=batch_size)\n",
    "            \n",
    "            model_preds += batch_pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "    \n",
    "    return model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7a74e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('./data/sample_submission.csv')\n",
    "test_points = h5py.File('./data/test.h5', 'r')\n",
    "\n",
    "test_dataset = VoxelDataset(test_df['ID'].values, None, test_points)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'],  collate_fn=detection_collate, shuffle=False, num_workers=0)\n",
    "\n",
    "checkpoint = torch.load('./ckpt/49E-val0.9712-focal2-y90rotate-voxelnet.pth')\n",
    "model = VoxelNet().to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b139487",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de520b1e8f0647019b8ef341e5bce38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = predict(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0febf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['label'] = preds\n",
    "\n",
    "test_df.to_csv('./submission/tta-49E-val0.9712-focal2-y90rotate-voxelnet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6955f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
