{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "415b2e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import timm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from easydict import EasyDict\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670ff275",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fb8633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_list, label_list=None, transforms=None) :\n",
    "        self.img_list = img_list\n",
    "        self.label_list = label_list\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_list[idx]\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        if self.transforms:            \n",
    "            img = self.transforms(image=img)['image']\n",
    "        \n",
    "        # training\n",
    "        if self.label_list is not None :\n",
    "            label = self.label_list[idx]\n",
    "            return img, torch.tensor(label)\n",
    "        \n",
    "        # test\n",
    "        else :\n",
    "            return img\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556f6b0",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa29401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, model_name) :\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name=model_name, num_classes=10, pretrained=True)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        x = self.model(x)            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542f5f91",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e896f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int32(W * cut_rat)\n",
    "    cut_h = np.int32(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def cutmix(imgs, labels):\n",
    "    lam = np.random.beta(1.0, 1.0)\n",
    "    rand_index = torch.randperm(imgs.size()[0]).cuda()\n",
    "    target_a = labels\n",
    "    target_b = labels[rand_index]\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(imgs.size(), lam)\n",
    "    imgs[:, :, bbx1:bbx2, bby1:bby2] = imgs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (imgs.size()[-1] * imgs.size()[-2]))\n",
    "\n",
    "    return imgs, lam, target_a, target_b\n",
    "\n",
    "def mixup(imgs, labels) :\n",
    "    lam = np.random.beta(1.0, 1.0)\n",
    "    rand_index = torch.randperm(imgs.size()[0]).cuda()\n",
    "    mixed_imgs = lam * imgs + (1 - lam) * imgs[rand_index, :]\n",
    "    target_a, target_b = labels, labels[rand_index]\n",
    "    \n",
    "    return mixed_imgs, lam, target_a, target_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2912f483",
   "metadata": {},
   "source": [
    "# Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acded6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(true_labels, model_preds) :\n",
    "    model_preds = model_preds.argmax(1).detach().cpu().numpy().tolist()\n",
    "    true_labels = true_labels.detach().cpu().numpy().tolist()\n",
    "    return accuracy_score(true_labels, model_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c008160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_parser(data_path, div, training=True) :\n",
    "    path = sorted(glob(data_path), key = lambda x : int(x.split('\\\\')[-1].split('.')[0]))   \n",
    "    \n",
    "    if training:    \n",
    "        return path[:div], path[div:]    \n",
    "    else :\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ae9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_parser(grid_shuffle_p=0.8) :\n",
    "    return A.Compose([\n",
    "        A.Rotate(limit=(45), p=1),\n",
    "        A.RandomGridShuffle(p=grid_shuffle_p, grid=(2,2)),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1ae14da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_label_dataset(df_path, div=0.8, grid_shuffle_p=0.8, training=True) :\n",
    "    all_df = pd.read_csv(df_path)\n",
    "    transform = transform_parser(grid_shuffle_p=grid_shuffle_p)\n",
    "    \n",
    "    if training :\n",
    "        train_df = all_df.iloc[:int(len(all_df)*div)]\n",
    "        val_df = all_df.iloc[int(len(all_df)*div):]\n",
    "        \n",
    "        train_img, valid_img = img_parser(CFG['DATA_PATH'], int(len(all_df)*div), training)\n",
    "        return (train_img, valid_img), (train_df['label'].values, val_df['label'].values), transform\n",
    "    \n",
    "    else :\n",
    "        img = img_parser(CFG['DATA_PATH'], div=None, training=training)\n",
    "        return np.array(img), all_df['label'].values, transform   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ba9e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_dataload(img_set, label_set, transform, shuffle) :\n",
    "    ds = CustomDataset(img_set, label_set, transform)\n",
    "    dl = DataLoader(ds, batch_size = CFG['BATCH_SIZE'], shuffle=shuffle, num_workers=0)    \n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67638670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_valid_dataload(img_set, label_set, transform) :   \n",
    "    train_loader = custom_dataload(img_set[0], label_set[0], transform, True)\n",
    "    val_loader = custom_dataload(img_set[1], label_set[1], transform, False)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ff4e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "CFG = {\n",
    "    'EPOCHS':70,\n",
    "    'LEARNING_RATE':1e-4,\n",
    "    'BATCH_SIZE':64,\n",
    "    'DATA_PATH' : './data/img/224img_test/*',\n",
    "    \n",
    "    'output' : 'scratch-weigt_freeze10E-mixup25E-grid_shuffle35E-focal-effib0.pth',\n",
    "    'LOG' : \"./tensorboard/PCA_img/weigt_freeze10E-mixup25E-grid_shuffle35E\",   \n",
    "    'reg_step' : [36, 61],\n",
    "    'focal_gamma' : 2,\n",
    "    'focal_alpha' : 2,\n",
    "    \n",
    "    'kfold' : 5,\n",
    "    \n",
    "    'model_name' : 'efficientnet_b0',\n",
    "    \n",
    "    'reuse': False,\n",
    "    'clssifier_freeze' : False,\n",
    "    'checkpoint' : './ckpt/23E-val0.9369-scratch-weigt_freeze5E-mixup15E-grid_shuffle25E-focal-effib0.pth',\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c6a42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict_hardVoting(models, test_loader, device):\n",
    "    model_preds = []\n",
    "    with torch.no_grad():\n",
    "        for img in tqdm(iter(test_loader)):\n",
    "            img = img.to(device)\n",
    "            \n",
    "            batch_len=[i for i in range(CFG['BATCH_SIZE'])]\n",
    "            batch_preds_score = []\n",
    "            batch_preds_label = []\n",
    "            for model in models :\n",
    "                model.eval()\n",
    "                pred = model(img)\n",
    "                pred = pred.max(1)\n",
    "                batch_preds_score.append(pred[0].detach().cpu().numpy())\n",
    "                batch_preds_label.append(pred[1].detach().cpu().numpy())\n",
    "\n",
    "            batch_preds_label = np.array(batch_preds_label)\n",
    "            \n",
    "            best_score_ind = np.argmax(batch_preds_score, axis=0)\n",
    "            model_preds += batch_preds_label[best_score_ind[batch_len], batch_len].tolist()\n",
    "\n",
    "    return model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "743b2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict_softVoting(models, test_loader, device):\n",
    "    model_preds = []\n",
    "    with torch.no_grad():\n",
    "        for img in tqdm(iter(test_loader)):\n",
    "            img = img.to(device)\n",
    "            \n",
    "            batch_len=[i for i in range(CFG['BATCH_SIZE'])]\n",
    "            batch_preds_score = []\n",
    "            for model in models :\n",
    "                model.eval()\n",
    "                pred = model(img)\n",
    "                batch_preds_score.append(pred.detach().cpu().numpy())\n",
    "\n",
    "            batch_preds_score = np.mean(np.array(batch_preds_score), axis=0)\n",
    "#             print(\"batch_preds_score.shape : \", batch_preds_score.shape)\n",
    "#             best_score_ind = np.argmax(batch_preds_score, axis=1)\n",
    "#             print(\"best_score_ind.shape : \",best_score_ind.shape)\n",
    "            model_preds += batch_preds_score.tolist()\n",
    "            \n",
    "    return model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3de3ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/sample_submission.csv')\n",
    "\n",
    "img_set, _, transform = image_label_dataset(df_path='./data/train.csv', div=0.8, grid_shuffle_p=0, training=False)\n",
    "\n",
    "test_loader = custom_dataload(img_set, None, transform, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbc29569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "checkpoint_1 = torch.load('./ckpt/69E-val0.9611-4fold_3fold_2fold_1fold_0fold_scratch-weigt_freeze10E-mixup25E-grid_shuffle35E-focal-effib0.pth')\n",
    "checkpoint_2 = torch.load('./ckpt/69E-val0.9571-3fold_2fold_1fold_0fold_scratch-weigt_freeze10E-mixup25E-grid_shuffle35E-focal-effib0.pth')\n",
    "checkpoint_3 = torch.load('./ckpt/65E-val0.9589-2fold_1fold_0fold_scratch-weigt_freeze10E-mixup25E-grid_shuffle35E-focal-effib0.pth')\n",
    "checkpoint_4 = torch.load('./ckpt/69E-val0.9545-1fold_0fold_scratch-weigt_freeze10E-mixup25E-grid_shuffle35E-focal-effib0.pth')\n",
    "checkpoint_5 = torch.load('./ckpt/67E-val0.9615-0fold_scratch-weigt_freeze10E-mixup25E-grid_shuffle35E-focal-effib0.pth')\n",
    "\n",
    "model_1 = CNN(CFG['model_name']).to(device)\n",
    "model_2 = CNN(CFG['model_name']).to(device)\n",
    "model_3 = CNN(CFG['model_name']).to(device)\n",
    "model_4 = CNN(CFG['model_name']).to(device)\n",
    "model_5 = CNN(CFG['model_name']).to(device)\n",
    "\n",
    "model_1.load_state_dict(checkpoint_1['model_state_dict'])\n",
    "model_2.load_state_dict(checkpoint_2['model_state_dict'])\n",
    "model_3.load_state_dict(checkpoint_3['model_state_dict'])\n",
    "model_4.load_state_dict(checkpoint_4['model_state_dict'])\n",
    "model_5.load_state_dict(checkpoint_5['model_state_dict'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "746e2138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 625/625 [03:47<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = ensemble_predict_hardVoting([model_1, model_2, model_3, model_4, model_5], test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1481035f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 625/625 [04:17<00:00,  2.42it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = ensemble_predict_softVoting([model_1, model_2, model_3, model_4, model_5], test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f619004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['label'] = preds\n",
    "\n",
    "test_df.to_csv('./submission/ensemble_softVoting-5fold-4fval0.9611-3fval0.9571-2fval0.9589-1fval0.9545-0fval0.9615.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "238e4afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_preds = np.array(preds)\n",
    "np_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "254c8b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"ensemble_softVoting-5fold.npy\", np_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d173c935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
