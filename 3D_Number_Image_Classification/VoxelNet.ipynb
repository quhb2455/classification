{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32523444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py # .h5 파일을 읽기 위한 패키지\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from cfg.voxelnet_cfg import config as cfg \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67cb3016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "# conv2d + bn + relu\n",
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k,s,p, activation=True, batch_norm=True):\n",
    "        super(Conv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=k,stride=s,padding=p)\n",
    "        \n",
    "        if batch_norm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "        else:\n",
    "            self.bn = None\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x=self.bn(x)\n",
    "        if self.activation:\n",
    "            return F.relu(x,inplace=True)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "# conv3d + bn + relu\n",
    "class Conv3d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, k, s, p, batch_norm=True):\n",
    "        super(Conv3d, self).__init__()\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=k, stride=s, padding=p)\n",
    "        if batch_norm:\n",
    "            self.bn = nn.BatchNorm3d(out_channels)\n",
    "        else:\n",
    "            self.bn = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "\n",
    "        return F.relu(x, inplace=True)\n",
    "\n",
    "# Fully Connected Network\n",
    "class FCN(nn.Module):\n",
    "\n",
    "    def __init__(self,cin,cout):\n",
    "        super(FCN, self).__init__()\n",
    "        self.cout = cout\n",
    "        self.linear = nn.Linear(cin, cout)\n",
    "        self.bn = nn.BatchNorm1d(cout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # KK is the stacked k across batch\n",
    "#         print(\"1. FCN x.shape : \",x.shape)\n",
    "        kk, t, _ = x.shape\n",
    "#         N, D, H, W = x.shape\n",
    "\n",
    "        x = self.linear(x.view(kk * t, -1))\n",
    "#         x = self.linear(x.view(N*D*H, -1))\n",
    "        \n",
    "\n",
    "#         print(\"2. FCN x.shape : \", x.shape)\n",
    "        x = F.relu(self.bn(x))\n",
    "        \n",
    "        return x.view(kk,t, -1)\n",
    "#         return x.view(N, -1, H, x.shape[1])\n",
    "\n",
    "# Voxel Feature Encoding layer\n",
    "class VFE(nn.Module):\n",
    "\n",
    "    def __init__(self,cin,cout):\n",
    "        super(VFE, self).__init__()\n",
    "        assert cout % 2 == 0\n",
    "        self.units = cout // 2\n",
    "        self.fcn = FCN(cin,self.units)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # point-wise feauture\n",
    "        pwf = self.fcn(x)\n",
    "        \n",
    "        #locally aggregated feature\n",
    "#         print(\"1. VEF pwf.shape : \", pwf.shape)\n",
    "#         print(\"1. VEF torch max pwf shape : \",  torch.max(pwf,2)[0].shape)\n",
    "#         print(\"1. VEF torch max pwf unsq(1) shape : \",  torch.max(pwf,2)[0].unsqueeze(2).shape)\n",
    "#         print(\"1. VEF torch max pwf unsq(1) repeat(1,35,1) shape : \",  torch.max(pwf,2)[0].unsqueeze(2).repeat(1,1,cfg.T,1).shape)\n",
    "        laf = torch.max(pwf,1)[0].unsqueeze(1).repeat(1,cfg.T,1)\n",
    "#         laf = torch.max(pwf,2)[0].unsqueeze(2).repeat(1,1,cfg.T,1)\n",
    "#         print(\"2. VEF laf.shape : \", laf.shape)\n",
    "        \n",
    "        # point-wise concat feature\n",
    "        pwcf = torch.cat((pwf,laf),dim=2)\n",
    "#         pwcf = torch.cat((pwf,laf),dim=3)\n",
    "#         print(\"3. VEF pwcf.shape : \", pwcf.shape)\n",
    "\n",
    "        # apply mask\n",
    "#         print(\"4. VEF mask shape : \", mask.shape)\n",
    "#         print(\"4. VEF mask unsq(2) shape : \", mask.unsqueeze(2).shape)\n",
    "#         print(\"4. VEF mask unsq(2) repeat(1,1, ??) shape : \", mask.unsqueeze(2).repeat(1, 1, self.units * 2).shape)\n",
    "        mask = mask.unsqueeze(2).repeat(1, 1, self.units * 2)\n",
    "#         mask = mask.unsqueeze(3).repeat(1, 1, 1, self.units * 2)\n",
    "#         print(\"4. VEF mask.shape : \", mask.shape)\n",
    "        pwcf = pwcf * mask.float()\n",
    "\n",
    "        return pwcf\n",
    "\n",
    "# Stacked Voxel Feature Encoding\n",
    "class SVFE(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SVFE, self).__init__()\n",
    "#         self.vfe_1 = VFE(6,32)\n",
    "        self.vfe_1 = VFE(7,32)\n",
    "        self.vfe_2 = VFE(32,128)\n",
    "        self.fcn = FCN(128,128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mask = torch.ne(torch.max(x,2)[0], 0)\n",
    "#         print(\"SVFE Mask value : \", mask)\n",
    "        x = self.vfe_1(x, mask)\n",
    "        x = self.vfe_2(x, mask)\n",
    "        x = self.fcn(x)\n",
    "#         print(\"SVFE x.shape: \",x.shape)\n",
    "        # element-wise max pooling\n",
    "        x = torch.max(x,1)[0]\n",
    "        return x\n",
    "\n",
    "# Convolutional Middle Layer\n",
    "class CML(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CML, self).__init__()\n",
    "        self.conv3d_1 = Conv3d(128, 64, 3, s=(2, 1, 1), p=(1, 1, 1))\n",
    "        self.conv3d_2 = Conv3d(64, 64, 3, s=(1, 1, 1), p=(0, 1, 1))\n",
    "        self.conv3d_3 = Conv3d(64, 64, 3, s=(2, 1, 1), p=(1, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv3d_1(x)\n",
    "        x = self.conv3d_2(x)\n",
    "        x = self.conv3d_3(x)\n",
    "        return x\n",
    "\n",
    "class RPN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RPN, self).__init__()\n",
    "        self.block_1 = [Conv2d(192, 192, 3, 2, 1)]\n",
    "        self.block_1 += [Conv2d(192, 192, 3, 1, 1) for _ in range(3)]\n",
    "        self.block_1 = nn.Sequential(*self.block_1)\n",
    "\n",
    "        self.block_2 = [Conv2d(192, 192, 3, 2, 1)]\n",
    "        self.block_2 += [Conv2d(192, 192, 3, 1, 1) for _ in range(5)]\n",
    "        self.block_2 = nn.Sequential(*self.block_2)\n",
    "\n",
    "        self.block_3 = [Conv2d(192, 256, 3, 2, 1)]\n",
    "        self.block_3 += [nn.Conv2d(256, 256, 3, 1, 1) for _ in range(5)]\n",
    "        self.block_3 = nn.Sequential(*self.block_3)\n",
    "\n",
    "        self.deconv_1 = nn.Sequential(nn.ConvTranspose2d(256, 256, 4, 4, 0),nn.BatchNorm2d(256))\n",
    "        self.deconv_2 = nn.Sequential(nn.ConvTranspose2d(192, 256, 2, 2, 0),nn.BatchNorm2d(256))\n",
    "        self.deconv_3 = nn.Sequential(nn.ConvTranspose2d(192, 256, 1, 1, 0),nn.BatchNorm2d(256))\n",
    "\n",
    "        self.score_head = Conv2d(768, 10, 1, 1, 0, activation=False, batch_norm=False)\n",
    "#         self.reg_head = Conv2d(768, 7 * cfg.anchors_per_position, 1, 1, 0, activation=False, batch_norm=False)\n",
    "        self.cls = Classifier(cin=640)\n",
    "    def forward(self,x, batch_size):\n",
    "        x = self.block_1(x)\n",
    "        x_skip_1 = x\n",
    "        x = self.block_2(x)\n",
    "        x_skip_2 = x\n",
    "        x = self.block_3(x)\n",
    "#         print(\"RPN block_3 x.shape : \",x.shape)\n",
    "        x_0 = self.deconv_1(x)\n",
    "#         print(\"RPN deconv_1 x_0.shape : \",x_0.shape)\n",
    "        x_1 = self.deconv_2(x_skip_2)\n",
    "#         print(\"RPN deconv_2 x_1.shape : \",x_1.shape)\n",
    "        x_2 = self.deconv_3(x_skip_1)\n",
    "#         print(\"RPN deconv_3 x_2.shape : \",x_2.shape)\n",
    "        x = torch.cat((x_0,x_1,x_2),1)\n",
    "#         print(\"RPN cat x.shape : \",x.shape)\n",
    "        x = self.score_head(x)\n",
    "#         print(\"RPN score_head x.shape : \",x.shape)\n",
    "        x = self.cls(x.view(batch_size, -1))\n",
    "#         print(\"RPN cls x.shape : \",x.shape)\n",
    "        return x #self.score_head(x),self.reg_head(x)\n",
    "    \n",
    "    \n",
    "# classifier\n",
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, cin, num_classes=10, dropout_rate=0.5):\n",
    "        super(Classifier, self).__init__()  \n",
    "        \n",
    "        self.linear_1 = nn.Sequential(\n",
    "            nn.Linear(cin, cin//2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate, inplace=False)\n",
    "        )\n",
    "        \n",
    "        self.linear_2 = nn.Sequential(\n",
    "            nn.Linear(cin//2, cin//4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate, inplace=False)\n",
    "        )\n",
    "        \n",
    "        self.linear_3 = nn.Linear(cin//4, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # KK is the stacked k across batch\n",
    "#         print(\"classifier x.shape : \", x.shape)\n",
    "#         print(\"classifier x.view.shape : \", x.view(2,-1).shape)\n",
    "        b, f, *_ = x.shape\n",
    "        x = self.linear_1(x.view(b, -1))\n",
    "        x = self.linear_2(x)\n",
    "        x = self.linear_3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class VoxelNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VoxelNet, self).__init__()\n",
    "        self.svfe = SVFE()\n",
    "        self.cml = CML()\n",
    "        self.rpn = RPN()\n",
    "        self.cls = Classifier(cin=64)\n",
    "        \n",
    "    def voxel_indexing(self, sparse_features, coords, batch_size):\n",
    "        dim = sparse_features.shape[-1]\n",
    "#         print(\"sparse_features.shape : \", sparse_features.shape)\n",
    "        \n",
    "        coords= coords.type(torch.long)\n",
    "#         print(\"coords.shape : \", coords.shape)\n",
    "#         print(\"coords values [:,0]: \",coords[:,0])\n",
    "        dense_feature = torch.zeros(dim, batch_size, cfg.D, cfg.H, cfg.W).to(cfg.device)\n",
    "#         print(\"dense_feature.shape : \", dense_feature.shape)\n",
    "        dense_feature[:, coords[:,0], coords[:,1], coords[:,2], coords[:,3]]= sparse_features.transpose(0,1)\n",
    "#         dense_feature[:, coords[:,0], coords[:,1], coords[:,2]]= sparse_features\n",
    "        return dense_feature.transpose(0, 1)\n",
    "\n",
    "    def forward(self, voxel_features, voxel_coords, batch_size):\n",
    "\n",
    "        # feature learning network\n",
    "        vwfs = self.svfe(voxel_features)\n",
    "        vwfs = self.voxel_indexing(vwfs,voxel_coords, batch_size)\n",
    "\n",
    "        # convolutional middle network\n",
    "        cml_out = self.cml(vwfs)\n",
    "        cml_out = cml_out.view(batch_size, -1,cfg.H, cfg.W)\n",
    "#         print(\"cml_out.shape : \", cml_out.shape)\n",
    "\n",
    "        # region proposal network\n",
    "        score = self.rpn(cml_out, batch_size)\n",
    "        # merge the depth and feature dim into one, output probability score map and regression map\n",
    "        # psm,rm = self.rpn(cml_out.view(cfg.N,-1,cfg.H, cfg.W))\n",
    "        \n",
    "        # classifier\n",
    "#         print(\"score shape : \", score)\n",
    "\n",
    "        return score #psm, rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c7a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_collate(batch):\n",
    "    voxel_features = []\n",
    "    voxel_coords = []\n",
    "    train = True if len(batch[0]) == 3 else False\n",
    "    \n",
    "    for i, sample in enumerate(batch):\n",
    "        voxel_features.append(sample[0])\n",
    "        voxel_coords.append(np.pad(sample[1], ((0,0), (1,0)), mode='constant', constant_values=i))\n",
    "\n",
    "    if train :\n",
    "        return np.concatenate(voxel_features), np.concatenate(voxel_coords), np.array(batch)[:, 2].astype(np.long)\n",
    "    else :\n",
    "        return np.concatenate(voxel_features), np.concatenate(voxel_coords), len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b2ed515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def detection_collate(batch):\n",
    "#     voxel_features = []\n",
    "#     voxel_coords = []\n",
    "#     label = []\n",
    "#     shapes = [s[0].shape[0] for s in batch]\n",
    "#     ind_max = np.argmax(shapes)\n",
    "#     ind_min = np.argmin(shapes)\n",
    "#     diff_shapes = shapes[ind_max] - shapes[ind_min]\n",
    "    \n",
    "#     for i, sample in enumerate(batch):\n",
    "\n",
    "#         if ind_max == i :\n",
    "#             voxel_features.append(sample[0])\n",
    "#             voxel_coords.append(sample[1])\n",
    "#         else :\n",
    "#             voxel_features.append(F.pad(sample[0], (0,0,0,0,diff_shapes,0), \"constant\", 0))\n",
    "\n",
    "# #         voxel_coords.append(\n",
    "# #             F.pad(sample[1], (0, 0, 1, 0), mode='constant', value=i))\n",
    "        \n",
    "#             voxel_coords.append(F.pad(sample[1], (0,0,diff_shapes,0), \"constant\", 0))\n",
    "        \n",
    "#         label.append(sample[2])\n",
    "\n",
    "#     return torch.stack(voxel_features, dim=0), torch.stack(voxel_coords, dim=0), torch.Tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e73538de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxelDataset(Dataset) :\n",
    "    def __init__(self, id_list, label_list, point_list) :\n",
    "        self.id_list = id_list\n",
    "        self.label_list = label_list\n",
    "        self.point_list = point_list\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        image_id = self.id_list[index]\n",
    "        points= self.point_list[str(image_id)][:]\n",
    "        \n",
    "#         rand_degree = np.random.choice([-np.pi/12, -np.pi/8, -np.pi/6, -np.pi/4, -np.pi/3], 3)\n",
    "#         rotated_points = self.rotate(rand_degree[0], rand_degree[1], rand_degree[2], points)\n",
    "#         voxel_features, voxel_coords= self.voxelization(rotated_points)\n",
    "\n",
    "        if self.label_list is not None:\n",
    "            points= self.trans_axis_range(points, axis=[0,1,2])\n",
    "            \n",
    "            x_degree = self.rand_degree(-np.pi/2, np.pi/2)\n",
    "            y_degree = self.rand_degree(-np.pi/2, np.pi/2)\n",
    "            z_degree = self.rand_degree(-np.pi/6, np.pi/6)\n",
    "        \n",
    "            points = self.rotate(x_degree, y_degree, z_degree, points)\n",
    "            points = self.jittering(points, (-0.02, 0.02))\n",
    "            points = self.scaling(points, (0.98, 1.02))\n",
    "            \n",
    "            voxel_features, voxel_coords= self.voxelization(points)\n",
    "            label = self.label_list[index]\n",
    "            return voxel_features, voxel_coords, label\n",
    "        else:\n",
    "            voxel_features, voxel_coords= self.voxelization(points)\n",
    "            return voxel_features, voxel_coords\n",
    "        \n",
    "    def rotate(self, a, b, c, dots):\n",
    "        mx = np.array([[1, 0, 0], [0, np.cos(a), -np.sin(a)], [0, np.sin(a), np.cos(a)]])\n",
    "        my = np.array([[np.cos(b), 0, np.sin(b)], [0, 1, 0], [-np.sin(b), 0, np.cos(b)]])\n",
    "        mz = np.array([[np.cos(c), -np.sin(c), 0], [np.sin(c), np.cos(c), 0], [0, 0, 1]])\n",
    "        m = np.dot(np.dot(mx,my),mz)\n",
    "        dots = np.dot(dots, m.T)\n",
    "        return dots\n",
    "    \n",
    "    def jittering(self, point, scale_range) :\n",
    "        point += np.random.uniform(*scale_range, size=point.shape) \n",
    "        return point\n",
    "    \n",
    "    def scaling(self, point, scale_range) :\n",
    "        point *= np.random.uniform(*scale_range)\n",
    "        return point\n",
    "    \n",
    "    def rand_degree(self, *rotation_range) :\n",
    "        return np.random.uniform(*rotation_range)\n",
    "    \n",
    "    def trans_axis_range(self, point, axis=[0]) :\n",
    "        # Transform train point range to test point range\n",
    "#         for ax in axis :\n",
    "#             point[:, ax] = point[:, ax] / (np.max(np.abs(cfg.train_range[ax])) + 0.1) * (np.min(np.abs(cfg.test_range[ax])) - 0.1)\n",
    "        point[:, axis] = point[:, axis] / (np.max(np.abs(cfg.train_range[axis])) + 0.1) * (np.min(np.abs(cfg.test_range[axis])) - 0.1)\n",
    "\n",
    "        return point\n",
    "    \n",
    "    def voxelization(self, point) :\n",
    "        point_reflectance = np.zeros((point.shape[0],1))\n",
    "        point = np.concatenate((point, point_reflectance), 1)\n",
    "        \n",
    "        voxel_coords = ((point[:, :3] - np.array([cfg.test_range[0][0], cfg.test_range[1][0], cfg.test_range[2][0]])) / \n",
    "                       (cfg.vw, cfg.vh, cfg.vd)).astype(np.int32)\n",
    "\n",
    "        # convert to (D,H,W)\n",
    "        voxel_coords = voxel_coords[:, [2,1,0]]\n",
    "        voxel_coords, inv_ind, voxel_counts = np.unique(voxel_coords, \n",
    "                                                        axis=0, \n",
    "                                                        return_inverse=True, \n",
    "                                                        return_counts=True)\n",
    "        voxel_features = []\n",
    "        for i in range(len(voxel_coords)) :\n",
    "            voxel = np.zeros((cfg.T, 7), dtype=np.float32)\n",
    "            pts = point[inv_ind == i]\n",
    "            \n",
    "            # Random sampling\n",
    "            if voxel_counts[i] > cfg.T :\n",
    "                random_sampling = np.random.randint(0, pts.shape[0], size=cfg.T)\n",
    "                pts = pts[random_sampling, : ]\n",
    "                voxel_counts[i] = cfg.T\n",
    "            \n",
    "            voxel[:pts.shape[0], :] = np.concatenate((pts, pts[:, :3] - np.mean(pts[:, :3], 0)), axis=1)\n",
    "            voxel_features.append(voxel)\n",
    "\n",
    "        return np.array(voxel_features), voxel_coords\n",
    "    \n",
    "    def __len__(self) :\n",
    "        return len(self.id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b1c94",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f985339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(label,pred) :\n",
    "    model_preds = pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "    true_labels = label.detach().cpu().numpy().tolist()\n",
    "    return accuracy_score(true_labels, model_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95ccf513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_load(model, optim, ckpt) :\n",
    "    checkpoint = torch.load(ckpt)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optim.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, \n",
    "                              T_max=CFG['cosine_lr_T_max'], \n",
    "                              eta_min=CFG['cosine_lr_eta_min'])\n",
    "    \n",
    "    return model, optim, scheduler, checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a00c5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    \n",
    "    if CFG['reuse'] :\n",
    "        model, optimizer, scheduler, E = weight_load(model, optimizer, CFG['checkpoint'])\n",
    "    else :\n",
    "        E = 0\n",
    "    \n",
    "    # tensorboard\n",
    "    log_writter = SummaryWriter(CFG['LOG'])\n",
    "    \n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    best_score = 0\n",
    "    for epoch in range(E, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        training_bar = tqdm(iter(train_loader))\n",
    "        batch = 1\n",
    "        for vf, vc, label in training_bar:\n",
    "                \n",
    "            vf = torch.tensor(vf).to(device)\n",
    "            vc = torch.tensor(vc).to(device)\n",
    "            label = torch.tensor(label, dtype=torch.long).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(vf, vc, batch_size=label.shape[0])\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc = cal_acc(label, output)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "            training_bar.set_postfix({\n",
    "                'Training Loss' : np.mean(train_loss), \n",
    "                'Training ACC' : np.mean(train_acc)})\n",
    "            \n",
    "            \n",
    "            log_writter.add_scalar('Training Loss',\n",
    "                                    loss.item(),\n",
    "                                    epoch * len(train_loader) + batch)\n",
    "            log_writter.add_scalar('Training Accuracy',\n",
    "                                    acc,\n",
    "                                    epoch * len(train_loader) + batch)\n",
    "            batch += 1\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        val_loss, val_acc = validation(model, criterion, val_loader, device, log_writter)\n",
    "        \n",
    "        log_writter.add_scalar('Validation Loss',\n",
    "                                val_loss,\n",
    "                                epoch)\n",
    "        log_writter.add_scalar('Validation Accuracy',\n",
    "                                val_acc,\n",
    "                                epoch)\n",
    "        \n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss)}] Val Loss : [{val_loss}] Val ACC : [{val_acc}]')\n",
    "        \n",
    "        if best_score < val_acc:\n",
    "            best_score = val_acc\n",
    "            torch.save({\n",
    "                    \"epoch\" : epoch,\n",
    "                    \"model_state_dict\" : model.state_dict(),\n",
    "                    \"optimizer_state_dict\" : optimizer.state_dict()\n",
    "                }, './ckpt/'+str(epoch)+'E-val'+str(best_score)+'-'+CFG['output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42e8fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device, log_writter):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    model_preds = []\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for vf, vc, label in tqdm(iter(val_loader)):\n",
    "            vf = torch.tensor(vf).to(device)\n",
    "            vc = torch.tensor(vc).to(device)\n",
    "            label = torch.tensor(label, dtype=torch.long).to(device)\n",
    "\n",
    "            model_pred = model(vf, vc, batch_size=label.shape[0])\n",
    "            loss = criterion(model_pred, label)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "            model_preds += model_pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "            true_labels += label.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            \n",
    "    return np.mean(val_loss), accuracy_score(true_labels, model_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19c53a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':10,\n",
    "    'LEARNING_RATE':1e-3,\n",
    "    'BATCH_SIZE':64,\n",
    "    \n",
    "    'cosine_lr_eta_min' : 1e-4,\n",
    "    'cosine_lr_T_max' : 5,\n",
    "    \n",
    "    'LOG' : \"./tensorboard\",   \n",
    "    'output' : 'jitter-scale-axis-voxelnet.pth',\n",
    "    \n",
    "    'checkpoint' : './ckpt/15E-val0.9366-lr_1e21e3-xaxis_reg-voxelnet.pth',\n",
    "    'reuse' : False\n",
    "}\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a15cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv('./data/train.csv')\n",
    "all_points = h5py.File('./data/train.h5', 'r')\n",
    "\n",
    "train_df = all_df.iloc[:int(len(all_df)*0.8)]\n",
    "val_df = all_df.iloc[int(len(all_df)*0.8):]\n",
    "\n",
    "train_dataset = VoxelDataset(train_df['ID'].values, train_df['label'].values, all_points)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], collate_fn=detection_collate, shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = VoxelDataset(val_df['ID'].values, val_df['label'].values, all_points)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], collate_fn=detection_collate, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc765ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8037d27d6a446c5b55da30473d1c64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = VoxelNet().to(device)\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                      T_max=CFG['cosine_lr_eta_min'], \n",
    "                                                      eta_min=CFG['cosine_lr_T_max'])\n",
    "\n",
    "train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0707111",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f614852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader, device, dim_changer=None):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    model_preds = []\n",
    "    with torch.no_grad():\n",
    "        for vf, vc, batch_size in tqdm(iter(test_loader)):\n",
    "            vf = torch.tensor(vf).to(device)\n",
    "            vc = torch.tensor(vc).to(device)\n",
    "\n",
    "            batch_pred = model(vf, vc, batch_size=batch_size)\n",
    "            \n",
    "            model_preds += batch_pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "    \n",
    "    return model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7a74e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('./data/sample_submission.csv')\n",
    "test_points = h5py.File('./data/test.h5', 'r')\n",
    "\n",
    "test_dataset = VoxelDataset(test_df['ID'].values, None, test_points)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'],  collate_fn=detection_collate, shuffle=False, num_workers=0)\n",
    "\n",
    "checkpoint = torch.load('./ckpt/20E-val0.9419-lr_1e21e3-xaxis_reg-voxelnet.pth')\n",
    "model = VoxelNet().to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b139487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49151d9e42f6429cab18778c5315ae95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = predict(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0febf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['label'] = preds\n",
    "\n",
    "test_df.to_csv('./submission/20E-val0.9419-lr_1e21e3-xaxis_reg-voxelnet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6955f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
